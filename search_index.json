[["index.html", "Economics 1 Economics 1.1 Macroeconomics 1.2 Outdated Marginalism 1.3 Market Primacy", " Economics Dyrehaugen Web Notebook 2022-06-30 1 Economics Economics is a reductionist carve-out of commodifiable pieces of the larger complex systems of society and ecology. Energy is money. Nearly everything else is credit. So what is modern economics about? It seems to be, mainly, about itself. The ‘science’ of maximizing and minimizing stuff you can’t measure. The cult of math modl-ing. Palley (2021) Life among the Econs - 50 years on (pdf) The failure of economics to cultivate the sustained acceptance of externalities is increasingly becoming the most pertinent fact about the whole discipline. Not only did economics – the ‘science of markets’ – not encourage acceptance of externalities, but it also made strenuous effort to downplay or even trivialize them. Austin (2021) (pdf) Humans typically assume that the world is ruled by uncertainty and apply according decision making rules. Experimental economists, by contrast, only consider well-defined, resolvable problems and usually don’t understand that their problems do not correspond to real life problems. If wants are unlimited, as economists assume they are, then the world is scarce by definition – a scarcity that can be confronted only by more work and growth. Growth promises everyone more tomorrow; but at the same time, there is never ‘enough for everyone to have a decent share’ Any sufficiently advanced religion is indistinguishable from economics. Ian Wright So entrenched has economic thinking become within modern culture that we are not aware it is the root cause of climate change and nature crises. Ceteris Paribus Economics cuts and reduces to formulate tractable problems that admit of elegant solutions, but the process inevitably renders economics a deeply decontextualized body of knowledge. The discipline concerns itself only with the monetizable subdomain of broader societal and ecological relations. 1.1 Macroeconomics If there’s one thing you should know about macroeconomics, it’s this: Convincing evidence is really really hard to come by, so people end up relying a lot on theory and making a lot of assumptions. In macroeconomics it’s very hard to get the kind of clean, convincing evidence that we have for policies; macroeconomic stuff affects everything and is affected by everything, so it’s incredibly hard to disentangle cause and effect. 1.2 Outdated Marginalism With its fixation on equilibrium thinking and an exclusive focus on market factors that can be precisely measured, the neoclassical orthodoxy in economics is fundamentally unequipped to deal with today’s biggest problems. Change within the discipline is underway, but it cannot come fast enough. Nowhere are the limitations of neoclassical economic thinking – the DNA of economics as it is currently taught and practiced – more apparent than in the face of the climate crisis. While there are fresh ideas and models emerging, the old orthodoxy remains deeply entrenched. Change cannot come fast enough. The economics discipline has failed to understand the climate crisis – let alone provide effective policy solutions for it – because most economists tend to divide problems into small, manageable pieces. Rational people, they are wont to say, think at the margin. What matters is not the average or totality of one’s actions but rather the very next step, weighed against the immediate alternatives.Such thinking is indeed rational for small discrete problems. Compartmentalization is necessary for managing competing demands on one’s time and attention. But marginal thinking is inadequate for an all-consuming problem touching every aspect of society.Economists also tend to equate rationality with precision. The discipline’s power over public discourse and policymaking lies in its implicit claim that those who cannot compute precise benefits and costs are somehow irrational. This allows economists – and their models – to ignore pervasive climate risks and uncertainties, including the possibility of climatic tipping points and societal responses to them. And when one considers economists’ fixation with equilibrium models, the mismatch between the climate challenge and the discipline’s current tools becomes too glaring to ignore. Brookes and Wagner (2021) Economics Needs a Climate Revolution 1.3 Market Primacy Austin The key driver of the ecological crisis is our hunger for ‘economic growth’ that continues to overwhelm our fast-developing – fast-recovering? – sense of the need to protect the global environment. Of course, efforts are being made to tackle the problem, but so entrenched has economic thinking become within modern culture that we are not yet reconciled to the fact that the form of economic thinking may be the root cause. Hence the dominant strategy we have adopted to address the sustainability crisis is that of ‘voluntary market-based’ approaches, under different banners: ‘sustainable development’, ‘sustainable capitalism’, ‘green growth’, ‘win-win’, and an ESG (environmental, social and governance) movement more generally. Alas, it is increasingly apparent that this overall strategy amounts merely to a grafting of environmental concern onto an economic system that remains stubbornly ‘business as usual’ in its deeper workings. Perversely, our concern for the environment has been transmuted into a potential new source of products and business lines: ‘eco’ as a new marketing seam to be mined, ‘green’ as a splendid new runway for growth! In other words, it is becoming a classic case of the ‘solution’ exacerbating the problem. The human mind has become aware of a problem it recognizes as serious, but its increasingly frantic efforts to address the problem seem unable to escape an imprisoning logic. That ‘logic’ is the entrenched norm of market primacy, to which Western cultures had iterated to by the late 1970s, for reasons that entirely predate our awareness of the Anthropocene and the Great Acceleration. The market primacy of our current ‘neoliberal’ social order emerged from political arguments seeking to safeguard individual rights and from decades of complementary economic theorizing that appeared to build a case for the superiority – almost infallibility – of market outcomes. Recognize our sustainability problems as ultimately a matter of collective cognition Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Cris is "],["economic-rent.html", "2 Economic Rent 2.1 Digital Rent", " 2 Economic Rent 2.1 Digital Rent Birch Big Tech ecosystems are important techno-economic sites of new and emerging forms of digital rentiership. Big Tech is characterized by the emergence of new and specifically digital forms of rentiership, defined as the construction and extraction of value through the techno-economic extension of ownership and/or control over assets, often resulting from some artificial or natural scarcity, quality, or productivity We outline four emerging forms of digital rentiership in Big Tech ecosystems reflecting the similarities and diversities in Big Tech firms themselves: ‘enclave rents’ created through the control of ecosystems; ‘expected monopoly rents’ created through the performative fulfilment of future narratives; ‘engagement rents’ constituted via rankings and metrics that differentiate users by their engagement with digital services and products; and ‘reflexivity rents’ obtained by exploiting ecosystem rules and norms. Birch (2021) Big Tech: Four Emerging Forms of Digital Rentiership "],["scarcity.html", "3 Scarcity 3.1 There is no Scarcity 3.2 How There’s More to Economics Than the Science of Scarcity 3.3 The Case against Civilization", " 3 Scarcity 3.1 There is no Scarcity Hickel When we look at the world in terms of real resources and energy (i.e., the stuff of provisioning), it becomes clear that there is no scarcity at all. The problem isn’t that there’s not enough, the problem, again, is that it is maldistributed. A huge chunk of global commodity production is totally irrelevant to human needs and well-being. Consider all the resources and energy that are mobilized for the sake of fast fashion, throwaway gadgets, single-use stadiums, SUVs, bottled water, cruise ships and the military-industrial complex. Consider the scale of needless consumption that is stimulated by manipulative advertising schemes, or enforced by planned obsolescence. Consider the quantity of private cars that people have been forced to buy because the fossil fuel industry and automobile manufactures have lobbied so aggressively against public transportation. Consider that the beef industry alone uses nearly 60% of the world’s agricultural land, to produce only 2% of global calories. There is no scarcity. Rather, the world’s resources and energy are appropriated (disproportionately from the global South) in order to service the interests of capital and affluent consumers (disproportionately in the global North). We can state it more clearly: our economic system is not designed to meet human needs; it is designed to facilitate capital accumulation. And in order to do so, it imposes brutal scarcity on the majority of people, and cheapens human and nonhuman life. It is irrational to believe that simply “growing” such an economy, in aggregate, will somehow magically achieve the social outcomes we want. Hickel 3.2 How There’s More to Economics Than the Science of Scarcity Gruen One way economists describe their discipline to themselves has proven beguilingly seductive since it was codified by Lionel Robbins 90 years ago — that economics is the science of scarcity and that it is, therefore, paradigmatically about trade-offs. This approach has become a kind of counterfeit metaphysics — a means by which practice becomes increasingly thoughtless and alienated from economic reality whilst practitioners affect rigor and insightfulness. Trade-offs As a Paradigm The reason for the tradeoff relationship between equality and efficiency is the empty one that equality is not efficiency. This structuring of entire bodies of thought around the empty observation that one thing is not another thing is replicated endlessly. Humans’ extraordinary and unique capacity for shared intentionality is the foundation of our astounding productivity. The most fundamental means by which this is done is via what I have called generative orders — language and culture being preeminent examples, though markets and money are others. Within these generative orders, our cognition of the world, our intentions, and our mutual expectations of each other are entangled. This foundation enables us to build other special-purpose institutions. My point has simply been to show one theoretical framing of the relationship between efficiency and equality that proceeds from careful, critical observation of and abstraction from reality. If this is well-judged, our understanding of reality improves as do our prospects of improving it. The textbook approach couldn’t be more different. Turns out that it is metaphysical fairy-floss. The “efficiency-equality” trade-off exists as a particular case of the general one that if you wish to achieve one thing, doing something else could get in your way. Gruen (2022) How There’s More to Economics Than the Science of Scarcity 3.3 The Case against Civilization Lanchester John Maynard Keynes’s famous 1930 essay “The Economic Possibilities for Our Grandchildren.” Keynes speculated that if the world continued to get richer we would naturally end up enjoying a high standard of living while doing much less work. He thought that “the economic problem” of having enough to live on would be solved, and “the struggle for subsistence” would be over: When the accumulation of wealth is no longer of high social importance, there will be great changes in the code of morals. We shall be able to rid ourselves of many of the pseudo-moral principles which have hag-ridden us for two hundred years, by which we have exalted some of the most distasteful of human qualities into the position of the highest virtues. We shall be able to afford to dare to assess the money-motive at its true value. The love of money as a possession—as distinguished from the love of money as a means to the enjoyments and realities of life—will be recognized for what it is, a somewhat disgusting morbidity, one of those semi-criminal, semi-pathological propensities which one hands over with a shudder to the specialists in mental disease. The world has indeed got richer, but any such shift in morals and values is hard to detect. Money and the value system around its acquisition are fully intact. Greed is still good. The study of hunter-gatherers, who live for the day and do not accumulate surpluses, shows that humanity can live more or less as Keynes suggests. It’s just that we’re choosing not to. A key to that lost or forsworn ability, Suzman suggests, lies in the ferocious egalitarianism of hunter-gatherers. For example, the most valuable thing a hunter can do is come back with meat. Unlike gathered plants, whose proceeds are “not subject to any strict conventions on sharing,” hunted meat is very carefully distributed according to protocol, and the people who eat the meat that is given to them go to great trouble to be rude about it. This ritual is called “insulting the meat,” and it is designed to make sure the hunter doesn’t get above himself and start thinking that he’s better than anyone else. “When a young man kills much meat,” a Bushman told the anthropologist Richard B. Lee, “he comes to think of himself as a chief or a big man, and he thinks of the rest of us as his servants or inferiors. . . . We can’t accept this.” The insults are designed to “cool his heart and make him gentle.” For these hunter-gatherers, Suzman writes, “the sum of individual self-interest and the jealousy that policed it was a fiercely egalitarian society where profitable exchange, hierarchy, and significant material inequality were not tolerated.” This egalitarian impulse, Suzman suggests, is central to the hunter-gatherer’s ability to live a life that is, on its own terms, affluent, but without abundance, without excess, and without competitive acquisition. The secret ingredient seems to be the positive harnessing of the general human impulse to envy. As he says, “If this kind of egalitarianism is a precondition for us to embrace a post-labor world, then I suspect it may prove a very hard nut to crack.” There’s a lot that we could learn from the oldest extant branch of humanity, but that doesn’t mean we’re going to put the knowledge into effect. A socially positive use of envy—now, that would be a technology almost as useful as fire. Lanchester (2017) The Case against Civilization "],["rationing.html", "4 Rationing", " 4 Rationing Climate Change Economics Curbing consumption can be done either through rationing or draconian taxation. Both are feasible technically although their political acceptability may not be the same. If one were to use rationing, one could introduce physical targets: there will be only x liters of gas per car annually and no family will be allowed to have more than two cars; or y kilograms of meat per person per month; or z kilowatts of electricity per household per month (or rolling blackouts). Clearly, there may be a black market for gas or meat, but the overall limits will be observed simply because they are given by the total availability of coupons. Some people might think that rationing is extraordinary, and I agree with them. But it has been done in a number of countries under wartime, and at times even during peacetime conditions, and it has worked. If indeed we face an emergency of such “terminal” proportions as the advocates of climate change claim, I do not see any reason why we should not resort to extreme measures. Milanovic "],["taxing.html", "5 Taxing 5.1 Tax Shifting 5.2 Georgism - Land Value Tax", " 5 Taxing Climate Change Economics But another approach (draconian taxation) is possible too. Instead of limiting physical quantities of goods and services that fulfill criteria (a) and (b) we would impose extremely heavy taxes on them. There is always a tax rate that would drive consumption of a good down to the level that we have in mind. It is here that I think we can use—again if we believe that the climate emergency is so dire—the lessons of covid. Economic dislocations would be huge. It is not only the question of the entire upper middle class and the rich in advanced countries (and, as we have seen, elsewhere) losing significant parts of their real income as prices of most “staple” commodities (for them) increase by two, three or ten times; the dislocation will affect large sectors of the economy. The effects will trickle down: unemployment will increase, incomes will plummet, the West will record the largest real income decline since the Great Depression. However if such policies were steadfastly pursued for a decade or two, not only would emissions plummet too (as they have done in 2020), but our behavior and ultimately the economy would adjust. People will find jobs in different activities that will remain untaxed and thus relatively cheaper and whose demand will go up. Revenues collected from taxing “bad” actvities may be used to subsidize “good” activities or retrain people who have lost their jobs. This is not magical thinking. These are policies that, with intergovernmental cooperation, knowledge of economics, data on global inequality, and the experience of covid, could be implemented. Milanovic 5.1 Tax Shifting FT US president Joe Biden’s plan to reform global corporate taxation will do little to help the countries most inneed of more tax revenues, say developing economies which are lobbying for greater power over multinationals. Washington’s ambitious proposal would tax 100 of the world’s largest companies on profits made in countries where they have little or no physical presence but derive substantial revenues and would introduce a global minimum tax rate, in a bid to end what it dubbed a “race to the bottom” where businesses channel profits through low-tax jurisdictions. But companies would pay most of their taxes in the country where they are headquartered, even if their profits — and in many cases the labour and raw materials used — are sourced from developing countries, senior diplomats and lobby groups told the Financial Times. They are also concerned that many developing countries are not participating in the negotiations over the proposal at the OECD and fear the eventual agreement is unlikely to reflect their interests. FT 5.2 Georgism - Land Value Tax Doucet Georgism is a school of political economy that is really upset about, among other things, the Rent Being Too Damn High. It seeks to liberate labor and capital alike from those who gatekeep access to scarce “non-produced assets,” such as land and natural resources, while still affirming the virtues of hard work and free enterprise. George uses the term “Land” to mean not just regular land, but everything that is external to human beings and the things they produce–nature itself, really. Georgism’s chief insight is to move economic thinking from a two-factor model (Labor and Capital) to a three-factor model (Land, Labor, and Capital). It’s chief (but not only) policy prescription is the Land Value Tax (LVT), which taxes real estate at as close to 100% of its “land rent” as possible (the amount of rent due to the land alone apart from “improvements” such as buildings). In actual practice, most Georgists seem to think 85% is a reasonable figure to target. Lars Doucet (2022) Does Georgism Work? Part 1: Is Land Really A Big Deal? Noah Smith (2022) How to sell Georgism to the middle class "],["nudging.html", "6 Nudging 6.1 Randomness", " 6 Nudging 6.1 Randomness Memo There is strong evidence from the lab that people have misperceptions about what randomness looks like. When a person is asked to generate a series that approximates the flipping of a coin, they will alternate between heads and tails too often, and balance the frequencies of heads and tails over too short a sequence. When people are asked to judge which of two different sequences of coin flips are more likely, they tend to pick sequences with more alternation, despite their probability being the same. What happens we look for a failure to perceive randomness in the outside world? Out of the lab? When people watch basketball, they often see a hot hand. They will describe players as “hot” and “in form”. Their belief is that the person who has just hit a shot or a series of shots is more likely to hit their next one. But is this belief in the “hot hand” a rational belief? Or is the hot hand an illusion, whereby, just like they do with coins, they are seeing streaks in what is actually randomness? In a famous examination of this question, Thomas Gilovich, Robert Vallone and Amos Tversky took shot data from a variety of sources, including the Philadelphia 76ers and Boston Celtics, and examined it for evidence of a hot hand. What did they find? The hot hand was an illusion. As Daniel Kahneman wrote in Thinking, Fast and Slow when describing this research: The hot hand is entirely in the eye of the beholders, who are consistently too quick to perceive order and causality in randomness. The hot hand is a massive and widespread cognitive illusion. Possibly even more interesting was the reaction to the findings from those in the sporting world. Despite the analysis, many sports figures denied that it could be true. Red Auerbach, who coached the Boston Celtics to nine NBA championships, said “Who is this guy? So he makes a study. I couldn’t care less.” This provides another insight, about which Gilovich wrote: The story of our research on the hot hand is only partly about the misperception of random events. It is also about how tenaciously people cling to their beliefs even in the face of hostile evidence. So, this isn’t just about the misperception of the hot hand, but also about the failure of people to see their error when presented with evidence about it. Let’s delve into how Gilovich, Vallone and Tversky showed the absence of a hot hand. Imagine a person who took ten shots in a basketball game. A ball is a hit, an X is a miss. What would count as evidence of a hot hand? What we can do is look at shots following a previous hit. For instance, in this sequence of shots there are 6 occasions where we have a shot following a previous hit. Five of those shots, such as the seventh here, are followed by another hit. We can then compare their normal shooting percentage with the proportion of shots they hit if the shot immediately before was a hit. If their hit rate after a hit is higher than their normal shot probability, then we might say they get a hot hand. This is effectively how Gilovich, Vallone and Tversky examined the hot hand in coming to their conclusion that it doesn’t exist. They also looked at whether there was a hit or miss after longer streaks of hits or misses, but this captures the basic methodology. It seems sensible. But let me take a detour that involves flipping a coin. Suppose you flip a coin three times. Here are the eight possible sequences of heads and tails. Each sequence has an equal probability of occurring. What if I asked you: if you were to flip a coin three times, and there is a heads followed by another flip in that sequence, what is the expected probability that another heads will follow that heads? Here is the proportion of heads following a previous flip of heads for each sequence. In the first row of the table, the first flip is a head. That first flip is followed by another head. After the second flip, a head, we also have a head. There is no flip after the third head. 100% of the heads in that sequence followed by another flip are followed by a head. In the second row of the table, 50% of the heads are followed by a head. In the last two rows, there are no heads followed by another flip. Now, back to our question: if you were to flip a coin three times, and there is a heads followed by another flip in that sequence, what is the expected probability that another heads will follow that heads? It turns out it is 42%, which I can get by averaging those proportions. 8 possible combinations of heads and tails across three flips Flips p(Ht+1|Ht) HHH 100% HHT 50% HTH 0% HTT 0% THH 100% THT 0% TTH – TTT – Exp.val 42% That doesn’t seem right. If we count across all the sequences, we see that there are 8 flips of heads that are followed by another flip. Of the subsequent flips, 4 are heads and 4 are tails, spot on the 50% you expect. What is going on in that second column? By looking at these short sequences, we are introducing a bias. The cases of heads following heads tend to cluster together, such as in the first sequence which has two cases of a heads following a heads. Yet the sequence THT, which has only one shot occurring after a heads, is equally likely to occur. The reason a tails appears more likely to follow a heads is because of this bias whereby the streaks tend to cluster together. The expected value I get when taking a series of three flips is 42%, when in fact the actual probability of a heads following a heads is 50%. As the sequence of flips gets longer, the size of the bias is reduced, although it is increased if we examine longer streaks, such as the probability of a heads after three previous heads. Why have I bothered with this counterintuitive story about coin flipping? Because this bias is present in the methodology of the papers that purportedly demonstrated that there was no hot hand in basketball. Because of this bias, the proportion of hits following a hit or sequence of hits is biased downwards. Like our calculation using coins, the expected proportion of hits following a hit in a sequence is lower than the actual probability of hitting a shot. Conversely the hot hand pushes the probability of hitting a shot after a previous hit up. Together, the downward bias and the hot hand roughly cancelled each other out, leading to the conclusion by researchers that each shot is independent of the last. The result is, that when you correct for the bias, you can see that there actually is a hot hand in basketball. When Miller and Sanjurjo crunched the numbers for one of the studies in the Gilovich and friends paper, they found that the probability of hitting a shot following a sequence of three previous hits is 13 percentage points higher than after a sequence of three misses. There truly is a hot hand. If Red Auerbach had coached as though there were no hot hand, what would his record have looked like? I should say, this point does not debunk the earlier point about people misperceiving randomness. The lab evidence is strong. People tend to see the hot hand when people flip coins. It is possible that people overestimate the strength of the hot hand in the wild, although that is hard to show. But the hot hand exists. Let’s turn back to one of the quotes I showed earlier. The story of our research on the hot hand is only partly about the misperception of random events. It is also about how tenaciously people cling to their beliefs even in the face of hostile evidence. The researchers expanded the original hot hand research from a story about people misperceiving randomness, to one of them continuing to do so even when presented with evidence that they were making an error. But, as we can now see, their belief in the hot hand was not an error. The punters in the stands were right. Their accumulated experience had given them the answer. The researchers were wrong. Rather than the researchers asking whether they themselves were making an error when people refused to believe their research, they double downed and identified a second failure of human reasoning. The blunt dismissal of people’s beliefs led behavioural scientists to hold an untrue belief for over thirty years This is a persistent characteristic of much applied behavioural science. It was an error I made many times when I first came to the discipline. We spend too little time questioning our understanding of the decisions or observations other people make. If we believe they are in error, we should first question whether the error is ours. Jason Collins on Nudgestock 2020 "],["discounting.html", "7 Discounting", " 7 Discounting Ecological Fallacy Solow (1974) like many other defenders of standard economics, resorts to an old paper of Harold Hotelling (1931) to convince us that neoclassical economists have not ignored the problem of intergenerational allocation. But he overlooks the important point that Hotelling’s analysis referred to some known amount of resources owned by an individual who discount future royalities. Of course, Hotelling was completely correct about the last point. Any individual must certainly discount the future for the indisputable reason that, being mortal, he stand a chance of dying any day. But a nation, let alone the whole of mankind, cannot behave on the idea that it might die tomorrow. They behave as if they were immortal and, hence, value future welfare situations without discounting. Georgscu-Roegen (1986) The Entropy Law and the Economic Process in Retrospect (pdf) "],["risking.html", "8 Risking 8.1 Uncertainty 8.2 Pooling Risk", " 8 Risking 8.1 Uncertainty Abstract Uncertainty is critical to questions about climate change policy. Recently developed recursive integrated assessment models have become the primary tool for studying and quantifying the policy implications of uncertainty. The first wave of recursive models has made valuable, pioneering efforts at analyzing disparate sources of uncertainty. We decompose the channels through which uncertainty affects policy and quantify them in a recursive extension of a benchmark integrated assessment model. We argue that frontier numerical methods will enable the next generation of recursive models to better capture the information structure of climate change and to thereby ask new types of questions about climate change policy Lemoine (2016) Uncertainty Recursive IAM Cubic damage triple the risk premium Traeger (ref.Gernot Wagner) 8.2 Pooling Risk We each put $100 a month in our individual piggy banks to cover potential medical costs one day. We all chip in $100 a month, for anyone who needs medical care this month. Same costs for everyone, totally different risks, totally different societies. Pooling risk reduces/eliminates volatility. Off course it also introduces adverse selection and moral hazard. (Peters/Pienar (Twitter)) "],["cooperation.html", "9 Cooperation", " 9 Cooperation Handley Abstract A fundamental puzzle of human evolution is how we evolved to cooperate with genetically unrelated strangers in transient interactions. Group-level selection on culturally differentiated populations is one proposed explanation. We evaluate a central untested prediction of Cul- tural Group Selection theory, by assessing whether readiness to cooperate between indivi- duals from different groups corresponds to the degree of cultural similarity between those groups. We documented the normative beliefs and cooperative dispositions of 759 indivi- duals spanning nine clans nested within four pastoral ethnic groups of Kenya—the Turkana, Samburu, Rendille and Borana. We find that cooperation between groups is predicted by how culturally similar they are, suggesting that norms of cooperation in these societies have evolved under the influence of group-level selection on cultural variation. Such selection acting over human evolutionary history may explain why we cooperate readily with unrelated and unfamiliar individuals, and why humans’ unprecedented cooperative flexibility is never- theless culturally parochial. We conclude that group-level selection on cultural variation has likely left a mark on the human cooperative psychology and continues to influence which social norms and institutions prevail in human societies. Handley (2020) Human large-scale cooperation as a product of competition between cultural groups (pdf) "],["decoupling.html", "10 Decoupling 10.1 Rebound (Jevons Paradox)", " 10 Decoupling Decoupling: the end of the correlation between increased economic production and decreased environmental quality. The needed decoupling does not occur! Not GLOBAL, not FAST-ENOUGH, not LONG-ENOUGH Vaden (abstract) The idea of decoupling “environmental bads” from “economic goods” has been proposed as a path towards sustainability by organizations such as the OECD and UN. Scientific consensus reports on environmental impacts (e.g., greenhouse gas emissions) and resource use give an indication of the kind of decoupling needed for ecological sustainability: global, absolute, fast-enough and long-enough. This goal gives grounds for a cate- gorisation of the different kinds of decoupling, with regard to their relevance. We conducted a survey of recent (1990–2019) research on decoupling on Web of Science and reviewed the results in the research according to the categorisation. The reviewed 179 articles contain evidence of absolute impact decoupling, especially between CO2 (and SOX) emissions and evidence on geographically limited (national level) cases of absolute decoupling of land and blue water use from GDP, but not of economy-wide resource decoupling, neither on national nor international scales. Evidence of the needed absolute global fast-enough decoupling is missing. Vaden 2020 Decoupling for sustainability (pdf) 10.1 Rebound (Jevons Paradox) Lange Literature on the rebound phenomenon has grown significantly over the last decade. However, the field is characterized by diverse and ambiguous definitions and by substantial discrepancies in empirical estimates and policy proposals. As a result, cumulative knowledge production is difficult. To address these issues, this article develops a novel typology. Based on a critical review of existing classifications, the typology introduces an important differentiation between the rebound mechanisms, which generate changes in energy consumption, and the rebound effects, which describe the size of such changes. Both rebound mechanisms and rebound effects can be analytically related to four economic levels – micro, meso, macro and global – and two time frames – short run and long run. The typology is populated with eighteen rebound mechanisms from the literature. This contribution is the first that transparently describes its criteria and methodology for developing a rebound typology and that gives clear definitions of all terms involved. The resulting rebound typology aims to establish common con­ ceptual ground for future research on the rebound phenomenon and for developing rebound mitigation policies. Lange (2021) Jevons Unravelled (pdf) "],["ergodicity.html", "11 Ergodicity 11.1 Almost surely 11.2 Kelly Criterion", " 11 Ergodicity Almost everyone responded to my question about the rationality of expected utility by talking about rationality and utility. But it’s the “expected” part that is the problem. Why would I only care about the mean? (Russel Roberts (Tweet)) EE doesn’t necessarily reject EUT; it offers one interpretation of EUT which is particularly useful and makes strong prediction. You can think of EE as an axiomatization of 19th-century EUT, an alternative axiomatization to von Neumann’s. 11.1 Almost surely Over the very long-term, an individual will tend to get around half heads and half tails. As the number of flips goes to infinite, the proportion of heads or tails “almost surely” converges to 0.5. This means that each person will tend to get a 50% increase half the time (or 1.5 times the initial wealth), and a 40% decrease half the time (60% of the initial wealth). A bit of maths and the time average growth in wealth for an individual is (1.5*0.6)0.5 ~ 0.95, or approximately a 5% decline in wealth each period. Every individual’s wealth will tend to decay at that rate. To get an intuition for this, a long run of equal numbers of heads and tails is equivalent to flipping a head and a tail every two periods. Suppose that is exactly what you did – flipped a heads and then flipped a tail. Your wealth would increase to $150 in the first round ($1001.5), and then decline to $90 in the second ($1500.6). You get the same result if you change the order. Effectively, you are losing 10% (or getting only 1.5*0.6=0.9) of your money every two periods. A system where the time average converges to the ensemble average (our population mean) is known as an ergodic system. The system of gambles above is non-ergodic as the time average and the ensemble average diverge. And given we cannot individually experience the ensemble average, we should not be misled by it. The focus on ensemble averages, as is typically done in economics, can be misleading if the system is non-ergodic. While the population as an aggregate experiences outcomes reflecting the positive expected value of the bet, the typical person does not. The increase in wealth across the aggregate population is only due to the extreme wealth of a few lucky people. 11.2 Kelly Criterion The only way for someone to maintain their wealth would be to bet a smaller portion of their wealth, or to diversify their wealth across multiple bets. The Kelly criterion gives the bet size that would maximise the geometric growth rate in wealth. \\[f = \\frac{bp-q}{b} = \\frac{p(b+1)-1}{b}\\] f is the fraction of the current bankroll to wager b is the net odds received on the wager (i.e. you receive $b back on top of the $1 wagered for the bet) p is the probability of winning q is the probability of losing (1-p) The Kelly criterion is effectively maximising the expected log utility of the bet through setting the size of the bet. The Kelly criterion will result in someone wanting to take a share of any bet with positive expected value. An alternative more general formula for the Kelly criterion that can be used for investment decisions is: \\[f = \\frac{p}{a} - \\frac{q}{b}\\] f is the fraction of the current bankroll to invest b is the value by which your investment increases (i.e. you receive $b back on top of each $1 you invested) a is the value by which your investment decreases if you lose (the first formula above assumes a=1) p is the probability of winning q is the probability of losing (1-p) (More on Evolving Preferences) A Primer (Jason Collins) "],["economic-planning.html", "12 Economic Planning", " 12 Economic Planning Krahé Planning—the setting of economic priorities via non-market means—must be distinguished from a command economy. A command economy, in contrast to planning, is one particular way of injecting plans into the division of labor, namely with command and control measures.16 As France, Sweden, Japan and other mixed economies demonstrated in the wake of WWII, there are many other ways to introduced plans into an economy: from direct public investment, via taxes and subsidies, to banking regulation, credit guidance, and foreign exchange allocation, to name but a few.17 The advantage of planning is its ability to focus resources, create and channel energies, and reduce uncertainty. Pierre Massé, former General Commissioner of Planning in France, called it l’anti-hasard. When the Monnet Plan was introduced in France in 1946, its goal was to reach the nation’s pre-war production level by 1948, and then surpass it by 50 percent by 1950. Under the slogan “modernization or decadence,” it prioritized investment over consumption, allocated scarce US dollar reserves to their most important uses, and channeled resources into sectors identified as crucial for restarting and growing France’s economy. “Bottlenecks were broken during the early days,”19 and while not all targets were reached, the plan provided “discipline, direction, vision, confidence, and hope.”20 The Monnet Plan navigated a situation not entirely unlike our own. In the postwar moment, both domestic funds and foreign exchange were scarce. Since dollars and francs were not freely exchangeable at the time, they had to be budgeted for separately, acting as a double budget constraint. Today, we again face a double budget constraint: economic and ecological. The Monnet Plan navigated this tightly binding double constraint through prioritization. Instead of attempting to plan for all sectors, the Plan focused on six strategic industries: electricity, steel, coal mining, transport, cement, and agricultural machinery.21 The lesson for today is obvious. Focus on the five sectors driving climate change, land use, and biodiversity loss today: energy, transport, industry,22 housing, and agriculture. The process of sectoral planning was led by a core staff in the Commissariat général du Plan, numbering around 100. This core staff cooperated with a number of so-called modernization commissions, composed of representatives from the state, employers, and trade unions, which tackled either specific sectors or cross-cutting themes like finance or labor. Krahé (2002) The Whole Field - Markets, planning, and coordinating the green transformation "],["austrian.html", "13 Austrian 13.1 Where Austrians got it wrong", " 13 Austrian MMT and Austrian economics are mirror images. MMT wants soft money to redistribute wealth to middle class. Austrians want hard money to maintain value of middle class savings. Dominant capitals, in control of state, ignore both and switch between soft and hard to maintain power. (Ian Wright) 13.1 Where Austrians got it wrong On Colin Drumm https://twitter.com/drumm_colin/status/1468779685916012546 Thread Reader https://threadreaderapp.com/thread/1441459276506021890.html Ethan Buchman 24 Sep, 24 tweets, 5 min read Allow me to attempt a more nuanced story of where the Austrians actually did go “wrong” and how they wound up so vilified. I’m still learning the history and formulating my thoughts, but here’s a humble attempt: 1/ A good deal of original Austrian thought was incorporated into the mainstream. Though it had Austrian origins, it became no longer “Austrian” in spirit. The ideas were so good, they had to be appropriated. 2/ This included, of course, marginal utility theory itself (from Menger) as well as ideas like opportunity cost. And Bohm-Bawerk’s capital theory of interest rates (inter-temporal coordination) was taken up by Wicksell who in turn had a heavy influence on Keynes 3/ Mises and Hayek were also heavily influenced by Wicksell, expounding what would become known as the Austrian Business Cycle Theory (ABCT), the foundation for their opposition to govt monopoly on money 4/ Roughly speaking, this is where the Austrians went astray - in obsessing over money as a neutral, non-political veil over real exchange and production, they were largely in denial of actual history, important parts of theory, and even the real world itself! 5/ Neglect of history is partially due to the earlier war with the German Historical School, the so-called Methodenstreit. The Austrians were hell bent on a Praxeology/“Methodological Individualism” which afforded little role for historical circumstance in economic analysis 6/ This is probably the largest error they made. I’m sure the Methodenstreit was a good time, but the inability to incorporate the historical/anthropological record greatly weakened their ability to theorize. They began from an assumption that prior man was similar to modern man 7/ But this is manifestly not true. Cognition and social structures change markedly over time, and this has significant impact on economic reality and coordination. Two very interesting related points here: 8/ First, Weber, who was actually a member of the Historical School, had a major impact on defining Methodological Individualism, and had a huge influence on Mises! 9/ Second, Schumpeter, who would perhaps be the greatest economist produced out of the Austrian tradition (him and Mises were students together of Bohm-Bawerk) actually defected towards the Historical School. Arguably this enabled him to break free from … 10/ Austrian constraints and develop a more complete theory and history of economics. Notably, his student Hyman Minsky, would get all the credit post 2008 for theories about the inherent instability of credit money, to the chagrin of Austrians everywhere. 11/ But Minsky’s Financial Instability Hypothesis wasn’t the ABCT. It was grounded in analysis of the actual web of liabilities produced by a financial system. While the Austrians wrote it all off as “bad”, Minsky sat down to analyze it and provide some constructive policy! 12/ Part of the failure of the Austrian imagination was to provide some meaningful alternative to abolishing fractional reserve banking in favour of pure commodity money. There are some inklings of the power of trade credit clearing in Mises’ TOMC, but it seemed to stop there 13/ Of course Hayek would eventually propose denationalized fiat, and a later tradition of Austrian inspired economists like (???) and (???) would bring this home through a theory and history of free banking - a stable fractional reserve based system 14/ Not to say Austrians were “wrong”, but there was an inherent sort of “arrested development” in their ideas. To continue the history, despite their brilliance, the Austrians took a beating in the 30s when Hayek tragically “lost” the debate to Keynes: 15/ After the second war, they turned their attention more to politics. Hayek of course founded an information theoretic approach to econ (prices as decentralized network of computation) which would have enormous influence. But increasingly “Austrian” would be less associated 16/ with the brilliant economic work of earlier years (so much of which was appropriated in the mainstream) and increasingly associated with a kind of “reactionary” and seemingly “far right” political philosophy 17/ In my own estimation this is at least a mischaracterization of Hayek’s agenda, though I can’t help but agree that his work suffered from a kind of arrested development. And the founding of the Mont Pelerin Society did not help much. 18/ The Austrian tradition had something of a boom in the 80s when it was vehemently taken up by Reagan and Thatcher, but in a way that was highly discretionary/selective. The existence of their large governments and their alliance with the banker class betrayed the essence of 19/ the Austrian teachings! It’s no wonder this would give rise to an abominable neoliberalism which I can only imagine caused more than a few of the Austrian greats to roll in their grave. Personally, I can’t help but feel there was a certain naiveté 20/ in their political philosophy, perhaps a result of their dismissal of the historical school and denial of the role of State, which ultimately led to their philosophy becoming a weapon of what were perhaps some of the most destructive “peacetime” regimes the West has seen 21/ Of course the Rothbardians would take the political philosophy much farther, and would lead to something of a “split” within the Austrians, seeing Hayek as somehow lesser for affording a greater role for the state and realizing that money need not be based on real commodities 22/ But obviously Hayek was on the right track, distracted though he may have been by the political climate. Arguably, his line of thought matured quite substantially in both the Free Banking school and in the Bloomington School of the Ostroms. 23/ And this is where many of us (in crypto, say) are picking up today. Recognizing the foundational brilliance of the Austrians but seeking to not make the same neoliberal mistakes by instead following the Austrians to their conclusion in the work of the Ostroms: local commons! "],["behavioural-economics.html", "14 Behavioural Economics 14.1 Biased Behaviour", " 14 Behavioural Economics Ole Peters The symbol of behavioral economics is a fly etched into a urinal to reduce spillage. Apparently, no scientific study of the effect exists. It’s becoming hard to avoid the impression that behavioral economics, broadly speaking, is a collection of made-up cocktail-party stories. Derman People are bad at making rational decisions. One of the hottest topics in finance and economics for the past two decades has been Behavioral Economics, a field that originated in the research of Daniel Kahneman and Amos Tversky. Tversky died in 1996, and Kahneman was awarded The Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred Nobel in 2002. The Nobel committee cited their joint work on “prospect theory as an alternative, that better accounts for observed behavior” of humans making decisions “when future consequences are uncertain” (aren’t they always?). Classical financial modeling assumes that people make decisions in a cold-blooded utilitarian way that is therefore susceptible to mathematics and statistics. Kahneman and Tversky (K&amp;T) cataloged a collection of irrational warm-blooded peculiarities in the way people choose between alternative bets on their own potential profits and losses when playing games of chance. Not everyone agrees with K&amp;T. Real life is not always a game of chance; while the probability of throwing heads and tails is known exactly, the probability of human behavior is not. Animate individuals are driven by motives that can defy statistics. Our legal system recognizes this, and finds defendants guilty or innocent not on the basis of statistical evidence but on the basis of judgement and believability. Prospect Theory K&amp;T developed what they call prospect theory. In prospect theory, as opposed to classical economic theory, K&amp;T replaced homo economicus’s rational notions of losses and gains and their probabilities by the empirically determined “irrational” values used by everyday fearful and greedy hot-blooded homo affectus. Classical economic theory was elegant but flawed, and prospect theory was a beautiful idea/ideal that aimed to fix it by taking account of actual human preferences in determining economic value. Unfortunately, that isn’t what happened. First, the ambitions of prospect theory as a science of human behavior foundered in a maelstrom of increasing mathematical complexity. Second, academics use the cover of behavioral economics to write papers on all sorts of irrelevant apparent irrationalities. Third, the part of behavioral economics that did flourish enormously is the notion that people are probabilistically challenged, and that it requires governments and agencies, helped by academics, to nudge people into doing what is “good” for them. Consider Richard Thaler, a Chicago academic who has been an influential and early researcher in behavioral finance and is also the co-author of Nudge, a book he says is about “enlisting the science of choice to make life easier for people and by gentling nudging them in directions that will make their lives better.” It’s remarkable that behavioral economics has evolved from a field of study into a tool for manipulating people. This is not government of the people by the people for the people. I think I’d rather be forced than nudged. At least then the battle lines are clearer. Derman (2020) Misbehavioural (pdf) 14.1 Biased Behaviour Fix According to behavioral economics, most human decisions are mired in ‘bias’. It muddles our actions from the mundane to the monumental. Human behavior, it seems, is hopelessly subpar.1 Or is it? You see, the way that behavioral economists define ‘bias’ is rather peculiar. It involves 4 steps: Start with the model of the rational, utility-maximizing individual — a model known to be false; Re-falsify this model by showing that it doesn’t explain human behavior; Keep the model and label the deviant behavior a ‘bias’; Let the list of ‘biases’ grow. Jason Collins (an economist himself) thinks this bias-finding enterprise is weird. In his essay ‘Please, Not Another Bias!’, Collins likens the proliferation of ‘biases’ to the accumulation of epicycles in medieval astronomy. Convinced that the Earth was the center of the universe, pre-Copernican astronomers explained the (seemingly) complex motion of the planets by adding ‘epicycles’ to their orbits — endless circles within circles. Similarly, when economists observe behavior that doesn’t fit their model, they add a ‘bias’ to their list. The accumulation of ‘biases’, Collins argues, is a sign that science is headed down the wrong track. What scientists should do instead is actually explain human behavior. To do that, Collins proposes, you need to start with human evolution. The ‘goal’ of evolution is not to produce rational behavior. Evolution produces behavior that works — behavior that allows organisms to survive. If rationality does evolve, it is a tool to this end. On that front, conscious reasoning appears to be the exception in the animal kingdom. Most animals survive using instinct. An organism’s ‘bias’ should be judged in relation to its evolutionary environment. Otherwise you make silly conclusions — such as that fish have a ‘bias’ for living in water, or humans have a ‘bias’ for breathing air. When behavioral economists conclude that our probability intuition is ‘biased’, they assume that its purpose is to understand the god’s eye view of innate probability — the behavior that emerges after a large number of observations. But that’s not the case. Our intuition, I argue, is designed to predict probability as we observe it … in small samples. Fix (2021) Is Human Probability Intuition Actually ‘Biased’? "],["corporations.html", "15 Corporations", " 15 Corporations Austin Having given greed freer rein, we have gradually super-sized the impulse via the creation of corporations – ‘corporate persons’. These larger-than-life figures roam the cultural landscape leaving us at Lilliputian scale to thread a careful path among them. Not only are they 1,000-fold – or 10,000- or 100,000-fold – larger than individual persons – the original ‘person’ concept – but they are legally bound to pursue self-interest in a way that we would never think of legally binding real individuals, and which no non-sociopath would ever accept. Indeed, we have effectively created gargantuan sociopaths and given them full protection of the law and increasing capacity to shape those laws. 78 Challenging these entities of our own creation is becoming difficult: exercise some moral leadership from within and you risk being denigrated as a ‘whistle-blower’. We have both super-sized and super-empowered self-interest, out of a fundamental belief that the market system can adequately harness the consequences. Austin (2021) Market-led Sustainability is a ‘Fix that Fails’… (pdf) "],["economic-growth.html", "16 Economic Growth 16.1 Growth without Economic Growth 16.2 Ditching Economic Growth", " 16 Economic Growth 16.1 Growth without Economic Growth Key messages The ongoing ‘Great Acceleration’ [1] in loss of biodiversity, climate change, pollution and loss of natural capital is tightly coupled to economic activities and economic growth. Full decoupling of economic growth and resource consumption may not be possible. Doughnut economics, post-growth and degrowth are alternatives to mainstream conceptions of economic growth that offer valuable insights. The European Green Deal and other political initiatives for a sustainable future require not only technological change but also changes in consumption and social practices. Growth is culturally, politically and institutionally ingrained. Change requires us to address these barriers democratically. The various communities that live simply offer inspiration for social innovation. EEA 16.2 Ditching Economic Growth Russel Growth may be central to mainstream economics but nature has paid the price through pollution, waste and climate change. Some economists say it’s time for a completely different approach. It was only in the mid-20th century, in the wake of the shattering impact of World Wars and when capitalism and communism were competing for global dominance, that we began to measure the success of an economy in terms of gross national product, or GDP. The faster GDP was rising, the better an economy could be said to be performing. But something happens as all that economic activity expands. The amount of energy and resources we use also increase. Ever since the industrial revolution, fossil fuels have set us on a course of furiously expanding production, which has also meant more waste and more pollution. Historically, greenhouse gas emissions have risen alongside GDP. As economies have grown richer, nature has paid the price. And as the climate crisis has become ever-harder to ignore, more people are questioning whether infinite economic growth is possible on a planet of finite resources. Zero-emissions with twice the GDP “The Intergovernmental Panel on Climate Change in their Fifth Assessment, have 116 mitigation scenarios with a chance of staying below the 2 degree Celsius threshold. All of those scenarios assume 2-3% GDP growth rates,” says Jon Erickson, an ecological economist at the Gund Institute for Environment in Vermont, adding that this implies doubling the global economy by somewhere around 2050 . These scenarios rely not just on switching to renewables, but also on the large-scale extraction of massive volumes of carbon from the atmosphere using as-yet unproven technology, which Erickson describes as “wildly unrealistic.” “None of those models and the IPCC community even bother simulating a scenario where the global economy contracts, stabilizes and maybe even degrows,” Erickson says. “Yet that’s probably the one realistic scenario that would significantly affect greenhouse gas emissions.” It is easy to see why the idea that we must keep growing is hard to give up. When economic activity declines and we go into recession, people lose their jobs and are plunged into poverty. Yet those arguing for “degrowth” — a managed contraction of economic activity— say it doesn’t have to be this way. ime for a different approach? Federico Demaria, an economist at the Autonomous University of Barcelona, who has authored several books on degrowth, says that neoclassical economics — which has dominated economic discourse over recent decades, has “never looked at the question of how an economy could be managed without growth. It only looked at questions like, why do economies grow? If it’s not growing, how can we make it grow? Or, how can we make it grow even faster?” These have become pertinent questions even — or especially — for wealthy, industrialized economies, where growth has slowed over recent decades. “What the mainstream economists are doing is just trying to relaunch growth,” Demaria says. A different approach, which aims to rein in growth without inflicting the pain that recession has traditionally entailed, comes from the field of ecological economics. Embedding economics in ecology Neoclassical economic models picture economies as closed systems, with no inputs of materials or energy and no outputs of pollution and waste. But ecological economists insist there is no real separation between economy and ecology. After all, if we destroy the planet that feeds us, economic activity will collapse pretty quickly too. In an effort to fix this oversight, Demaria is among those devising new economic models that include factors like emissions and resources use. They are also working in things like social equality, debt, deficits and monetary systems, which have social impacts, and play into cycles of boom and bust. Degrowthers argue that we do have to tighten our belts — and it doesn’t have to be painful. If we could reverse the central logic of economic systems that prioritize growth over human and ecological wellbeing, they don’t believe we would miss the furious activity that’s keeping a minority of the human population in must-have products and ever-more material wealth. Russel (2020) Climate crisis: Is it time to ditch economic growth? "],["institutional.html", "17 Institutional 17.1 Veblen", " 17 Institutional 17.1 Veblen Like his fellow economists, Veblen was excited by the prospect that, after Darwin, the study of human society could be placed on a scientific footing. Unlike most of his colleagues, he did not think the economic system was working. In one way or another, they maintained that an economy metes out its own rewards in proportion to the productivity of those who constitute the economy. For Veblen, it was not the fittest—that is, the most productive—who were surviving and prospering. On the contrary, the winners were a “leisure class” of unproductive parasites devoted to what he called “conspicuous consumption.” In the process, they were damaging rather than serving the interests of society as a whole. This was, to speak in rational terms, inefficient and irrational. Capitalism = Plunder Veblen’s early translation of the Icelandic epic [Laxdøla-saga] did foretell the visionary economics that would carry him to fame. In the introduction, added thirty-five years later, he wrote that “the Viking age was an enterprise in piracy and slave-trade” and that the Vikings thus anticipated modern “business enterprise,” which is driven “by its quest for profits” and reliant on “getting something for nothing by force and fraud.” Behind the gently waggish satire of terms like “leisure class” and “conspicuous consumption” lies a proposition that is much sterner, and more scientific. In Veblen’s view, modern capitalism, which congratulates itself on its high level of civilization, is in essence a highly organized system of barbaric plunder. He was also lucky to take classes from a young professor named John Bates Clark (1847–1938)—“one of the most important economic theorists America has ever produced,” in Camic’s opinion. Though Clark exposed Veblen to the classical economics descended from Adam Smith and embodied by John Stuart Mill, he added his own “strong objections.” Like the so-called “historical” school of economics, which he had encountered during two years of study in Germany, Clark insisted that economic man was not a mere creature of self-interest, but rather a social being. Morality exerted an objectively measurable influence on economic life, and economics had to be cognizant of it. At that stage of his career, Clark identified as a “Christian socialist.” Veblen himself already had a reputation as an atheist. Veblen’s doctorate in 1884, which he received after transferring to Yale, was one of the first dozen granted in philosophy at any American university. What followed, as noted above, was years of unemployment. In all his many writings, Veblen distinguished between the industrial, which is about making useful things and providing useful services, and the pecuniary, which is about trying to get something for nothing. It is the pecuniary, he argued, that has become the dominant mode of the modern American economy and that recalls his predatory Viking ancestors. But his scorn for pecuniary plunderers was also an inheritance. Where he grew up, the value of honest labor and the denunciation of “idleness, waste, extravagant display, and ill-gotten acquisitions” were drilled into the local children in church and in elementary school, and Camic explains why they would be. The family farm, “which both owns the means of production and provides the labor power to set them in motion,” if not exactly outside the capitalist system, was external enough to generate sharp critique of that system, especially as banks, railroads, and middlemen gradually extended their influence. Robbins (2021) A Theory of Thorstein veblen "],["keynesian-economics.html", "18 Keynesian Economics", " 18 Keynesian Economics Mason A central divide between Keynesian and orthodox macroeconomic theory is the view of the interest rate. Mainstream textbooks teach that the interest rate is the price of saving, balancing consumption today against consumption in the future — a tradeoff that would exist even in a nonmonetary economy. Keynes’ great insight was that the interest rate in a monetary economy has nothing to do with saving but is the price of liquidity, and is fundamentally under the control of the central bank. He looked forward to a day when this rate fall to zero, eliminating the income of the “functionless rentier”. Kenesian Climate Economics As applied to climate policy, this view has several implications. First, market interest rates tell us nothing about any tradeoff between current living standards and action to protect the future climate. Second, there is no reason to think that interest rates must, should or will rise in the future; debt-financed climate investment need not be limited on that basis. Third, while investment in general is not very sensitive to interest rates, an environment of low rates does favor longer-term investment. Fourth, low interest rates are the most reliable way to reduce the debt burdens of the public (and private) sector, which is important to the extent that high debt ratios constrain current spending. Mason (2021) Climate Policy from a Keynesian Perspective "],["marxist.html", "19 Marxist", " 19 Marxist Trigg Abstract There is general agreement amongst scholars of Marx that his monetary theory is incomplete, especially in his most detailed writings on credit in the third volume of Capital. Moreover, in these unfinished notes Marx takes sides with the banking school approach, notable for its opacity compared to the clear axioms of its currency school counterpart. A reconstruction is proposed based on Marx’s step-by-step method, commencing with a critique of Say’s Law under simple commodity circulation, these foundations formalised here using the model of pure labour developed by Pasinetti (1993). Piecing together the fragments, and filling in some of the gaps in Marx’s writings on money, the analysis builds from commodity money and private debt contracts, to the modelling of pure credit and pure banking systems. Adapting the Pasinetti model of a real economy, its endogenous money requirements provide an alternative to the exogenous money approach of the currency school: a streamlined analytical core to the banking school approach, as interpreted by Marx. In addition, the structure of payment crises — as an extension of Marx’s possibility theory of crises — is examined with money as a means of payment required to settle debts between producers and the banking system. Trigg (2021) Reconstructing Marx’s Theory of Credit and Payment Crises under Simple Circulation (pdf) "],["neo-classical-economics.html", "20 Neo Classical Economics 20.1 General Equilibrium 20.2 Free Market 20.3 Human Capital Theory 20.4 Efficiency 20.5 Socialist Alternative to Human Capital Theory 20.6 Production Function 20.7 Keen Critique", " 20 Neo Classical Economics Neoclassicism is a religion dressed as a science Neoclassical economic theory has never resembled a scientific enterprise. It’s simply an ideology presented through an avalanche of mathematics. The underlying assumptions of neoclassical theory all serve to justify the capitalist status quo. When we equate market value with utility, we implicitly assume that individuals’ income indicates their contribution to society. Fix GDP](https://economicsfromthetopdown.com/2019/12/15/why-we-should-abandon-real-gdp-as-a-measure-of-economic-activity/) 20.1 General Equilibrium Ayres In a closed Walrasian model resources are assumed t o be generated by labor and capital. The neo-classical (Walrasian) equilibrium system does not qualify as a dissipative structure. The neoclassical system is, in effect, a perpetual motion machine. Ayres (1988) Self-organisation in Biology and Economics (pdf) 20.2 Free Market Fix According to neoclassical economics, the most efficient way to organize hu- man activity is to use the free market. By stoking self interest, the theory claims, individuals can benefit society. This idea, however, conflicts with the evolution- ary theory of multilevel selection, which proposes that rather than stoke self interest, successful groups must suppress it. Which theory better describes how human societies develop? I seek to an- swer this question by studying the opposite of the market: namely hierarchy. I find evidence that as human societies develop, they turn increasingly to hier- archical organization. Yet they do so, paradoxically, at the same time that the language of free markets becomes more common, and culture becomes more individualistic. This evidence, I argue, contradicts free-market theory, but only if we treat it as a scientific doctrine. If instead we treat free-market theory as an ideology, the pieces come together. Free-market thinking, I speculate, may stoke the forma- tion of hierarchy by cloaking power in the language of ‘freedom’. In this evolutionary context, the theory of free markets is an outlier. It posits that, contrary to what we observe among other social organisms, humans need not suppress self-interest to organize in large groups. And we need not use hierarchical organization. We can build complex societies, the theory claims, using decentralized competition. Treating firms (notindividuals) as the unit of competition legitimizes the firm as an autonomous unit, while leaving the firm’s internal structure as a ‘black box’. By championing firm autonomy, free-market theory may legitimize the firm’s internal chain of command, thereby justifying the accumulation of power. Neoclassical economics may be best treated as a belief system whose existence should be explained using the tools of cultural evolution. Neoclassical theory — claims that outcomes from perfectly competitive markets are ‘optimal’, whereas outcomes from centralized control are ‘inefficient’. It is much like if biologists deemed single-celled organisms to be ‘optimal’, but deemed multicellular organisms ‘inefficient’. Hierarchi - supressing lower-level selection More complex structure is built from simpler components. Growth of complexity involve the centralization of control. Nested hierarchy occurs through a process of evolutionary problem solving. Structures evolve that solve specific problems. Newly created structure serves as the building block to solve new problems. Large, complex organisms are not composed of autonomous units. The growth of complexity involve gradual loss of autonomy among sub-units and the growth of centralized control. As societies become more populous, they add new layers of administrative hierarchy. Centralized control arise for two (related) reasons. First, assembling a larger system from many smaller components requires coordination. Second, there is the problem of the ‘self-interest’ of sub-units. The major evolutionary transitions happened by merging sub-units that were previously autonomous. According to the theory of multilevel selection, this merger is not possible unless the ‘self-interest’ of sub-units is suppressed. The key insight of multilevel selection theory is that high-level organization requires high-level selection that suppresses selection at lower levels. Group-level selection suppresses individual-level selection. Successful groups suppress lower levels of selection by turning to top-down ‘management’. Large-scale organization is accomplished by integrating subunits into a hierarchical control structure. Whether complex organization requires hierarchy is an open question. But it does seem that complexity and hierarchy go hand in hand. Free Market - No Hierarki According to the neoclassical theory of free markets, hierarchy is unnecessary for group organization. Instead, neoclassical theory argues that humans can organize effectively without any form of centralized control. All that is needed is a competitive market. The ‘first fundamental theorem of welfare economics’claims that under conditions of perfect competition (in which all firms are ‘price takers’), markets will allocate resources in a way that is ‘Pareto efficient’ With their welfare theorem in hand, neoclassical economists look at hierarchical organization and see an ‘inefficient’ system. The growth of hierarchy with economic development Fix (2021) Economic Development and the Death of the Free Market (pdf) 20.3 Human Capital Theory The idea of workers as embodied capital Individual capabilities are fundamentally social The sentiment behind eugenics (that some people are far more productive than others) lingers on in mainstream academia. It survives – even thrives – in human capital theory. In the 1950s, economists at the University of Chicago tackled the question of individual income. Why do some people earn more than others? The explanation that these economists settled on was that income resulted from productivity. The claim that income stems from productivity was not new. It dated back to the 19th-century work of John Bates Clark (1899) and Philip Wicksteed (1894), founders of the neoclassical theory of marginal productivity. Clark and Wicksteed, though, were concerned only with the income of social classes. What the Chicago-school economists did was expand productivist theory to individuals. Doing so required inventing a new form of capital. The idea was that individuals’ skills and abilities actually constituted a stock of capital – human capital. This stock made individuals more productive, and hence, earn more income. The idea that skills constituted “human capital” was initially greeted with skepticism. For one thing, the term itself smacked of slavery. (Capital is property, so “human capital” implies human property.) For another, human capital theory overtly justified inequality. It implied that no matter how fat their incomes, the rich always earned what they produced. Any attempt (by the government) to redistribute income would therefore “distort” the natural order. During the 1950s and 1960s, there was little tolerance for such views. It was the era of welfare-state expansion, driven by Keynesian-style thinking. Yes, big government may have been “distorting” the free market – but society seemed all the better for it. Until the 1970s, human capital theory remained obscure. In the 1990s, a second generation of economists took up the human-capital mantle. By then, neoliberal politics was in full swing. The fact that human capital theory explicitly justified inequality was no longer a liability. Today, the fortunes of human capital theory seem to have peaked. We can see the scientific flaws by returning to William Muir’s chicken experiment. I have already told you about his psychopathic chickens, created by breeding the most productive hens. But I have not told you about his alternative trial. In it, he bred the most productive group of chickens. The result was an astonishing increase in egg-laying productivity. The reason this group selection worked is that chickens are social animals. That means productivity is influenced by the social environment. By selecting productive groups, Muir selected for egg-laying ability, but also for sociality. The resulting social hens flourished together. Human capital theory supposes that income stems from productivity, and that this productivity is an isolated trait of the individual. When we expose the realities of power (a social trait), we undermine the legitimacy of the social order. Blair Fix: Human Capital Theory RWER95 (pdf) 20.4 Efficiency Klees Much has been written about the failure of neoclassical economic theory (NCT), so I won’t belabor the point, but I do want to highlight what is too rarely said – that the central concept of NCT, economic (Pareto) efficiency is empty in theory and practice. The great feat of neoclassical economics has been to convince people that there is a vantage point to view society, separable from concerns with equity and distribution. This vantage point, defined as economic efficiency, supposedly allows one to see if society as a whole is better off, such that decisions to produce a particular array of goods and services could be made in the interests of everybody, irrespective of how little one had, thus separating efficiency decisions from equity ones. However, if prices are not defined according to the exact dictates of what economists call “perfect competition,” then private profitability tells us nothing about the comparative social advantages and the consequent “efficiency” of producing, let’s say, more yachts for rich people instead of more rice and beans for poor people. Similarly, to argue that the allocation of resources can be “efficient” even if half the world is starving to death is ridiculous, but that is exactly what neoclassical economics says. I find this legerdemain of inventing a concept of efficiency separate from equity, based on a completely unreal, obviously untrue, abstraction, is absurd on the face of it. If the absurdity of this framework is not obvious, one only has to look at what NCT calls “second best theory.” The “first best” world is that of perfect competition; “second best” refers to a world with at least one “imperfection,” say, one monopoly in a world that was otherwise perfectly competitive. Second-best theory essentially asks: “If we don’t live in the first-best world of perfect competition but have, let’s say, only one imperfection in an otherwise perfect world, what are the results?” It turns out, reluctantly admitted by neoclassical economists – second best is their own theory, not a plot by critics – that with just one imperfection, there are ripples so that all market prices become distorted, and Adam Smith’s famous invisible hand is no longer a good guide to the social interest, and the system is no longer efficient — nor is there even any sense of whether it is close to efficiency. In the real world of multiple imperfections – where none of the assumptions of perfect competition hold – even if the neoclassical concept of efficiency had some meaning in theory, in practice, it is an abysmal failure, a completely empty idea. The implication of my critique of NCT is that if economic efficiency is meaningless, neoclassical economics is useless. There is no reason to stubbornly hang on to NCT and its justification for capitalism in the way that even critical neoclassical economists like Krugman, Reich, Rodrik, and Stigltiz do. Klees (2021) Neoclassical Economics is Dead. What Comes Next? 20.5 Socialist Alternative to Human Capital Theory The liberation of learning from the tyranny of earning. Mehta ‘THE DEATH OF HUMAN CAPITAL?: Its Failed Promise and How to Renew It in an Age of Disruption’, by Phillip Brown, Hugh Lauder, and Sin Yi Cheung, disputes the theory behind one of the strangest features of the past 40 years of neoliberal economics, one rarely tackled so directly. This is human capital theory (HCT), which tends to shift the responsibility for good jobs and wages from business to higher education. At one time, a company that laid off hundreds or thousands of workers would be admitting its managerial failure and incompetence; in the 1980s and ’90s, the mass layoff came to signal instead the kind of decisive cost-cutting that would pump up stock price. Disposing of workers was just the first step. The next was to demand that they embark on a journey of self-improvement. This would make them employable in the “new economy” for the “jobs of the future.” Public officials stopped expecting that firms maintain employment; they wrote tax law that favored companies that sent union jobs overseas. The once and future worker could only be worthy of new jobs if the country’s colleges — two-year and four-year, assisted by a new collection of for-profit colleges and training companies — acquired the proverbial “laser focus” on job-ready skills. HCT, which appealed to conservatives and liberals alike, had become the master paradigm of the “information society” and the “knowledge economy” by the early 1960s. Forerunners of the theory had drawn the interest of some 19th-century economic thinkers, such as John Stuart Mill and Alfred Marshall (who identified what he called “personal capital”), and crucial postwar contributions came from a set of Chicago School economists: Milton Friedman, Theodore W. Schultz, Jacob Mincer, and, most insistently, Gary Becker. HCT was taken up by university presidents like Clark Kerr to explain why universities were at the heart of the postwar economy, where new wealth came from knowledge as human capital and not just physical capital or physical labor. The theory was adopted by New Democrats in the 1980s and ’90s, who liked the claim that knowledge work, in alliance with technology, turbocharged the creation of value and wealth compared to regular industrial labor. In the apparent ebbing of direct colonial extraction, knowledge work was to keep the West on top of the economic food chain, relegating the Global South to manual labor on products conceived and designed in London, Stuttgart, and Cupertino. In his influential 1991 book The Work of Nations, Robert B. Reich (Bill Clinton’s first secretary of labor) synthesized an evolutionary theory of the economy in which production workers in the United States would (and should) be replaced by that more advanced employee, the “symbolic analyst,” who works with the mind on numbers, words, and ideas. HCT made the collective level of education — defined as its fit with advancing technology — the prime mover of contemporary capitalism. Getting this fit between education and technology became a main objective of public policy. HCT emerged near the end of a century of high economic growth — a period during which, as Robert J. Gordon documents in his 2016 book, The Rise and Fall of American Growth, productivity and wages grew mostly because of revolutionary inventions — think electric grids, cars, telephones, and elevators — coupled with Fordist and then New Deal approaches to the distribution of the resulting economic gains. Promoting education principally as human capital is not simply narrow-minded but increasingly dangerous. Using the monetary rewards of education to promote and finance it becomes an increasingly bad idea when those monetary rewards fail to materialize. Having done serious damage to HCT, the authors offer a new kind of human capital theory. This is somewhat confusing, unless we accept that their progressive educational theory, grounded in John Dewey, among others, seeks to combine educational contributions to production with human development rather than eliminating the productivity dimension. The emphasis of the new human capital is on education for personal growth, nurturing a holistic relation between knowing and doing. But if their theory makes personal growth the central goal, it is still a human capital theory, whose focus includes without being limited to the “economic productivity of the human being.” The authors are invoking the capabilities approach rooted in heterodox economics as practiced by Amartya Sen and Martha Nussbaum, and also socially grounded political theory, philosophy, and the psychology of labor and creativity, whose key figures include Hannah Arendt, W. E. B. Du Bois, and C. L. R. James, as well as Aristotle and Karl Marx. The authors are trying to construct a socialist alternative to human capital theory. Replace HCT with an understanding of education and labor as related but distinct modes of human empowerment. Education should focus on the full development of all capabilities of each individual in the whole population. Schooling must be universal, and in the 21st century postsecondary education should be too (though modes and structures will vary widely). Education must develop the whole range of capabilities and not just those with manifest relevance to jobs and wages; capabilities go well beyond the life of homo economicus. Which capabilities to emphasize will vary from person to person: a student who loves history, public policy, or set design should receive systematic education in the deep content and skills of history, journalism, and theater — and not, as now, given a smattering while being advised to be more interested in and better at math, coding, or accounting. Learning should be seen as central to the individual’s entire life and not mainly as an investment in a wage. Individual capabilities are fundamentally social, derived from overall social intelligence and embedded in social relations that mix labor and learning on an ongoing basis. The authors’ new-HCT model would lead to a double transformation. The first is “the liberation of learning from the tyranny of earning.” Business has used HCT to cut education down to its own size, reducing social, cultural, and scientific knowledge that would serve the world in long-range and unpredictable ways. ‘The Death of Human Capital?’ points toward a world beyond human capital theory, which has functioned as a (failed) alternative to industrial policy, impaired equitable social development, and constrained the power of education. Other authors should build on the project under way here. Mehta (2021) A Socialist Alternative to Human Capital Theory? 20.6 Production Function Fix Friedman’s famous ‘F-twist’, in which he argued that a theory’s assumptions are irrelevant. All that matters, Friedman claimed, is that the theory makes accurate predictions. Friedman’s F-twist gets dubious assumptions off the hook. But there is still the problem of predictions. How do you ensure that your theory is consistent with the evidence? Here, neoclassical economists have hit upon a tidy trick: frame your theory in terms of an accounting identity. Since the identity is true by definition, any ‘test’ of the theory will come out in your favor. When neoclassical economists test their theory of income (the theory of marginal productivity), they invoke an accounting identity. They correlate two related forms of income (usually sales and wages) and then call one of the incomes ‘productivity’. Since they always find a correlation, they always ‘confirm’ their theory of income. Nifty! Then there’s the neoclassical theory of economic growth. The theory assumes that economic output is governed by a ‘production function’ that dictates how inputs of capital, labor and ‘technological progress’ are transformed into economic output. And guess what … this approach seems to have overwhelming empirical support. The problem, pointed out by Anwar Shaikh, is that the production function is actually a re-arrangement of a national-accounting identity. The production function ‘works’ because it is true by definition. Nice! Fix (2021) The Truth About Inflation 20.7 Keen Critique Bichler Nitzan Neoclassical economics is the of- ficial scientific underpinning of capitalism as well as its main ideological defence, and according to Keen, it fails in both tasks. Contrary to received opinion, neoclassicism cannot explain capi- talism – either in detail or in the aggregate – and the policies it prescribes do not support but undermine the very system it defends. The book focuses on three key issues: the bizarre neoclassical perspective that money, credit and debt do not matter for the macroeconomy; the neoclassical insistence that the economy’s complex, nonlinear turbulences are best explained in linear, self-equilibrating terms; and the fact that neoclassicists have hijacked the economics of climate change, using patently false assumptions to justify do-nothing policies with untold future consequences. In the neoclassical universe, government is bad business. But the book also has one important limitation: it is about economics. Keen offers to replace neoclassical dogma with a new way of thinking, researching and en- gaging with the economy. And while we agree that neoclassicism is a religion dressed as a sci- ence, in our view, what should come in its stead is not a different type of economics, but a new theory of capitalism more broadly. This isn’t semantic nit-picking. All economic theories – including neoclassicism – engage with non-economic entities and forces. They all agree, willingly or reluctantly, that politics, sociology, anthropology, psychology, international relations and other aspects of society affect the economy. But these effects, whether supportive or distortive, are assumed external to the economy proper. And this assumption is pivotal. Although the effects of these so-called external factors alter economic outcomes, they leave the economic categories themselves intact. And this bifurcation, we argue, is the Achilles’ heel of all economic theories, orthodox and heterodox, old and new. In our view, capitalism is not an economic system, but a conflictual mode of power. Those who rule this mode of power – its dominant capitalists, politicians, mainstream academics, opinion makers and the various organizations they control – make every effort to conceal its power features. This is why neoclassical economics, beholden to its masters, can never be a science. But the problem besieges every and any economic theory that keeps power external to its basic categories. In our opinion, it is only when the study of capitalism substitutes for the narrow understanding of its economy that power can assume centre stage to reveal what economics is structured to conceal. Bichler Nitzan (2021) on Stev Keen’s Manifesto (pdf) "],["neo-liberal-economics.html", "21 Neo-liberal Economics 21.1 Washinton Consensus 21.2 Neoliberalism vs Capitalism", " 21 Neo-liberal Economics Neoliberalism is the ideology that advocates market primacy of social coordination. The neoliberal solution to climate change is to hope that somehow it will become profitable to save the planet. This will not work. (@ExistentialComics) Because neoliberalism has granted markets primacy, and because markets are vulnerable to large-scale runaway loops, neoliberalism is effectively a runaway feedback loop. 21.1 Washinton Consensus John Williamson coined the term “Washington Consensus” to refer to a set of ten economic policies and reforms that received widespread support at the time. These policies included - maintaining fiscal discipline, - reordering public spending priorities (from subsidies to health and education expenditures), - reforming tax policy, - allowing the market to determine interest rates, - maintaining a competitive exchange rate, - liberalizing trade, - permitting inward foreign investment, - privatizing state enterprises, - deregulating barriers to entry and exit, and - securing property rights. Williamson was writing in the context of Latin America as it was emerging from the debt crisis of the 1980s. His list of policies was not proscriptive but descriptive of what he thought various Washington-based institutions, such as the US Treasury, the International Monetary Fund, the World Bank, and various think tanks, agreed would stabilize and restore growth in the region. Williamson’s original conception indicated the general direction in which policy should move, away from a heavily statist approach while retaining an important regulatory role for government. Another version came to represent an extreme market-fundamentalist neoliberal approach that simplified economic policy to “stabilize, liberalize, and privatize” with minimal government, all of which was far from Williamson’s original intent. Critics charged that the Washington Consensus ignored the problems associated with rising inequality and even encouraged the weakening of social safety nets. A series of financial crises—the tequila crisis in 1994–95, the Asian crisis in 1997–98, and the Russian crisis of 1998—further damaged the reputation of Washington Consensus–type policies. Of course, critics of the Washington Consensus usually did not argue that emerging markets should pursue policies of fiscal indiscipline, high inflation, financial repression, trade protectionism, overvalued exchange rates, more nationalization of business, and the like. Rather, they tended to argue that the original list of ten policies was incomplete and that additional policies were needed to improve economic performance. Williamson’s list was also very general, leaving ample room for debate as to how far to go in achieving those policy objectives. But as we mark the thirtieth anniversary of John Williamson’s initial discussion of the Washington Consensus, it is important to recognize that a growing body of recent research suggests that the Consensus has produced tangible benefits while unorthodox populist policies have entailed significant economic costs. A key challenge for policymakers is to ensure that the benefits of economic reform are widely shared so that the divisions that lead to economic populism do not arise and erase those gains. Peterson Imagine writing ‘Washington Consensus really does work’ at the end of a year when central banks bought all bonds issued by fiscal authorities in high income countries, so said fiscal authorities can put safety nets under both capital and labour. Including Nicaragua is problematic to start with: first Ortega regime had to contend with an internal war against paramilitaries (Contras) financed by US in the famous Iran-Contras affair. You’d think the paper would mention that. Guys, the Sandinista Revolution was distracted from growth outcomes by Ronald Reagan bombing them for refusing to say ‘yes uncle, mi patria tu patio’ Ironically, Ortega’s rein since 2007 is far more consistent with the paper’s definition of ‘populism’, but it’s a Washington Consensus, pro-market populism, so of course, we go for the civil war period. An entire subsection on infant mortality under Ortega vs ‘synthetic Nicaragua’ that does not mention the war! Of course, synthetic Nicaragua is one that the US empire doesnt bomb. Gabor (twitter comment) Un thread à lire absolument pour voir le type d’analyse défendu par certains économistes américains universitaires réputés. Où inventer des pays “fictifs” entièrement à sa main est censé permettre de “prouver” le bénéfice de certaines lignes idéologiques. (@NBarreyre) Alves Decades of research have documented the devastating impacts of the Washington Consensus in the developing world. Yet revisionist accounts of this story have emerged in recent years. Remarkable amongst these, a recent blog post by the Peterson Institute for International Economics – “Washington Consensus stands the test of time better than populist policies” – draws on research that is jaw-droppingly ideological and flawed. For decades, mainstream and heterodox economists broadly agreed that the Washington Consensus failed (Stewart 1995, Krueger 2004, Mkandawire 2005). Debt-crisis ridden developing countries that implemented the reforms associated with privatization, liberalisation and deregulation in the 1980s and 1990s tended to see an increase in poverty along with worsening health and educational outcomes. This led to the 1980s and 1990s being dubbed the ‘lost decades’ of development (Easterly 2001) and ultimately paved the way for the the post-Washington Consensus and pro-poor policies (Saad-Filho 2011). But this is about to change. New methods that ‘produce credible counterfactuals in case studies’, turn the conventional wisdom of the Washington Consensus failure on its head (Marrazzo and Terzi 2017, Absher et al. 2020, Grier and Grier 2021). Essentially, the counterfactual approach involves first creating fictitious or synthetic countries, whose policy makers chose the opposite policy trajectory, and then testing whether the Washington Consensus package works better than the alternative. The results, the PIIE blog informs us, stack up for the Washington Consensus: countries adopting WC policies are shown to (eventually) be better off in GDP per capita terms. In contrast, left-wing populists – of the Latin American pedigree – hurt their economies by throwing the Washington Consensus policies out with the neoliberal bathwater. If one unpacks its mechanics carefully, the counterfactual approach turns out to be a thinly-veiled ideological attempt to whitewash the Washington Consensus, to resurrect its key tenets: that minimising the footprint of the state is the right policy choice in health or education, that macroeconomic policy should mean inflation targeting by central banks not active fiscal policy by elected politicians, that state-owned companies are all white elephants in urgent need of privatization, that trade unions harm labour markets. How does one empirically create a fictitious country? The synthetic control method predicts a ‘no Ortega’ growth/infant mortality path by creating a pool of ‘donor’ countries and calibrating their relative contribution to a synthetic Nicaragua such that the pre-Ortega growth or infant mortality path is close to actual Nicaragua. The Washington Counterfactual thus creates a synthetic Nicaragua composed of 23% Chile, 54% Honduras, 9% Mexico, 8% Norway, and 7% the US. Or, to bring a historical touch to the method, synthetic Nicaragua, like a neoliberal Frankenstein, consist of 7% country bombing Nicaragua (US), 54% country used by the CIA/US to bomb Nicaragua (Honduras), 23% country where Washington Consensus was being implemented by Chicago Boys and a military dictatorship (Chile), 8% country never analytically paired with the Washington Consensus (Norway), and Mexico. It is this synthetic Nicaragua where per capita GDP would have been 5.000 USD dollars higher, a Nicaragua that the US empire does not bomb. Revisionist accounts of the Washington Consensus matter because the pandemic has revived the debate around the role that the state should play in the economy. If anything, this is a powerful reminder that all economics is political, however much some hide it behind new or ‘sophisticated’ econometric techniques. Alves on Peterson 21.2 Neoliberalism vs Capitalism Hickel Neoliberalism is not the disease. It is just a symptom of the disease. The disease is capitalism. First, we need to understand what capitalism actually is. Under capitalism, the purpose of production is not primarily to meet human needs. This is no generic economy. Rather, the purpose is to maximize and accumulate profit. That is the core objective. Toward this end, capital seeks to cheapen inputs—labour and nature—as much as possible. For most of its history, capital brutally exploited workers in the core economies, and relied on imperialism to guarantee a study supply of cheap labour and resources in the global South. But this arrangement came under threat after WWII. Labour movements in the core succeeded in winning better wages, better working conditions, and a wide-range of public services: healthcare, housing, education, transit… Meanwhile, in the South, anti-colonial movements overthrew imperialism and began introducing socialist reforms: nationalizing resources, improving wages, building public services, and using tariffs, capital controls and industrial policy to achieve economic sovereignty. This radical turn dramatically improved the lives of working people, North and South. But the new regime of fair wages and resource prices made capital accumulation in the core increasingly untenable, triggering a crisis for elites in the 1970s. As it turns out, capitalism cannot function for very long under conditions of worker justice and decolonization. For capitalists in the core, it was clear that something had to change. The core states faced a choice: either they could accept the fair wages and decolonization, abandon capital accumulation and shift to a post-capitalist economy… or they could attack wages and somehow re-impose the imperial arrangement. They opted hardcore for the latter. At home, they dismantled the unions and shredded public services. They slashed all manner of regulations and protections, in a desperate bid to restore the conditions for capital accumulation. Today we know this as neoliberalism. Neoliberalism was imposed even more brutally across the South, through structural adjustment programs. They reversed the socialist reforms of the anti-colonial era, cut wages and resource prices, and destroyed economic sovereignty… subordinating Southern economies once again. This was not some kind of “mistake”. Not just bad theory. Neoliberalism was imposed in order to restore the conditions for capital accumulation. It was an orchestrated backlash against the successes of the labour movement and the anti-colonial movement. This is why, despite 40 years of data on how destructive neoliberal policies are, we are still stuck in this nightmare. We are stuck because the obvious solution—worker justice, regulation, and economic sovereignty in the South—is inimical to capital accumulation in the core. There is a way out of this nightmare, and that is to abandon capital accumulation as an objective and transition to a post-capitalist economy. Neoliberalism is just a symptom. If we want to advance we need to deal with the underlying structural problem. *steady, that is. We can have a democratic economy organized around meeting human needs at a high standard, where production is socially just and ecologically regenerative. Such a system is possible, but it requires transitioning out of capitalism. Hickel (2022) Twitter thread ThreadReader "],["steady-state-economics-sse.html", "22 Steady State Economics (SSE) 22.1 Herman Daly", " 22 Steady State Economics (SSE) 22.1 Herman Daly Point Policy Summary From “A Steady-State Economy,” by Herman E. Daly School of Public Policy, University of Maryland, College Park MD 20742 USA Cap-auction-trade systems for basic resources. Cap limits to biophysical scale according to source or sin k constraint, whichever is more stringent. Auction captures scarcity rents for equitable redistribution. Tra de allows efficient allocation to highest uses. Ecological tax reform—shift tax base from value added (labor and capital) and on to “that to which value is added”, namely the entropic throughput of resources extracted from nature (depletion), through the economy, and back to nature (pollution). Internalizes external costs as well as raises revenue more equitably. Prices the scarce but previously unpriced contribution of nature. Limit the range of inequality in income distribution—a minimum income and a maximum income. Without aggregate growth poverty reduction requires redistribution. Complete equality is unfair; unlimited inequality is unfair. Seek fair limits to inequality. Free up the length of the working day, week, and year—allow greater option for leisure or personal work. Full-time external employment for all is hard to provide without growth. Re-regulate international commerce—move away from free trade, free capital mobility and globalization, adopt compensating tariffs to protect efficient national policies of cost internalization from standards-lowering competition from other countries. Downgrade the IMF-WB-WTO to something like Keynes’ plan for a multilateral payments clearing union, charging penalty rates on surplus as well as deficit balances— seek balance on current account, avoid large capital transfers and foreign debts. Move to 100% reserve requirements instead of fractional reserve banking. Put control of money supply and seigniorage in hands of the government rather than private banks. Enclose the remaining commons of rival natural capital in public trusts, and price it, while freeing from private enclosure and prices the non rival commonwealth of knowledge and information. Stop treating the scarce as if it were non scarce, and the non scarce as if it were scarce. Stabilize population. Work toward a balance in which births plus immigrants equals deaths plus out-migrants. Reform national accounts—separate GDP into a cost account and a benefits account. Compare them at the margin, stop growing when marginal costs equal marginal benefits. Never add the two accounts. Herman Daly (2008) Steady State Economy (Sustainable Development Commision) (pdf) "],["spatial-economics.html", "23 Spatial Economics 23.1 Urban Economics 23.2 History of Urban Economics 23.3 Regional Economics", " 23 Spatial Economics Agglomoration Economics (Ongoing Research Programme) HSCIF 23.1 Urban Economics When and why did the expertise associated with economics as an academic discipline become so highly valued in the world of public policy? The embedding of agglomerationism within the thinking of policy-makers and governmental institutions provides a fascinating example of a broader shift towards the growing impact of economic expertise, and indeed of individual economists, on policy-making. This focus sits within a wider field of study which is interested in the complex roles that economists have at times played – as public intellectuals, policy experts and academic specialists. How different kinds of analytical tools and a particular style of economic reasoning made their way into the world of elite decision-making is a major theme of interest for many historians and social scientists. So too is the related question of how quantification (testable theoretical hypotheses, measurement technique and indicators, as well as decision-models) has over the last few decades gained ascendancy in policy circles. History of Urban Policy Expertise 23.1.1 Expertise ExpertiseunderPressure What is the role of experts in understanding social change? Expert judgment today is both intensely sought out, across private and public spheres, and also intensely criticised and derided with well-publicised failures to predict various high profile social and natural phenomena. Does the problem lie with the very idea that objective expertise about complex processes is attainable? Or does it stem from the way that expert judgment is developed and communicated? Or, perhaps it reflects the diminished standing of experts and expert knowledge in democratic and pluralistic societies? To explore these questions, we propose three case studies in which expert judgment is both consequential and controversial. They are the UK Government’s emergency response, the use of agglomeration theory in city planning, and deep philosophical controversies about the possibility and objectivity of social science. These cases differ in scope and focus but they enable us to analyse four distinct features of legitimate expertise: sensitivity to temporal scale, translatability in space, ambivalence about precision, and moral responsibility. The overarching goal of the project is to establish a broad framework for understanding what makes expertise authoritative, when experts overreach, and what realistic demands communities should place on experts. CRASSH Expertise under Pressure Programme 23.1.2 Trusting Science Bennett Trust is necessary for many kinds of policy, particularly where that policy requires citizens to comply with rules that come at significant cost, and coercion alone would be ineffective. What is distinctive about our pandemic policies is that they depend not just on public trust in policy, but public trust in the science that we are told informs that policy. When public policy claims to follow the science, citizens are asked not just to believe what they are told by experts, but to follow expert recommendations. While ministers defer to scientists, those same scientists have been eager to point out that their role is exclusively advisory. We are still being asked by the government to trust in recommendations provided by experts, even if the government is not being led by evidence in the way it would have us believe. The communications strategy may not be honest. Public trust in science is both a necessary and desirable feature of an effective public health response to the pandemic. But it is desirable only insofar as it is well placed trust. What makes trust in experts reasonable, when it is? A perceived threat to knowledge about a range of basic facts that most of us don’t have the resources to check for ourselves. If an expert tells me that something is the case this is enough reason for me to believe it too, provided that I have good reason to think that the expert in question has good reason to believe what they tell me. Is it still reasonable to trust science when it doesn’t just provide policy-relevant facts, but leads the policy itself? Knowledge regarding the relevant facts might not reliably indicate ability to reason well about what to do in light of the facts. Well-placed trust in the recommendation of an expert is more demanding than well-placed trust in their factual testimony. A good reason for an expert to think I should do something is not necessarily a good reason for me to do it. This is because what I value and what the expert values can diverge without either of us being in any way mistaken about the facts of our situation. One helpful measure to show the public that a policy does align with their interest is what is something called expressive overdetermination: investing policy with multiple meanings such that it can be accepted from diverse political perspectives. Reform to French abortion law is sometimes cited as an example of this. After decades of disagreement, France adopted a law that made abortion permissible provided the individual has been granted an unreviewable certification of personal emergency. This new policy was sufficiently polyvalent to be acceptable to the most important parties to the debate; A second helpful measure, which complements expressive overdetermination, is to recruit spokespersons that are identifiable to diverse groups as similar to them in political outlook. This is sometimes called identity vouching. The strategy is to convince citizens that the relevant scientific advice, and the policy that follows that advice, is likely not to be a threat to their interests because that same consensus is accepted by those with similar values. Expressive overdetermination and identity vouching are ways of showing the public that a policy is in their interests. Whether they really are successful at building public trust in policy, and more specifically in science-led policy, is a question that needs an empirical answer. What I have tried to show here is that we have good theoretical reasons to think that such additional measures are needed when we are asking the public not just to believe what scientists tell us is the case, but to comply with policy that is led by the best science. Public trust in science comes in at least two very different forms: believing expert testimony, and following expert recommendations. Efforts to build trust in experts would do well to be sensitive to this difference. [Bennett - Trusting the experts take more than belief(Blog Post)]https://hscif.org/trusting-the-experts-takes-more-than-belief/) 23.2 History of Urban Economics Cherrier and Rebours The field of ‘Urban Economics’ is an elusive object. That economic phenomena related to the city might need a distinctive form of analysis was something economists hardly thought about until the early 1960s. In the United States, it took a few simultaneous scholarly articles, a series of urban riots, and the attention of the largest American philanthropies to make this one of the hottest topics in economics. The hype about it was, however, short-lived enough so­­­ that, by the 1980s, urban economics was considered a small, ‘peripheral’ field. It was only through the absorption into a new framework to analyze the location of economic activities – the ‘New Economics Geography’ – in the 1990s that it regained prominence. Understanding the development of urban economics as a field, or last least the variant which originated in the US and later became international, presents a tricky task. This is because the institutional markers of an academic field are difficult to grasp. A joint society with real estate economists was established in 1964, and a standalone one in 2006; a journal was founded in 1974, with an inaugural editorial which stated that: “Urban economics is a diffuse subject, with more ambiguous boundaries than most specialties. Situated within a master-discipline (economics) that is often described as exhibiting an articulated identity, clear boundaries with other sciences and strict hierarchies, urban economics is an outlier. There is, however, one stable and distinctive object that has been associated with the term ‘urban economics’ throughout the 1970s, the 1980s, the 2000s and the 2010s: the Alonso-Muth-Mills model (AMM). It represents a monocentric city where households make trade-offs between land, goods and services, and the commuting costs needed to access the workplace. The price of land decreases with distance from the city center. The model was articulated almost simultaneously in William Alonso’s dissertation, published in 1964, a 1967 article by Edwin B. Mills, and a book by John Muth published in 1969. This trilogy is often considered as a “founding act” of urban economics. Agglomeration In 1956, William Alonso moved from Harvard, where he had completed architecture and urban planning degrees at the University of Pennsylvania. He became Walter Isard’s first graduate student in the newly founded department of “regional science.” He applied a model of agricultural land use developed 150 years earlier by the German economist Johann Von Thünen to a city where all employment is located in a Central Business District. His goal was to understand how the residential land market worked and could be improved. His resulting PhD, Location and Land Use, was completed in 1960. Around that time, young Chicago housing economist Richard Muth spent a snowstorm lockdown thinking about how markets determine land values. The resulting model he developed was expanded to study population density. And a book based on it was published a decade later: Cities and Housing. Drafts of Alonso and Muth’s work reached inventory specialist Edwin Mills in 1966, while he was working at the RAND corporation, and trying to turn models describing growth paths over time into a model explaining distance from an urban center. His “Aggregative Model of Resource Allocation in a Metropolitan Area” was published the next year. This new set of models immediately drew attention from a wide array of transportation economists, engineers and geographers concerned with explaining the size and transformation of cities, why citizens chose to live in centers or suburbs, and how to develop an efficient transportation system. The economists included Raymond Vernon and Edgar Hoover, whose study of New York became the Anatomy of the Metropolis; RAND analyst Ira Lowry, who developed a famous spatial interaction model; spatial and transportation econometrician Martin Beckman, based at Brown; and Harvard’s John Kain, who was then working on his spatial mismatch hypothesis and a simulation approach to model polycentric workplaces. Through the early works of Brian Berry and David Harvey, quantitative urban geographers also engaged with these new urban land use models. But the development of a new generation of models relying on optimization behavior to explain urban location was by no mean sufficient to engender a separate field of economics. Neither Alonso, who saw himself as contributing to an interdisciplinary regional science, nor Muth, involved in Chicago housing policy debates, cared much about its institutionalization. But both were influenced and funded by men who did. Muth acknowledged the influence of Lowdon Wingo, who had authored a land use model. Together with Harvey Perloff, a professor of social sciences at the University of Chicago, they convinced the Washington-based think-thank Resource for the Future to establish a “Committee for Urban Economics” with the help of a grant by the Ford Foundation. The decision was fueled by urbanization and dissatisfaction with the urban renewal programs implemented in the 1950s. Their goal was to “develop a common analytical framework” through the establishment of graduate programs in urban economics. Their agenda was soon boosted by the publication of Jane Jacobs’ The Death and Life of Great American Cities, and by growing policy interest in the problems of congestion, pollution, housing segregation and ghettoization, labor discrimination, slums, crime and local government bankruptcy, and by the stream of housing and transportation acts which were passed in response to these. The Watts riots, followed by the McCone and Kerner commissions, acted as an important catalyst. The Ford Foundation poured more than $ 20 millions into urban chairs, programs and institutes through urban grants awarded to Columbia, Chicago, Harvard and MIT in 1967 and 1970. The first round of funds emphasized “the development of an analytical framework”, and the second sought “a direction for effective action.” As a consequence of this massive investment, virtually every well-known US economist turned to urban topics. At MIT, for instance, Ford’s money was used to set up a two-year “urban policy seminar,” which was attended by more than half of the department.The organizer was welfare theorist Jerome Rothenberg, who had just published a book on the evaluation of urban renewal policies. He was developing a large-scale econometric model of the Boston area with Robert Engle and John Harris, and putting together a reader with his radical colleague Matt Edel. Department chair Carry Brown and Peter Diamond were working on municipal finance. Robert Hall was studying public assistance while Paul Joskow examined urban fire and property insurance. Robert Solow developed a theoretical model of urban congestion, published in a 1972 special issue of the Swedish Journal of Economics, alongside a model by taxation theorist Jim Mirrlees investigating the effect of commuter and housing state tax on land use. Solow’s former student Avinash Dixit published an article modeling a tradeoff between city center economies of scale and commuting congestion costs in another special issue on urban economics in the Bell Journal the next year. A survey of the field was also published in the Journal of Economic Literature, just before the foundation of the Journal of Urban Economics in 1974. Segregation But the publication of a dedicated journal, and growing awareness of the “New Urban Economics” was not the beginning of a breakthrough. It turned out to be the peak of this wave. On the demand side, the growing policy interest and financial support that had fueled this new body of work receded after the election of Richard Nixon and the reorientation of federal policies. On the supply side, the mix of questions, methods and conversations with neighboring scholars that had hitherto characterized urban economics was becoming an impediment. More generally, the 1970s was a period of consolidation for the economics profession. To be considered as bona fide parts of the discipline, applied fields needed to reshape themselves around a theoretical core, usually a few general equilibrium micro-founded workhorse models. Others resisted, but could rely on separate funding streams and policy networks (development and agricultural). Urban economics was stuck. Policy and business interest was directed toward topics like housing, public choice and transportation. And, combined with the growing availability of new microdata, micro-econometrics advances, and the subsequent spread of the personal computer, this resulted in an outpouring of applied research. Computable transportation models and real estate forecasting models were especially fashionable. On the other hand, a theoretical unification was not in sight. Workhorse models of the price of amenities, the demand for housing, or suburban transportation, were proposed by Sherwin Rosen, William Wheaton and Michelle White, among others. But explanations of the size, number, structure and growth of cities were now becoming contested. J. Vernon Henderson developed a general equilibrium theory of urban systems based on the trade-off between external economies and diseconomies of city size, but in these agglomeration effects did not rely on individual behavior. Isard’s former student Masahita Fujita proposed a unified theory of urban land use and city size that combined externalities and the monopolistic competition framework pioneered by Dixit and Joseph Stiglitz, but without making his framework dynamic or relaxing the monocentric hypothesis. At a point when there was growing interest in the phenomenon of business districts – or Edge cities as journalist Joël Garreau called them, this was considered a shortcoming by many economists. General equilibrium modelling was rejected by other contributors, including by figures like Harry Richardson, and a set of radical economists moving closer to urban geographers (such as David Harvey, Doreen Massey and Allen Scott) working with neo-Marxist ideas. Renewal In the 1990s, various trends aimed at explaining the number, size, evolution of cities matured and were confronted to one another. In work which he framed as contributing to the new field of “economic geography,” Krugman aimed to employ his core-periphery model to sustain a unified explanation for the agglomeration of economic activity in space. At Chicago, those economists who had spent most of the 1980s modeling how different types of externalities and increasing returns could help explain growth – among them Robert Lucas, José Scheikman and his student Ed Glaeser – increasingly reflected on Jane Jacob’s claim that cities exist because of the spillover of ideas across industries which they facilitate. Some of them found empirical support for her claim than for the kind within-industry knowledge spillovers Henderson was advocating. Krugman soon worked with Fujita to build a model with labour mobility, trade-offs between economies of scale at the plant level and transportation costs to cities. Their new framework he was adamant to compare to Henderson’s general equilibrium model of systems of cities. He claimed that their framework enabled the derivation of agglomeration from individual behavior and could explain not only city size and structure, but also location. In his review of Krugman and Fujita’s 1999 book with Venables, Glaeser praised the unification of urban, regional and international economics around the microfoundations of agglomeration theory. He also contrasted Krugman’s emphasis upon transportation costs – which were then declining – with other frameworks focusing on people’s own movement, and began to sketch out the research program focused on idea exchanges that he would develop in the next decades. He also insisted on the importance of working out empirically testable hypotheses. The “New Economic Geography” was carried by a newly-minted John Bates Clark medalist who had, from the outset, promised to lift regional, spatial and urban economics from their “peripheral” status through parsimonious, micro-founded, tractable and flexible models. It attracted a new generation of international scholars, for some of whom working on cities was a special case of contributing to spatial economics. In the process, however, olders ties with geographers were severed, and questions that were closely associated with changing cities, like the emergence of the digital age, congestion, inequalities in housing, segregation, the rise of crime and urban riots, became less central to the identity of this field. The field lost some sort of autonomy. Most recently, Glaeser’s insistence that urban models need to be judged by their empirical fit may be again transforming the identity of urban economics. The shift is already visible in the latest volume of the series of Handbooks in Urban and Regional Science. Its editors (Gilles Duranton, Henderson and William Strange) explain that, while its previous volume (2004) was heavily focused on agglomeration theory, this one is “a return to more traditional urban topics.” And the field is now characterised not in terms of a unified, theorical framework, but with reference to a shared empirical epistemology about how to develop causal inferences from spatial data. Overall, the successive shifts in urban economists’ identity and autonomy which we describe here, were sometimes prompted by external pressures (urban crises and policy responses) and sometimes from internal epistemological shifts about what counts as “good economic science.” A key development in the 1970s was the unification around general equilibrium, micro-founded models. It is widely held that the profession is currently experiencing an “applied turn” or a “credibility revolution”, centered on the establishment of causal inference (gold) standards. How this will affect urban economics remains unclear. Cherrier and Rebours 23.2.1 Jane Jacobs Considering her contribution to economic theory may seem counter-intuitive. In addition to lacking academic credentials, she took little interest in engaging the discipline of economics. Her models were neither formal nor developed in reference to existing models. And her view of economic theory in general was dismissive. In the opening chapter of Cities and The Wealth of Nations, “Fool’s paradise,” Jacobs lays out a history of economic thought and arrives at this sweeping conclusion: “Choosing among the existing schools of thought is bootless. We are on our own.” The same dismissive stance extended to academic institutions, as she refused numerous honorary degrees from various Universities. Jacobs Externalities Some economists picked up on her insights. A type of economic externality has been derived from her detailed historical accounts of new economic activities arising from urban diversity. Chicago and Harvard urban economists Glaeser, Kallal, Scheinkman, and Shleifer credited Jacobs in 1992 for identifying cross-industry knowledge transfers, which they dubbed “Jacobs externalities.” The concept was based on Jacobs’ The Economy of Cities and posits that knowledge transfer occur between different industries, and that local competition supports economic growth. This came four years after future Nobel prize recipient Robert Lucas pointed to Jacobs’ work while investigating the external effects of human capital in his 1988 article On the Mechanics of Economic Development, although without formalizing his insight. Lucas’ endorsement earned Jacobs increasing recognition among economists over the following decades. Paul Krugman described her as a “patron saint of the new growth theory” and her unusual status was summed up by Robert Dimand and Robert Koehn who saw her as “her own distinctive kind of political economist … an exceptional instance of a woman without academic affiliation or university training achieving recognition among leading academic economists”. And a considerable literature grew up after Glaeser et al.’s piece. Despite this interest in her work, extended reassessments of her contribution to economic thought have yet to appear. The city economy model, first developed in The Economy of Cities,argues that the desirable diversification of local economic activities depends largely on the destination of goods and services entering the city’s economy. The key claim is that imports are key to economic development: they embody knowledge and allow further diversifications in the local economy, as imports are gradually replaced by local supply, and make “room” for new imports – in a similar manner to import substitution. Jacobs uses this model to stress the long-term undesirability of overspecialization derived from a focus on maximizing exports, and the importance of a large and diverse local economy – ultimately delivering a critique of comparative advantages as an organizing principle of trade. The more niches that are filled in a given natural ecology, other things being equal, the more efficiently it uses the energy it has at its disposal … That is another way of saying that economies producing diversely and amply for their own people and producers, as well as for others, are better off than specialized economies … The most elaborate study of Jacobs’ use of biological and ecological analogies is provided in mathematician and philosopher David Ellerman’s paper How Do We Grow? Jane Jacobs on Diversification and Specialization (2005). Depicting the city economy’s boundaries as an open system governed by evolutionary dynamics: “development is a conceptualized form of social learning.” Incoming goods, the products of foreign know-how, are vectors of developmental learning. And exports of commodities and services fund these imports. When imports feed into the somewhat enclaved export economy (i.e. overspecialized), they have a lesser effect then when they are dissipated in local consumption. Following Geoffrey Hodgson’s taxonomy in Economics and Evolution (1993), part of Jacobs’ system could be characterized as phylogenetic and non-consummatory, that is, as exhibiting an open-ended process of evolutionary selection among a population of firms and individuals. Jacobs targeted development schemes developed by the World Bank. She pointed to the inherent weaknesses of Robert McNamara’s development strategies for addressing “basic human needs” (literacy, nutrition, reduction in infant mortality, and health) of poor populations. She argued that because economic development is a process, it cannot be thought of as a “collection of things” which can be bought or provided. The “basic human needs approach” ignored the necessity for solvent markets to support increased agricultural yields and the populations that were being displaced. As they could no longer rely on agricultural work to sustain themselves, displaced workers failed to find jobs in nearby city economies, where labor markets had not evolved alongside the increased agricultural yields through a succession of appropriate feedback mechanisms triggering the needed corrections. And she made the same argument against technology transfers in the “Green Revolution” of the 1960s and 1970s. The mechanism of feedback relationships is one example among others of Jacobs’ usage of systemic concepts to draw boundaries around the city economy as a system and elaborate on its behavior. Further examination of Jacobs’ use of these concepts within the paradigm she adopted may reveal a consistent link between her analysis of cities as economic units and the policies she is tended to critique. In short, future attempts at more comprehensive interpretations of Jacobs’ economic thought might benefit from stepping away from the urban focus of The Death and Life of Great American Cities while considering more carefully her later economic writings. Divry on Jacobs 23.3 Regional Economics Rebours The history of regional science offers an interesting case study, as well as a one of the few examples, of the institutionalization of an entirely new scientific field in the years after 1945. Its foundation by Walter Isard and a group of social scientists in the 1950s represents the most institutionalized attempt to stimulate the relationship between economics and geography. The original project of Isard, who was trained as an economist at Harvard, was to promote the study of location and regional problems. And at the outset, regional science was, in various ways, a success. It attracted many scholars from different disciplines, mostly economics, geography and urban/regional planning, and it quickly became institutionalized formally through the foundation of the Regional Science Association (RSA) in 1954 and establishment of a Regional Science Department at the University of Pennsylvania in 1958. At the same time, the creation of the Papers and Proceedings of The Regional Science Association in 1955 and of the Journal of Regional Science in 1958, offered new publication venues for scholars interested in location analysis, in particular quantitative geographers who found it difficult to publish in traditional geography journals. Within economics, regional science influenced analytical works in urban economics, as, for instance, William Alonso’s thesis, widely recognized as one of the foundational works of urban economics, was written at Penn under the supervision of Isard in 1960. However, the prevailing processes of knowledge production and evaluation which shaped the emergence of this new field were deeply influenced by economics. Geographers became dissatisfied with Isard’s vision of the hierarchical division between geographers and economists, and the primacy given to economic theorizing and modelling as the core of the new regional science. Thus, the social organization of the field of regional science and its interactions with other disciplines mirrored the particularity of economics, a hierarchical discipline organized around a strong theoretical core and an insularity from the rest of social sciences. In the late 1940s, Isard became increasingly concerned about the lack of interest among economists in the location of economic activities. His perception of the subject was not really different to his colleagues, but he wanted to improve the theory they used, which, following the British tradition of the late 19th century, suffered from a lack of spatial dimension. He did not seek to challenge the general equilibrium economic theory that was becoming dominant, but sought instead to integrate a spatial aspect within it. In 1949 Isard was recruited to Harvard by Wassily Leontief to develop an input-output approach to regional development. During the war, input-output analysis received much attention because it enabled the American Air Force to identify the best targets for bombing. As a consequence, Leontief had received large research funds to develop his input-output framework. Isard expressed a hierarchical division between economists, who provided the analytical foundations of regional science, and the geographers, who provided the empirical facts and testing. While, the identity of economics was legitimated and reinforced by its success during the war, in geography, there was an increasing dissatisfaction with the regional geography approach that dominated the field in the1950s. The Cold War context facilitated the promotion of a new generation of quantitative geographers looking for more scientific methods. By the mid-1970s, regional science experienced a progressive decline when geographers started to distance themselves from the analytical methods that were promoted by Isard. But even after the Regional Science Department at Penn closed its doors in 1993, regional science journals remained a going concern and continued to promote studies of spatial issues notably from urban economics and, after 1991, New Economic Geography. Rebours "],["ecological-economics.html", "24 Ecological Economics 24.1 Natural Resources and Energy", " 24 Ecological Economics Its challenge would not be to blend the different domains of study under the same mindset – combining plants and profits in a single analysis – but to train students to see in complementary, but conflicting, ways. While ‘environmental economists’ argue that we can easily correct markets by pricing carbon emissions and other pollutants – no matter that we barely have, in practice – the larger issue is that many of our ecological challenges are not amenable to a commodification ‘fix’, which relies on treating the environment as parts. The issue comes to a head in the question of whether we should impute dollar values for ‘ecosystem services’ – to put a price on the Amazon rainforest, say. The question is not whether we can impute such values, but rather whether it is intelligent to do so. In this critical matter, which has divided ecologists, is the issue of whether ecology should yield to a dominant economic way of thinking or make a stand for its different way of seeing – a different way of appreciating and valuing – that challenges economics’ monetary default. The pragmatic view has been to impute monetary values because we cannot afford for ecosystems to be valued at zero. [DH: rather - at infinite! nature is Holy!!] Indeed, when such estimates are made, they reveal that the ‘value’ of global ecosystem services dwarfs global GDP! Market measures of value miss more than they grasp. Sustainable business is confronting the fact it does not constitute ‘ecological’ thinking but rather the appropriation of some ecological concerns into a framework that remains steadfastly economic. The scale and stubbornness of major problems simply may not yield to a more-of-the-same technological fixing mentality, but instead require deeper cultural rebalancing, through a ground-up awakening. The broader vision of the ecologist has room to understand the role the economist plays, but the economist – and the businessman and the investor – do not seem to know that they need the ecologist. Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis Parrique Let me introduce Romanian-American mathematician and economist Nicholas Georgescu-Roegen (1906–1994) who, at the beginning of the 1970s, laid out one theory so disruptive that it led to the creation of a new school of economic thought: ecological economics. His main idea, exposed in The Entropy Law and the Economic Process (1971), was that economic organization is a continuation of biological organization. Why? Because all machines are necessarily made of materials and use energy, and because all labour involves our biological bodies, which are also made of materials and use energy. The economy is — unavoidably — a bioeconomy, which means it is a subsystem of the larger finite and non-growing ecosystem that is the Earth. The logical conclusion becomes inevitable: nature holds non-negotiable market power and humans can only use whatever nature supplies. This also means that the prosperity of the economy is fundamentally linked to that of ecology. In the same way that a healthy organ cannot thrive for long in a dying body, an economy will not prosper within a collapsing biosphere (or at least not for long). In terms of manufacturing, this means that certain factors of production are non-substitutable. Any human-made artefact is necessarily made out of natural resources such as materials and energy and so therefore cannot be a true substitute to it. “One cannot build the same wooden house with half the timber no matter how many saws and carpenters one tries to substitute,” wrote Herman Daly (another economist who has laid out a deep theory to explain why infinite growth is an ecological impossibility). Regardless of how ingenious you are and the budget of your R&amp;D department, you will not be able to build a wooden house without wood. If all economic activities require energy and materials, it means economic practices are unavoidably entropic (the second law of thermodynamics), which means they neither create nor destroy matter or energy but only transform it from a higher to a lowerquality. Consider this an inescapable law of diminishing returns applied to the economy as a whole. You can produce more for a time, and produce more efficiently to be able to keep producing for a longer period of time, but you cannot keep increasing production forever. This is because all of the materials and energy we use come from a nature that is fundamentally finite in its ability to provide resources and assimilate waste. What kind of theory do green growth advocates offer in opposition to that? Well, not much, in my opinion. The core assumption of modern mainstream economics comes from a 1974 paper from American economist Robert Solow where he integrated natural resources as an input into the neoclassical production function while assuming its perfect substitutability with human-made capital. “If it is very easy to substitute other factors for natural resources,” Solow writes, “the world can, in effect, get along without natural resources.” Now, economists who think this makes sense should spend a bit more time in their garden, realising that it is not “very easy” (or even possible at all) to substitute other factors for natural resources (good luck growing food with a high-tech, smart shovel but without soil, bees, and water). So now, which theory should we choose? Should we trust experts who have developed their entire school of economics since the 1980s on the very question of how economy interacts with ecology, or should we rather ask a random neoclassical economist what they think on a matter they have only studied peripherally? I love both Nicholas Georgescu-Roegen and Robert Solow for different reasons, but picking Solow to understand the relation between growth and the environment would be like picking Zlatan Ibrahimović to play tennis — not the wisest pick. The current hype for green growth is scientifically ungrounded, both empirically and theoretically. Parrique (2022) Degrwoth is Good Economics Parrique Home (pdf) 24.1 Natural Resources and Energy Garzon The economist Georgescu-Rogen (2007) was one of the first to warn of the serious deficiencies in traditional ways of thinking about the economy. In particular, he highlighted the gap in economic models regarding the consumption of energy and materials. Both components restrict the possibilities of economic growth in ways that economics had ignored until just a few years ago[1]. In fact, planet Earth is a closed system of materials so that, aside from the very exceptional arrival of a meteorite or the removal of a human artefact, neither of which are significant in quantitative terms, the mass of materials is always the same. In the case of energy, planet Earth is an open system inasmuch as we receive energy flows from solar radiation, but even then, the laws of physics impose limits on energy use. Every human process involves use of a series of energy sources governed by the laws of physics, particularly the laws of thermodynamics. The second principle of thermodynamics establishes that the quality of energy usable by human beings is decreasing and that, in converting energy (for example, converting the energy deriving from solar radiation to photosynthesis or generating electricity through photovoltaic panels), it is not possible to maintain 100% of the available energy. Much of the energy is dissipated as heat, so that conversion presupposes the transformation of high-quality, low-entropy energy, such as carbon, into low-quality, high-entropy energy such as heat. The history of technological development is the history of a constant struggle to improve the energy efficiency of such conversions. Flows of materials and flows of energy can be understood as two distinct aspects of the same process. In fact, a continuous flow of materials is only possible if there is a continuous flow of energy at the same time. In addition, these two restrictions on economic growth interact in very diverse ways and the ecological pressure and impact of productive activity also show up in the alteration of geochemical cycles. There is no doubt that human beings have lived on Earth for at least two hundred thousand years, although most of the time they did so in hunter-gatherer social groups. The end of the last ice age, which occurred some twenty thousand years ago, gave way to an extraordinarily warm climate which, in its turn, enabled human beings to develop new economic and social practices, such as agriculture (developed some 12,000 years ago). Scientists have agreed to call this warm era the Holocene, in which current civilizations developed. Planetary Boundaries and Eco-Social Crisis One of the main problems with the planetary boundaries’ framework, however, is that it looks at social metabolism in an essentially technical way. If the analysis is not broadened, the framework seems to place responsibility on abstract notions such as «humanity» or «the human being», when it is obvious that neither the causes nor the consequences of the ecological impact are symmetrically distributed either across the class structure or between the different geographical regions. There is in fact no global ecological crisis which means the same for all human beings (Brand et al., 2021). Therefore it is much more appropriate to talk of an eco-social crisis, because this helps to highlight the importance of socio-political relationships when assessing environmental degradation processes and seeking solutions. Garzon (2022) The limits to growth: eco-socialism or barbarism "],["econophysics.html", "25 Econophysics 25.1 Economy as dissipative system", " 25 Econophysics Blair Fix Econophysics is an attempt to understand economic phenomena (like the distribution of income) using the tools of statistical mechanics. The particle model of physics demonstrates how a seemingly equal process (the random exchange of energy) can give rise to wide inequalities. If econophycisists are correct, this model tells us why human societies are mired by inequality. It’s just basic thermodynamics. The idea required a leap of faith: treat humans like gas particles. Econophysicists highlighted an interesting parallel. When humans exchange money, it is similar to when gas particles exchange energy. One party leaves with more money/energy, the other party leaves with less. With the parallel between energy and money, ecophysicists arrived at a startling conclusion. Their models showed that when humans exchange money, inequality is inevitable. When econophysicists use ‘random exchange’ to explain income, many people are horrified by the lack of causality. To understand the behavior of large groups of particles, Boltzmann was forced to use the mathematics of probability. The resulting uncertainty in cause and effect made him uneasy. Quantum mechanics would later show that at the deepest level, nature is uncertain. But this quantum surprise does not mean that probability and determinism are always incompatible. In many cases, the use of probability is just a ‘hack’. It is a way to simplify a deterministic system that is otherwise too difficult to model. Like a coin toss, econophysicists think we can treat monetary exchange in probabilistic terms. Econophysicists think we can model the exchange of money without understanding property transactions. Blair Fix Garrett Abstract Climate change is a two-way street during the Anthropocene: civilization depends upon a favorable climate at the same time that it modiﬁes it. Yet studies that forecast economic growth employ fundamentally diﬀerent equations and assumptions than those used to model Earth’s physical, chemical, and biological processes. In the interest of establishing a common theoretical framework, this article treats humanity like any other physical process; that is, as an open, nonequilibrium thermodynamic system that sustains existing circulations and furthers its material growth through the consumption and dissipation of energy. The link of physical to economic quantities comes from a prior result that establishes a ﬁxed rela- tionship between rates of global energy consumption and a historical accumulation of global economic wealth. What follows are nonequilibrium prognostic expressions for how wealth, energy consumption, and the Gross World Product (GWP) grow with time. This paper shows that the key components that determine whether civilization “innovates” itself toward faster economic growth include energy reserve discovery, improvements to human and infrastructure longevity, and reductions in the amount of energy required to extract raw materials. Growth slows due to a combination of prior growth, energy reserve depletion, and a “fraying” of civilization networks due to natural disasters. Theoretical and numerical argu- ments suggest that when growth rates approach zero, civilization becomes fragile to such externalities as natural disasters, and is at risk is for an accelerating collapse. Linking physical to economic quantities comes from a ﬁxed relationship between rates of global energy consumption and historical accumulation of global economic wealth. When growth rates approach zero, civilization becomes fragile to externalities, such as natural disasters, and is at risk for accel- erating collapse. Garrett Memo As with any other natural system, civilization is composed of matter. Internal circulations are maintained by a dissipation of potential energy. Oil, coal, and other fuels “heat” civilization to raise the potential of its internal components. Dissipative frictional, resistive, radiative, and viscous forces return the potential of civilization to its initial state, ready for the next cycle of energy consumption. The material growth and decay of civilization networks is driven by a long-run imbalance between energy consumption and dissipation. Treating civilization as a dissipative physical system like any other on our planet. Garrett Summary This paper has presented a physical basis for interpreting and forecasting global civilization growth, with the intent that it might be used to develop a consistent theoretical basis for forecasting interactions between humanity and climate during the Anthropocene. The perspective is that, like a living organism [Vermeij, 2009], energy consumption and dissipation drives material ﬂows to civilization. If there is a net convergence of matter within civilization, then civilization grows. Growth increases the availability of new and existing reserves of matter and energy, and this leads to a positive feedback loop that allows growth to persist or even accelerate. These rather general thermodynamic results can be expressed in purely economic terms because there appears to be a ﬁxed link between global rates of primary energy consumption and a very general expres- sion of human wealth: 𝜆 = 7.1 ± 0.1 Watts of primary energy consumption is required to sustain each $1000 of civilization value, adjusting for inﬂation to the year 2005 (see supporting information and Garrett [2012a]). It was argued that wealth does not rest in inert “physical capital”, as in traditional treatments. Rather, wealth can be interpreted to include all aspects of civilization, even the purely social. Value lies in the density of a network of connections between civilization elements, insofar as this network contributes to a global scale consumption and dissipation of energy (equation (41)). Global economic production Y is positive when consumption exceeds dissipation, and there is a net diﬀusion of matter to civilization that grows its size. This leads to an economic growth model for wealth C and economic production Y that is more simple, physical, and dimensionally self-consistent than mainstream models: dC = Y dt (70) Y = 𝜂C (71) where Y is directly proportional to a lengthening of civilization’s networks and growth of its energy reserves. The real rate of return on wealth 𝜂 is somewhat analogous to the total factor productivity in traditional models. Prognostic expressions for 𝜂 presented here show that its value is determined by a combination of rates of civilization decay, the quantity of available energy reserves, the amount of energy required to incorporate raw materials into civilization’s structure, and the accumulated size of civilization due to past raw material ﬂux convergence. Current values of the rate of return can be inferred from equation (71). For example, current global rates of return are about 2.2% per year [Garrett, 2012a]. Trends in 𝜂 can be forecast based on estimates of future decay and rates of raw material and energy reserve discovery (equation (56)). Thus, this paper oﬀers a set of prognostic expressions for the growth of civilization, expressible in economic and energetic terms that can be linked to physically measurable quantities. The implications that have been described are summarized as follows: - Civilization inﬂation-adjusted wealth is sustained by global energy consumption and grows only as fast. - Some combination of price inﬂation and unemployment is related to rates of civilization decay. - Rates of return on wealth decline in response to accelerated decay or increased resource scarcity. - Rapid rates of current growth act as a drag on future rates of growth. - Rates of return grow when there is “innovation” through technological change. - The GWP grows when energy consumption grows super-exponentially (at an accelerating rate), or when global energy reserve discovery exceeds depletion. - If growth rates of wealth approach zero, civilization becomes fragile with respect to externally forced decay. This appears to be particularly true if prior growth was super-exponential. Many of these conclusions might seem intuitive, or as if they have been expressed already by others within more traditional economic perspectives. What is novel in this study is the expression of the eco- nomic system within a deterministic thermodynamic framework where a very wide variety of economic behaviors are derived from only a bare minimum of ﬁrst principles. More importantly, a suﬃcient set of statistics exists for global economic productivity, inﬂation, energy consumption, raw material extraction and energy reserve discovery that the nonequilibrium solutions presented here can be evaluated and falsiﬁed with no requirement for any a priori tuning or ﬁtting to historical data. Such evaluation will be addressed in Part II. Speciﬁcally, it will be shown that the logistic equation given by equation (64) closely matches the evolution of global economic rates of return since 1950, allowing for observed rates of technological change deﬁned by equation (56). Logistic behavior has been recognized in the evolution of human empires throughout history. It will be shown to be evident in global rates of economic growth as well. Global civilization has enjoyed explosive growth since the industrial revolution, but it is unclear how long this can be sustained when it is facing ongoing resource depletion, pollution, and climate change. Global economic wealth is tied to energy consumption, and energy consumption through combustion is tied to carbon dioxide emissions. Without a suﬃciently rapid switch to noncarbon sources of energy, growing wealth is necessarily linked to growing emissions. Yet accumulating carbon dioxide in the atmosphere is also likely to drive accelerating civilization decay through ampliﬁed hydrological extremes, storm intensiﬁcation, sea level rise, and mammalian heat stress. The prognostic expressions that have been derived here might be useful to help guide a physically plausible range of future timelines for civilization growth and decay, particularly in models that couple human and climate systems during the Anthropocene. Garrett (2014) Long-run evolution of the global economy:1. Physical basis (pdf) 25.1 Economy as dissipative system Ayres In a closed Walrasian model resources are assumed t o be generated by labor and capital. The neo-classical (Walrasian) equilibrium system does not qualify as a dissipative structure. The neoclassical system is, in effect, a per- petual motion machine. This fact was emphatically pointed out by the Nobel prize-winning chemist F. Soddy in 1922 (Daly, 1980), but Soddy’s work was vir- tually ignored by economists. The first economists to stress the dissipative nature of the economic system were Boulding (1966) and Georgescu-Roegen (1971). The relevance of mass and energy conservation to environmental- resource economics was first emphasized by Kneese et al. (1970). In reality, resource inputs originate outside the economic system per se: they include air, water, sunlight and material substances, fuels, food, and fiber crops, all of which embody free energy or available work. The economic system, in reality, is absolutely dependent on a continu- ing flow of free energy from the environment. Evidently, the real economic system looks very much like a self-organizing dissipative structure in Prigogine’s sense: it is dependent on a continuous flow of free energy (the sun or fossil fuels), and it exhibits coherent, orderly behavior. Moreover, like living organisms, it embodies structural information as morpho- logical differentiation and functional specialization. Since the economy is, by assumption, a dissipative structure, it depends on a continuous flow of free energy and materials from and to the environ- ment. Such links are precluded by closed neoclassical general equilibrium models, either static or quasi-static. The energy and physical materials inputs to the economy have shifted over the past two centuries from mainly renewable to mainly nonrenewable sources. Dynamic economic growth is driven by technological change (generated, in turn, by economic forces), which also results in continuous structural change in the economic system. For instance, so-called Leontief input- output coefficients do not remain constant. It follows, incidentally, that a long-term survival path must sooner or later reverse the historical shift away from renewable resources. This will only be feasible if human technological capabilities continue to rise to levels much higher than current ones [8]. But, since technological capability is itself an output of the economic system, it will continue to increase if, and only if, deliberate investment in R&amp;D is continued or even increased. In short, the role of knowledge-generating activity in retarding global entropy seems to be growing in importance. The economic system is not necessarily stable against all pertur- bations, and the more it is intentionally managed to optimize growth, the more it becomes vulnerable to the consequences of human error. Ayres (1988) Self-organisation in Biology and Economics (pdf) Shiozawa Many protests and contestations have been voiced out against equilibrium theory. Some argued that it neglects the increasing returns to scale which underlies in the development of modern industries. Others contested the maximizing principle which is always supposed in the formulation of economic behaviors, both for consumers and producers. In 1970’s, many eminent economists criticized the state of the art of economic science and proposed to abandon a equilibrium analysis. But, this has not been done, partly for lack of new framework and partly for fear of us loosing ready made formulae for economic behaviors. New image of systems theory is requested and I think this new image should be the notion of “dissipative structure”. Professor Prigogine, in his early days of his research, was interested in non-equilibrium phenomena and remarked to the dissipative structure, which appears both in space and time. The importance of dissipative structure is evident, if one once knows that any living systems and subsystems are far from equilibrium but that they are all dissipative structure. Most simple example of dissipative structure is given as the flame of a candle. Once lid, a candle continues to burn unless all wax is consumed or the oxygen is exhausted. Dissipative structure sometimes takes the form of stationary state but it is very different from equilibrium. The latter is sensitive to boundary conditions. The concept of dissipative structure is important for economics, because it makes possible to have new idea how economic system works. In the equilibrium framework, boundary conditions are imposed as constraints of the system. In the dissipative framework, boundary conditions are not directly relected to the speed of the consumptions or the extent of employment. It is instead the internal structure which determines volumes and speeds of economic quantities. Most simple example is the extent of cultivated field. When there is a large surface of cultivable field and there is relatively small population, it is easy to see that whole surface is not necessarily cultivated. Some part which can be cultivated by the population will be cultivated effectively. Keynes was the first person to realize that, in economy, it is not the boundary condition or the amount of resources which determines how much of the resources are used. 50 years have passed after Keynes went to other world. During these years, many efforts had been made, in vain, to harmonize Keynesian macroeconomic theory with the neo-classical micro-economics. This is a natural outcome. The micro-economics, which is based on equilibrium framework, denies the existence of internal structure such as dissipative structure. Unless we are emancipated from the framework of general equilibrium, there will be no breakthrough for a new economics. If the problem is only the existence of internal structure, the economics system can be characterized as self-organizing system. But, the economy is not only a self-organizing system. Viewed as an ecological system, it is a system which constantly brings resources in and cast waste off. Economic activities are based on the constant flow of energy and materials. So the economy is also a dissipative structure. The proper difficulty of the economics is that the complexity is the real condition for the economic agents. This is not true for physical and chemical sciences. If we consider the boundednes of our rationality, it becomes rather evident that our behavior is not directed by a decision made once for all. It is a continuous sequence of adaptive adjustments, which will be organized according to rough program of purpose pursuit. Consequently, the theoretical framework of the economics should be reorganized as process analysis. Equilibrium analysis has been the obstruction for the economics to proceed to this old but still new direction. Shiozawa (1996) Economy as a Dissipative Structure (pdf) "],["complexity-economics.html", "26 Complexity Economics", " 26 Complexity Economics The discovery that higher order phenomena cannot be directly extrapolated from lower order systems is a commonplace conclusion in genuine sciences today: it’s known as the “emergence” issue in complex systems (Nicolis and Prigogine, 1971, Ramos-Martin, 2003). The dominant characteristics of a complex system come from the interactions between its entities, rather than from the properties of a single entity considered in isolation. The fallacy in the belief that higher level phenomena (like macroeconomics) had to be, or even could be, derived from lower level phenomena (like microeconomics) was pointed out clearly in 1972—again, before Lucas wrote—by the Physics Nobel Laureate Philip Anderson: The main fallacy in this kind of thinking is that the reductionist hypothesis does not by any means imply a “constructionist” one: The ability to reduce everything to simple fundamental laws does not imply the ability to start from those laws and reconstruct the universe. (Anderson, 1972, p. 393) The impossibility of taking a “constructionist” approach to macroeconomics, as Anderson described it, means that if we are to derive a decent macroeconomics, we have to start at the level of the macroeconomy itself. This is the approach of complex systems theorists: to work from the structure of the system they are analysing, since this structure, properly laid out, will contain the interactions between the system’s entities that give it its dominant characteristics. Neoclassical macroeconomists have tried to derive macroeconomics from the wrong end—that of the individual rather than the economy—and have done so in a way that glossed over the aggregation problems that entails by pretending that an isolated individual can be scaled up to the aggregate level. It is certainly sounder—and may well be easier—to proceed in the reverse direction, by starting from aggregate statements that are true by definition, and then by disaggregating those when more detail is required. Using these definitions, it is possible to develop, from first principles that no macroeconomist can dispute, a model that does four things that no DSGE model can do: it generates endogenous cycles; it reproduces the tendency to crisis that Minsky argued was endemic to capitalism; it explains the growth of inequality over the last 50 years; and it implies that the crisis will be preceded, as it indeed was, by a “Great Moderation” in employment and inflation. The three core definitions from which a rudimentary macro-founded macroeconomic model can be derived are the employment rate (the ratio of those with a job to total population, as an indicator of both the level of economic activity and the bargaining power of workers), the wages share of output (the ratio of wages to GDP, as an indicator of the distribution of income), and, as Minsky insisted, the private debt to GDP ratio. A simple model can explain most of the behaviour of a complex system, because most of its complexity come from the fact that its components interact—and not from the well-specified behaviour of the individual components themselves So the simplest possible relationships may still reveal the core properties of the dynamic system—which in this case is the economy itself. Even at this simple level, its behaviour is far more complex than even the most advanced DSGE model, for at least three reasons. Firstly, the relationships between variables in this model aren’t constrained to be simply additive, as they are in the vast majority of DSGE models: changes in one variable can therefore compound changes in another, leading to changes in trends that a linear DSGE model cannot capture. Secondly, non-equilibrium behaviour isn’t ruled out by assumption, as in DSGE models: the entire range of outcomes that can happen is considered, and not just those that are either compatible with or lead towards equilibrium. Thirdly, the finance sector, which is ignored in DSGE models (or at best treated merely as a source of “frictions” that slow down the convergence to equilibrium), is included in a simple but fundamental way in this model, by the empirically confirmed assumption that investment in excess of profits is debt-financed With a higher propensity to invest comes the debt-driven crisis that Minsky predicted, and which we experienced in 2008. However, something that Minsky did not predict, but which did happen in the real world, also occurs in this model: the crisis is preceded by a period of apparent economic tranquillity that superficially looks the same as the transition to equilibrium in the good outcome. Before the crisis begins, there is a period of diminishing volatility in unemployment. The difference between the good and bad outcomes is the factor Minsky insisted was crucial to understanding capitalism, but which is absent from mainstream DSGE models: the level of private debt. It stabilizes at a low level in the good outcome, but reaches a high level and does not stabilize in the bad outcome. The model produces another prediction which has also become an empirical given: rising inequality. Workers’ share of GDP falls as the debt ratio rises, even though in this simple model, workers do no borrowing at all. If the debt ratio stabilises, then inequality stabilises too, as income shares reach positive equilibrium values. But if the debt ratio continues rising—as it does with a higher propensity to invest—then inequality keeps rising as well. Rising inequality is therefore not merely a “bad thing” in this model: it is also a prelude to a crisis. The dynamics of rising inequality are more obvious in the next stage in the model’s development, which introduces prices and variable nominal interest rates. As debt rises over a number of cycles, a rising share going to bankers is offset by a smaller share going to workers, so that the capitalists share fluctuates but remains relatively constant over time. However, as wages and inflation are driven down, the compounding of debt ultimately overwhelms falling wages, and profit share collapses. Steve Keen "],["functional-finance.html", "27 Functional Finance", " 27 Functional Finance Functional finance is a heterodox macroeconomic theory developed by Abba Lerner during World War II that seeks to eliminate economic insecurity (i.e., the business cycle) through government intervention in the economy. Functional finance emphasizes the result of interventionist policies on the economy. It actively promotes government deficit spending as an effective way of reducing unemployment. Functional finance is based on three major beliefs: It is the role of government to stave off inflation and unemployment by controlling consumer spending through the raising and lowering of taxes. The purpose of government borrowing and lending is to control interest rates, investment levels, and inflation. The government should print, hoard or destroy money as it sees fit to achieve these goals. Functional finance also says that the sole purpose of taxation is to control consumer spending because the government can pay its expenses and debts by printing money. Furthermore, Lerner’s theory does not believe it is necessary for governments to balance their budgets. Lerner was a follower of the extremely influential economist John Maynard Keynes and helped to develop and popularize some of his ideas. Keynesian economics embraced the concept that optimal economic performance could be achieved by using economic intervention policies by the government to influence Investopedia Levy Publications Tankus on Krugman vs MMT "],["macro-finance.html", "28 Macro-Finance 28.1 Institutional Supercycles 28.2 Central Banking", " 28 Macro-Finance The ultimate driver of government financing costs is the central bank. 28.1 Institutional Supercycles Dafermos Supercycle We build upon the Minskyan concepts of ‘thwarting mechanisms’ and ‘supercycles’ to develop a framework for the analysis of the dynamic evolutionary interactions between macrofinancial, institutional and political processes. Thwarting mechanisms are institutional structures that aim to stabilise the macrofinancial system. The effectiveness of such structures changes over time, creating a secular cyclical pattern in capitalism: the supercycle. We develop a macrofinancial stability index and identify two supercycles in the post-war period, which we label the industrial and financial globalisation supercycle respectively. For each, we apply a four-phase classification system, based on the effectiveness of institutions, customs and political structures for stabilising the macrofinancial system. The supercycles framework can be used to explain and anticipate macroeconomic, financial and thus political developments, and moves beyond conventional approaches in which such developments are treated as exogenous shocks. Framework Despite the bidirectional and dynamic nature of the interactions between institutions and macrofinancial processes, analysis is often partial and static. In the political economy literature, institutional change is linked with exogenous macroeconomic or financial shocks, such as shifts in inflation or the policy interest rate (Iversen and Soskice, 2012; Gabor and Ban, 2013). Conversely, macrofinancial developments are explained as arising from exogenous institutional change, such as alterations to financial regulation or labour market legislation. A framework in which institutional change and macrofinancial processes are dynamically interlinked is still missing. In this paper, we develop an evolutionary framework that connects macrofinancial processes and institutional change. The foundations of this framework lie with two largely overlooked concepts in Minsky’s analysis of financial capitalism (Palley, 2011). The first is that of ‘thwarting mechanisms.’ This concept draws on Minsky’s insight that, although capitalism is inherently unstable, this instability rarely becomes explosive because of the existence of ‘customs, institutions or policy interventions’ that tame destabilising forces (Ferri and Minsky, 1992, p. 84). Thwarting mechanisms counteract the inherent instability of capitalism, allowing for long periods of high economic activity and social and financial stability. However, the effectiveness of thwarting mechanisms varies over time, eventually diminishing as a result of the profit-seeking actions of economic agents and the generation of new sources of long-run instability. This endogenous erosion gives rise to crises, which, in turn, lead to the development of new thwarting mechanisms. The rise and fall of thwarting mechanisms generates secular cycles in macrofinancial stability. This ‘supercycle’ is the second concept we borrow from Minsky and Palley. Thwarting Mechanisms Minsky’s concept of thwarting mechanisms: ‘customs, institutions or policy interventions that make observed values of macroeconomic variables different from what they would have been if each economic agent pursued “only his own gain”’. Thwarting mechanisms reduce the amplitude of basic cycles, constraining instability by imposing ceilings and floors on the dynamic path of the economic system. Floor mechanisms aim to ensure a minimum level of aggregate demand, thereby placing a floor under the level of economic activity. These mechanisms may be the result of deliberate policy interventions, (e.g. activist fiscal policy), or a side effect of other developments (e.g. expansion of household debt to maintain consumption spending). Ceiling mechanisms aim to impose upper limits on the economic expansion by restricting activities that may enhance growth but also generate instability. Examples of ceiling mechanisms include inflation targeting, financial regulation aimed at reducing procyclicality and leverage, and capital controls to restrict speculative financial inflows. The supercycle is a long-run institutional and political cycle over which the effectiveness of a particular configuration of thwarting mechanisms first increases and then declines. The configuration of thwarting mechanisms shapes the supercycle, hardwiring powerful macroeconomic ideas into policy regimes. Macrofinancial stability is primarily driven by the effectiveness of thwarting mechanisms. Four Phases Four phases of the supercycle: expansion, maturity, crisis and genesis. During the expansion phase, newly introduced thwarting mechanisms are effective, leading to economic expansion and broad social and financial stability: economic and financial activity is disrupted by the recessions of the basic cycles, but thwarting mechanisms prevent a systemic crisis. Economic agents learn how to adapt to the new institutional environment, however, innovating to preserve or increase their profits and thereby reducing the effectiveness of thwarting mechanisms. Further, mechanisms introduced to reduce one source of instability may over time create others, potentially as a result of interaction with other thwarting mechanisms. Once the effectiveness of thwarting mechanisms starts to decline, the cycle enters the maturity phase, during which economic expansion continues but the macrofinancial stability of the system is diminishing. The declining effectiveness of thwarting mechanisms ultimately leads to crisis, because the institutional framework is no longer sufficient to constrain the dynamics of the basic cycle. At this point, a basic-cycle recession leads to deep economic, political and social instability, and institutional restructuring. While government intervention may stabilise the economy, broad-based recovery is impossible because existing thwarting mechanisms are ineffective: the institutional structure is can no longer ensure macrofinancial stability. The ensuing genesis phase sees attempts to establish a new configuration of thwarting mechanisms, attempts shaped by political struggles. When – or if – effective new mechanisms are introduced, the next supercycle begins. In the case that – for political, social or technological reasons – such mechanisms cannot be introduced, the crisis phase will be prolonged, likely accompanied by political and social turmoil. Institutional Change Institutions, understood as ‘rules of the game’ provide mechanisms to facilitate market exchange in the presence of transactions costs that prevent an optimal frictionless equilibrium. Causation is largely unidirectional: given the presence of transactions costs, exogenously imposed ‘good’ institutions – usually understood as the rule of law, secure property rights, well-developed financial markets etc. – produce good economic outcomes. Changes in institutional structure, understood as formal ‘rules of the game’ result from ongoing optimisation by market participants over the costs of reconfiguration relative to the benefits, given incomplete information sets, technology, and firm-specific knowledge. Recent work goes beyond this micro-based analysis to examine the possibility of emergent properties in complex evolutionary systems, using agent-based modelling techniques. Thwarting mechanisms can be viewed as constraining the macrofinancial instability that arises from the emergent properties of such complex systems. Institutional structure itself, understood as the configuration of thwarting mechanisms, emerges and evolves as a result of the profit-seeking behaviour of agents. The agent-driven erosion of thwarting mechanisms can give rise to macrofinancial instability. The distribution of power influences the design of thwarting mechanisms; it does not merely affect macroeconomic targets and the associated policy design. Policy makers need to establish mechanisms that keep a range of key macroeconomic variables within certain bounds, irrespective of the primary macroeconomic target; otherwise, macroeconomic instability would undermine political stability. The way that thwarting mechanisms are eroded is specific to each supercycle: the strength of labour in the 1970s undermined the wage-price consensus, giving rise to inflationary pressure, while the strength of finance in the 2000s placed limits on the effectiveness of mechanisms to constrain financial instability. Macrofinancial Stability Index (MSI) The MSI is constructed using a number of ‘floor’, ‘ceiling’ and ‘corridor’ macroeconomic and financial variables. The MSI is calculated as one minus an average of the normalised distances of floor, ceiling and corridor variables from their maximum, minimum and average values respectively (over the period under investigation. The MSI thus takes values between 0 (minimum stability) and 1 (maximum stability). High-income countries experienced common secular cyclical movements in their macrofinancial stability in the post-World War II period. The ideological shift on macroeconomic management at the end of the 1970s brought independent central banks oriented to inflation targeting and fiscal deficits financed on sovereign debt markets. Mass privatisation reduced the state’s economic footprint, while previous gains on employment protection and unemployment benefits were substantially rolled back. Growth increasingly relied on rapid expansion of leverage and increasing financial activity. The financial sector, in turn, found that new institutional structures were required to enable leverage to expand beyond traditional constraints. During the expansionary phase of the FG supercycle, shadow banking expanded significantly, absorbing the flow of assets resulting from the continued expansion of credit. Securitisation and the originate-to-distribute model allowed banks to transform illiquid assets, mortgage loans in particular, into marketable securities. These securities were financed with short-term liabilities such as repos and asset-backed commercial paper (ABCP). Growth became increasingly reliant on collateral-based financial activity. Collateral plays a central role in funding neorentier balance sheets. Neorentiers issue short-term (often overnight) repo deposits secured by tradable collateral. For lenders such as institutional cash pools or money market funds, collateral makes repos a better liquidity management vehicle than unsecured bank deposits. Repo borrowing allows a wide range of institutions to access money market funding, while rising asset prices lead to increasing leverage capacity because repo collateral is marked to market. The use of collateral functionally, and imperfectly, replaces direct sovereign guarantees on short-term liquid assets. The rise of collateral-based finance fundamentally changed the relationships between central banks and governments. In the 1990s, central banks in high-income countries collectively sanctioned neorentiers’ turn to shadow deposits by liberalising repo markets, often to enable Ministries of Finance to develop liquid government bond markets. States turned to neorentiers in the age of independent central banks and capital market financing of budget deficits, introducing reforms in sovereign bond markets designed according to neorentier preferences: regular auctions facilitated by primary dealers and deregulated repo markets The promise of liquidity for sovereign bonds entrenches the ‘infrastructural power’ of finance: neorentiers promise liquidity to Ministries of Finance, and well-functioning monetary transmission mechanisms to central banks, improving their ability to oppose policy innovations or tighter regulatory measures. The rising power of neorentiers thus serves to discipline states, curbing fiscal and regulatory thwarting mechanisms: market financing of fiscal deficits privileges neorentiers as mediators between the monetary and the fiscal arms of the state, and creates conflicting objectives for the central bank and the Treasury. Easy credit conditions allowed sustained expansion of private debt, enabling aggregate demand to keep up with productive capacity in the face of weak income growth and government retrenchment. Credit-financed consumption took over from capital investment as the driver of growth. While crisis-era innovations succeeded in preventing financial system collapse and depression, growth has not returned. In our framework, this less due to ‘secular stagnation’ than to the institutional architecture of the FG supercycle – weak and ‘flexible’ labour, high inequality and government retrenchment. Without a change in this architecture – without a new set of thwarting mechanisms – it is difficult to identify a likely source of sustained demand growth other than a return to credit expansion. Overall, while institutional changes improved the effectiveness of stabilising mechanisms in the period prior to the coronavirus pandemic, the continuous push for asset-based welfare reinforced the structural drivers of neorentier capitalism without delivering a new engine of growth. When the coronavirus crisis struck, a new configuration of thwarting mechanisms that could foster economic expansion alongside financial stability had not yet emerged. The thwarting mechanisms of the next supercycle will be, at least in part, the result of the rapid institutional change that has taken place as a result of this crisis, and of greater awareness of the potential for future pandemics. Inevitably, the next supercycle will also be conditioned by the even greater crisis of climate change. Green Supercycle What is most urgently required, in light of the COVID-19 and climate crises, is a detailed understanding of the current genesis phase and the prospects for the emergence of a new set of thwarting mechanisms that would underpin a green supercycle. Dafermos Gabor (2020) Institutional Supercycles (pdf) 28.2 Central Banking Braun The impact of international economic integration on social protection is conditional on the monetary regime. The role of the European Central Bank (ECB) as the supranational enforcer of the economic logic of integration since monetary union. While Polanyi conceptualized central banking as an institution of non-market coordination that evolved to protect the domestic economy from gold standard pressures, the ECB has acted as an enforcer of disembedding “euro standard” pressures vis-à-vis national labor market and welfare state institutions. Despite lacking the mandate or the authority to override national legislation, the ECB, strategically pursuing its organizational and systemic interests, pushed for structural reforms via discursive advocacy and conditionality. Our results show that Europe’s prospects for Polanyian non-market coordination are determined by Frankfurt as much as by Luxembourg and Brussels. The death of ‘Social Europe’ The European Commission’s slogan of “a Europe that protects”, introduced in 2019, subtly diverges from the Treaty of Rome’s commitment to “proper social protection.” This is no accident. The euro area debt crisis accelerated labor market deregulation and welfare state retrenchment, and the idea of a “Social Europe” has been declared “dead.” At the same time, and particularly among those most affected by by these developments, protectionist and nationalist sentiments have been on the rise. Brussels watchers have read “a Europe that protects” as a bellwether of a new, non-liberal politics of protection. The European Union (EU) as a unique case combining high levels of protection with full “globalization in the strict sense of the word”, namely unrestricted competition for capital, goods and services. The strictures of the euro considerably amplified the economic logic of integration relative to the legal and political logics expressed through the ECJ and the Commission, respectively. With the introduction of the euro in 1999, this economic logic of integration found its institutional expression in the European Central Bank (ECB). Since then, the relationship between economic integration and social protection has been shaped in Frankfurt as much as in Brussels and Luxembourg. Structural Reforms The ECB defined structural reforms, in strikingly Polanyian terms, as policies that “change the fabric of an economy, the institutional and regulatory framework in which businesses and people operate.” This advocacy constitutes a puzzle: The ECB lacks both a mandate and the legal means to shape labor market and social policies at the member-state level. Pushing to “change the fabric” of societies therefore entails significant reputational risks. Why, then, did the ECB chose to push for structural reforms? Our explanatory framework places the emphasis on the ECB’s organizational (credibility and legitimacy) and systemic (survival of the euro) interests. In pursuing those interests, the ECB strategically adjusted the method and content of its structural reform advocacy to fit the economic and political context. In the wake of the euro area debt crisis, the ECB acquired the power—shared with the Commission and the International Monetary Fund (IMF)—to impose and enforce policy conditionality. Polanyi Our analysis, while drawing on Polanyi, fills an important gap in Polanyian thinking on the political economy of central banking. According to Polanyi, national central bank- ing evolved as an expression of the countermovement to the commodification of money under the international gold standard. Whereas Polanyi said little about potential con- flicts between non-market coordination in the domain of money (central banks) and social protection in the domain of labor (social policies and trade unions), this conflict subse- quently moved to the very center of macroeconomic governance. A large literature has since studied the interaction between national central banks and national labor market policies and wage-setting actors. The institutional setting of this interaction changed dramatically with EMU, which established a supranational monetary regime with its own supranational central bank. From the beginning, heterogeneous labor market in- stitutions and social policies threatened divergent national inflation developments, which clashed with the ECB’s one-size-fits-all monetary policy. Whereas Polanyi would have expected a central bank to protect national economies from the disembedding pressures of the monetary regime, the ECB has instead embodied these very pressures, acting as a—if not the—key planner of laissez-faire in national labor markets. Looking beyond Europe, our analysis contributes to the literature on policy diffusion in the context of economic globalization. Here, national policymakers routinely encounter the problems of translating and enforcing perceived functional pressures emanating from the international level. The IMF, guided by the “Washington Consensus,” made its emergency lending conditional on gov- ernments’ implementing specific structural reforms, playing the role of both translator and enforcer. Central banks, as the ultimate repositories of “epistemic authority” on economic matters, are uniquely positioned to play a similar role at the domestic level. In the euro area, the role of translator and—to a lesser but significant extent—enforcer of perceived functional pressures was assumed by the ECB ECB identified—and sought to counter via structural reforms and public-sector wage restraint—the diverging trend in unit labor costs as early as 2005, years before the European Commission. The ECB has been a highly articulate proponent of specific structural reforms in national labor markets and social policy regimes. When unit labor cost divergence, first recognized and prioritized by Trichet, threatened the very effectiveness of supranational monetary policy, the ECB began to pro- mote structural reforms as means of macroeconomic adjustment, both in public speeches and behind the scenes with national policymakers. Executive Board members urged gov- ernments to seek downward wage adjustments, both via structural labor market reforms and by imposing wage restraint on the public sector. When the euro-area debt crisis hit, the ground for its interpretation as a crisis of competitiveness divergence had already been prepared by the ECB. When circumstances added formal and informal conditionality to the ECB’s toolkit, it wielded those instruments to help enforce labor market liberalization, internal devaluation, and public sector wage cuts. It was only when deflationary pressures and criticism in the European Parliament and elsewhere threatened its legitimacy that the ECB abandoned its advocacy of structural reforms. Despite lacking both a mandate and the legal means to directly override national regulations, the ECB has been a keen supranational advocate of market-enhancing integration in the field of labor market and social policy. This analysis also sheds new light on the broader political economy of central banking. Polanyi and others have shown that national central banking evolved under the interna- tional gold standard to buffer the disruptive adjustment pressures on national economies. The supranational ECB provided such protection for the financial system, but not for labor. Instead, emulating the role the IMF in other parts of the world, the ECB trans- lated—and subsequently helped to enforce—the perceived functional pressures of interna- tional monetary and financial integration. Whether the ECB is constitutionally wedded to the role of “prime mover in the move to a market society” remains to be seen. 135 Its recent shift from structural reform advocacy to calls for wage increases has been echoed in the US, where the Federal Reserve has signaled that it will prioritize employment and wage growth over consumer and asset price stabilization. Central banks may yet again become “active agents of the countermovement.” Braun (2021) Planning Laissez-faire: Supranational Central Banking (pdf) "],["externalities.html", "29 Externalities 29.1 Commons 29.2 History of Economics’ ‘Externalities’ 29.3 Ecosystem Services 29.4 Environmental Degradation 29.5 Energy and Transport", " 29 Externalities 29.1 Commons Resource extraction and pollution of the commons power the beating heart of global economic prosperity. 29.1.1 Hardin and Ostrom Nijhuis The features of successful systems, Ostrom and her colleagues found, include clear boundaries (the ‘community’ doing the managing must be well-defined); reliable monitoring of the shared resource; a reasonable balance of costs and benefits for participants; a predictable process for the fast and fair resolution of conflicts; an escalating series of punishments for cheaters; and good relationships between the community and other layers of authority, from household heads to international institutions. Like Hardin, many conservationists assume that humans can only be destructive, not constructive, and that meaningful conservation can be achieved only through total privatisation or total government control. In southern Africa in the 1980s, some conservationists recognised that parks and reserves, many created by colonial governments, had divided subsistence hunters and farmers from much of the wildlife that had long sustained them – and which, in some cases, they’d managed as a commons for generations. The resulting lack of local support meant that even the best-patrolled park boundaries were vulnerable to incursions by human neighbours, people unlikely to tolerate – much less protect – the large, sometimes troublesome species that ranged beyond even the largest reserves. In 1987, when the South African conservationist Garth Owen-Smith attended a conference on community-based conservation in Zimbabwe, a comment by Harry Chabwela, the director of Zambia’s national parks, left a lasting impression. ‘At this conference we have talked a lot about giving local people this and giving them that, but what has been forgotten is that they also want power,’ Chabwela said. ‘They want a say over the resources that affect their lives. That is more important than money.’ In 1996, the Namibian National Assembly passed a law that allowed groups of people living on communal land to establish institutions called conservancies. Conservancies would be governed by elected committees, and all members would share the benefits of any tourism or commercial hunting within conservancy boundaries. Nijhuis (2021) The miracle of the commons 29.2 History of Economics’ ‘Externalities’ Duncan Austin Incomplete Markets Externalities were generally ignored through most of the 20 Century. After Pigou had identified the problem in the 1920s, there followed a long barren period for “welfare economics”, the natural home for this type of thinking. This lasted until the early 1970s when there were the first stirrings of renewed interest by serious economists. Framed as “externalities”, market failures could be more easily dismissed. The term encourages a perception of unpriced damages as being mere residuals to the centrepiece of a priced economy. Since Pigou, some have sought to “beef up” the terminology. K. William Kapp, for example, bluntly described the market mechanism, in toto, as a “cost-shifting” institution. In this framing, externalities are not a bug, but a feature. The mathematization of economics – another marker of the discipline’s scientific aspiration – exacerbated the situation. The desire for manageable equations and functioning models further pushed troublesome market imperfections away. Possibly, there was the sense that positive and negative externalities might roughly cancel each other out, leaving GDP incomplete but still reliable enough as a directional indicator. That rests on the assumption that positive and negative externalities are symmetrical in nature. However, there is an important asymmetry. Positive externalities take the form of “free goodies”, whereas certain negative externalities constitute systemic risks that may be catastrophic to “trip” or breach. While you generally cannot have too much of a positive externality – a “free good thing” – too much of certain unwanted harms may induce systemic failure. Externalities exist because markets have an incomplete grasp of what humans value. Markets work off prices and not everything has – or can have – a price. As such, marketed values – or prices – exist amidst a broader “value field” of things that humans care about and which have an influence on our wellbeing. Pigou’s proposition was an inconvenient truth for economics. It suggested that there are real limits to what conventional economics might say about matters of human value and, hence, to how far markets might serve human wellbeing. The inconvenience of his idea may be why Pigou is not better known – seemingly more tolerated, than celebrated Complete markets…? As a discipline, economics did the very human thing of trying to ignore a difficult proposition. By not confronting Pigou’s awkward challenge, the door was opened for a line of theorizing th that led in exactly the opposite direction. Economists for most of the 20 Century sought to establish economics as a comprehensive corpus of thought with universal application. Hence, by the 1950s, a very appealing theory of complete markets had been developed. No externalities in this theory, none at all. Complete market theory is the laying down of a conceptual blanket over all our preferences that leaves no space for externalities. The formulation of complete markets theory was deemed a major milestone for economics. Its authors were Kenneth Arrow and Gérard Debreu. It provided the cornerstone for the discipline’s claim for the superiority of markets as a mechanism for social coordination. Rather, the key mistake made by 20 Century economics was not in misunderstanding externalities, but in grossly underestimating their magnitude and so foreclosing a debateon the innate limits of economic thinking. The discipline considered that markets were “complete enough” to safely proceed as if they were actually complete! We are now waking up to the consequences of that misjudgement. … Or very incomplete markets? Consider, for example, a recent study by Robert Costanza and colleagues. They estimated the monetary value of the “services” provided free by the Earth’s ecosystem at $125 trillion in 2 2011, nearly twice the value of global GDP (gross domestic product) at the time. The authors believe this to be a conservative estimate because it grasps only about half of the “services” we know ecosystems provide. Other studies have contemplated the value of unmonetized social systems, including one estimate that unpaid housework in the UK in 2016 was about 65 percent of GDP – another 3 huge block of value not captured by the market. Just combining this figure with the Costanza et al. figure suggests that measured GDP captures about a third of some larger conception of value. From its very inception, GDP has been derided as an incomplete measure of wellbeing. However, in elevating GDP to its current perch of influence, the working assumption has been that GDP, and the market system it reflects, captures the lion’s share of what matters. What the latest estimates of “externalities” and non-market values suggest – and what our sustainability crisis seems to underscore – is that our perception of GDP’s reach may be horribly off. Such an estimate suggests that it is not that the market does not capture all things of value, it does not even capture most things of value. Far from externalities being peripheral, they may be the main event! The failure of economics to fully incorporate externalities in its 20th-century theorizing now appears to be the dropped stitch that defines the whole discipline. For a long time, this was a tolerable neglect as markets were more robustly counterbalanced by pre- market institutions that upheld unpriced values, and as the environment was able to absorb the fewer demands of a smaller, less consumptive population. But, with the onset of climate and biodiversity emergencies, the context has changed considerably. It matters more and more that we might not have slightly incomplete markets, but very incomplete markets. It has left us at the start of the 21 Century transforming the matter and energy of the world using economic and financial tools that have only a very limited grasp of the reality they fashion. In a world of very incomplete markets, things of human value lie in two separate realms – the marketed domain and the non-marketed domain. Some of the growth of the marketed economy genuinely arises from human ingenuity and creativity unlocking better ideas and products from new combinations of inputs. This is “good” growth, which ought to be celebrated and encouraged. However, other parts of monetized “growth” arise from simply running down the stocks of what is valuable but in the non-marketed realm. This is the illusion of wealth creation based on registering the increase in marketed value, but not recording the decrease in unmarketed values. In contrast to growth from genuine ingenuity, this is robbing Peter to pay Paul. Measured economic “growth” overall combines in unknown proportions a “creative growth”, which we want to encourage, and a “parasitic growth”, which we do not. At an aggregate level, it is almost impossible to trace the origins – creative or parasitic – of GDP growth, and very few official metrics make any attempt to do so. Our working assumption is that all economic growth is good – as it indeed would be if we had complete markets eliminating the possibility of parasitic growth. However, in not knowing the real-world mix between creative and parasitic growth, do we want more GDP growth, or less? It is not clear. And, given that companies work to the same price register as GDP, do we want companies to beat profit expectations or would it be better if they missed them? Who really knows? The conventional argument – captured by the notion of an Environmental Kuznets Curve – is that it is only by increasing monetary wealth that we can develop better technology to protect the environment. However, it is not clear in the aggregate whether the deployment of such new capabilities ever makes good the damage done by the initial enabling wealth creation. While anecdotes can be summoned to support the idea – electric cars, wind turbines, LEDs etc – thus far, at the global level that matters, data shows we remain in net ecological destruction mode. Markets within Cultures Paradoxically, then, to use markets more than we are, to introduce more externality pricing, would require a new cultural level reassertion that markets are a tool within culture. We need not a sustainable economy, but a sustainable culture that has an economy. Such a culture would establish room for governments to introduce new markets which powerful market ncumbents may not like, but which improve human wellbeing. In turn, such a culture would also invigorate non-market means to protect our environment, for we must remember that not everything of human value can be priced and “internalized”. The interesting question, worth a moment’s reflection, is: why is that? Commodifiable Externalities Though Pigou identified commodifiable externalities, there are many things of human value that cannot withstand the disembedding from their context necessary for them to be commodified and, hence, be transactable via market exchange. Such values are non- transactable because they are irrevocably embedded either in specific things – they are unique – or in specific relations – they exist “between” certain things. Some examples: friendship, reputation, loyalty, integrity, trust, community, mental health, etc. If you believe you have purchased any of these items, you might want to check the label. What is tricky is that most things in the world bear both separable transactable values and intrinsic non-transactable values. A tree has both separable value as a feedstock for furniture and paper and intrinsic value as part of the ecosystem in which it is relationally embedded. We tend to value trees in managed plantations for their separable values, but we value General Sherman, the 26-story-tall giant sequoia that is the largest known tree on Earth, for its non-separable attribute of being uniquely the tree we call General Sherman. With General Sherman, we have chosen to perceive and value its uniqueness over its instrumental value. Indeed, we might say that General Sherman is price-less. The “economist” denies the validity of this perspective by arguing that everything has a price. To say that something is priceless is merely to say that nobody has yet offered a high enough price. In turn, the “ecologist” denies the “economist’s” perspective, arguing that while you can apply such economic thinking to General Sherman, it is the wrong sort of thinking to apply. Both the “economic” transactional perspective and the “ecological” intrinsic perspective are beneficial and valid, but they are incompatible. The decision to apply an economic perspective to the external world is always a value judgment that necessarily transcends economics. More, it is a value judgment that can never be justified or refuted on economic grounds precisely because it is an argument about the validity of applying an economic perspective. All this is a discussion that the field of economics may well have taken more seriously 100 years ago, had it been more open to the significance and implications of Pigou’s formulation of externalities. Alas, we are now having to unknit to pick up this dropped stitch in a world now confronting large-scale problems of missed externalities. Incompletness Theorem of Economics Economics might be well served by formalizing an incompleteness theorem that would act as a proverbial knot-in-a-handkerchief reminder about the limits of claims that economics can make. It is an oddity of human intellectual thought that the most logical of our sciences, mathematics, had a formal Incompleteness Theorem as early as 1930, while economics formalized a complete market theory in the 1950s and seemingly still has no definitive statement of incompleteness. Ringfecing Economics One of the ways, then, that we could better protect ecological values is for economics to recognize – re-cognize – the wisdom of culturally ring-fencing where economic thinking is preferred. In other words, to recognize the non-monetizable value of non-economic thinking. Designating areas as protected are to explicitly restrain the ever-eager economic perspective. Such boundaries need to be upheld at the social or cultural level to count for anything. If not individuals can always free ride and extract the monetary instrumental value that others have agreed not to pursue. While economics is undoubtedly a valuable form of knowledge, it is a way of seeing things, not the way. A full century after Pigou formalized the idea of externalities, we might mark the anniversary by taking more seriously the effort to clarify the appropriate reach of economics and markets within the broader social and cultural context. Economics is about solved Political Problems Arguably, one of the most important questions in economics is not even an economic question. The field effectively punts the matter of its own ontology – the things that economics can talk about – to a different discipline. In Abba Lerner’s words: “An economic transaction is a solved political problem. Economics has gained the title of Queen of the Social Sciences by choosing solved political problems as its domain.” Economics has been strangely content to focus its efforts on pattern-seeking within a domain it leaves other disciplines to define, but in the absence of contemplating its boundaries more explicitly, it has hubristically come to believe it has greater reach than it really has. In turn, this leaves most economists – and the great many people who think and act economically in conducting their professional duties – dangerously unaware of where economic thinking is beneficial and valid and where it ultimately hits limits. Duncan Austin: Pigou and the dropped stitch of economics RWER95 (pdf) 29.3 Ecosystem Services Constanza • Global loss of ecosystem services due to land use change is $US 4.3–20.2 trillion/yr. • Ecoservices contribute more than twice as much to human well-being as global GDP. • Estimates in monetary units are useful to show the relative magnitude of ecoservices. • Valuation of ecosystem services is not the same as commodification or privatization. • Ecosystem services are best considered public goods requiring new institutions. In 1997, the global value of ecosystem services was estimated to average $33 trillion/yr in 1995 $US ($46 trillion/yr in 2007 $US). In this paper, we provide an updated estimate based on updated unit ecosystem service values and land use change estimates between 1997 and 2011. We also address some of the critiques of the 1997 paper. Using the same methods as in the 1997 paper but with updated data, the estimate for the total global ecosystem services in 2011 is $125 trillion/yr (assuming updated unit values and changes to biome areas) and $145 trillion/yr (assuming only unit values changed), both in 2007 $US. From this we estimated the loss of eco-services from 1997 to 2011 due to land use change at $4.3–20.2 trillion/yr, depending on which unit values are used. Global estimates expressed in monetary accounting units, such as this, are useful to highlight the magnitude of eco-services, but have no specific decision-making context. However, the underlying data and models can be applied at multiple scales to assess changes resulting from various scenarios and policies. We emphasize that valuation of eco-services (in whatever units) is not the same as commodification or privatization. Many eco-services are best considered public goods or common pool resources, so conventional markets are often not the best institutional frameworks to manage them. However, these services must be (and are being) valued, and we need new, common asset institutions to better take these values into account. Constanza (2014) Global value of ecosystem services (Paywall) (pdf) Constanza (2019) Natural Capital and ERcosystem Services “The fossil fuel industry has been granted the greatest market subsidy ever: the privilege to dump its waste products into the atmosphere at no charge.” (Michael Mann) 29.4 Environmental Degradation 29.4.1 Kuznets and Engel Curves Environmental Kuznets and Engel’s Curve I think degrowth is wrong on the merits (environmental kuznets curves and environmental engel curves are both concave), but it’s also an obvious nonstarter even if it was founded on solid footing, To spell this out: if EKC and EECs are concave, redistribution within or between countries doesn’t necessarily reduce environmental damages. (John Voorheis (twitter)) Abstract Maneejuk: This study aims to examine the relationship between economic development and environmental degradation based on the Environmental Kuznets Curve (EKC) hypothesis. The level of CO 2 emissions is used as the indicator of environmental damage to determine whether or not greater economic growth can lower environmental degradation under the EKC hypothesis. The investigation was performed on eight major international economic communities covering 44 countries across the world. The relationship between economic growth and environmental condition was estimated using the kink regression model, which identifies the turning point of the change in the relationship. The findings indicate that the EKC hypothesis is valid in only three out of the eight international economic communities, namely the European Union (EU), Organization for Economic Co-operation and Development (OECD), and Group of Seven (G7). In addition, interesting results were obtained from the inclusion of four other control variables into the estimation model for groups of countries to explain the impact on environmental quality. Financial development (FIN), the industrial sector (IND), and urbanization (URB) were found to lead to increasing CO 2 emissions, while renewable energies (RNE) appeared to reduce the environmental degradation. In addition, when we further investigated the existence of the EKC hypothesis in an individual country, the results showed that the EKC hypothesis is valid in only 9 out of the 44 individual countries. Maneejuk (2020) Does the Environmental Kuznets Curve Exist? (pdf) The Kuznets curve expresses a hypothesis advanced by economist Simon Kuznets in the 1950s and 1960s. As an economy develops, market forces first increase and then decrease economic inequality. Since 1991 the environmental Kuznets curve (EKC) has become a standard feature in the technical literature of environmental policy, though its application there has been strongly contested. The environmental Kuznets curve (EKC) is a hypothesized relationship between environmental quality and GDP growth: according to its argument, which is spurious, various indicators of environmental degradation tend to get worse as modern economic growth occurs until average income reaches a certain point over the course of development, at which point some studies have argued, they improve. It first became popular as introduced by Gene Grossman and Paul Krueger in their working paper: “Environmental Impacts of a North American Free Trade Agreement.” This paper simply showed that the non-direct Greenhouse gas, Sodium Dioxide, Dark Matter, and Suspended particles followed an inverted-U shaped pattern. This was almost immediately misinterpreted by the World Bank and Beckerman and adopted into policy as an argument that all negative environmental effects would follow an EKC pattern. Copious research has concluded that beyond these pollutants, and issues like water quality, that immediately threaten human health, GDP growth essentially harms, and does not help the environment with no lasting “turning point.” The EKC has led to poor policy choices reaping untold environmental damage. Wikipedia 29.5 Energy and Transport The “hidden cost” of our largely fossil fuel-based energy and transport systems could add up to around $25 trillion (£18 trillion) – the equivalent of more than a quarter of the world’s entire economic output. That’s according to new research, which estimates the hidden environmental, social and health costs associated with different forms of transport and electricity generation. Sovacol (2021) [(pdf)[(pdf/Sovacol_2021_Energy_and_Transport_Externalities.pdf) Independent "],["capital.html", "30 Capital 30.1 Produced Capital 30.2 Natural Capital 30.3 Human Capital 30.4 Social Capital", " 30 Capital 30.1 Produced Capital 30.1.1 Cambridge Controversy Spash Produced capital. What is it? Produced capital is defined as ‘capital goods embodied in human-made goods or structures, such as roads, buildings, machines, and equipment’ (Dasgupta, 2021, p. 507). These are physical assets, generated by human transformation of natural capital, that are used to provide a flow of goods or services, e.g. a sewing machine, factory or computer. A private house counts as produced capital because it provides services (e.g. shelter) repeatedly over time. Intangible assets, such as company patents, are also included. Produced capital is then a diverse stock measured as a value in national wealth accounts, and an increase of which contributes to economic growth (GDP). Thus, ‘inclusive wealth increases if and only if aggregate consumption is less than net domestic product (NDP), that is, GDP less the depreciation of all capital assets’ (Dasgupta, 2021, p. 138, emphasis original). Measuring the value of capital is then essential to the whole approach. How is it valued? Different forms of capital cannot be aggregated physically (i.e. hammers and tractors do not add together). So what is the aggregate or total amount of capital? The stock can be measured either as: (i) the monetary cost of production or (ii) the monetary returns attributed to specific capital on future output produced (i.e. future profits). The former, (i) involves capital itself in the pro- duction of capital and so ends in circularity with the value of capital determining the value of capi- tal, ad infinitum. One work around is to adopt a labour theory of value, so that all produced capital s valued by the labour required for its production. Today this classical economic theory is generally rejected outside of classical Marxist economics. As a neoclassical economist, Dasgupta opts for (ii), claiming that: ‘[a]ssets acquire their value from the services they provide over their remaining life’ (Dasgupta, 2021, p. 138). This leads to an asset man- agement approach whereby different types of assets, or forms of capital, are required to produce the same rate of return in order to achieve an optimally managed investment portfolio (i.e. that maximizes returns by equating returns on every investment at the margin). More simply, this means whether investing in produced capital, education or blue whales the economic agent (‘citizen investor’) seeks the same return. What is ignored by Dasgupta is a long history, that involved his own University and Economics Department, concerning problems with measuring capital. What is problematic about it? The failings of both approaches, (i) and (ii) above, were the subject of the ‘Cambridge Capital Con- troversy’, involving combat between economists in Cambridge England and USA. Starting in the 1950s this continued for two decades, or more, and was never resolved (see Cohen &amp; Harcourt, 2003). In case (i) there is the need to take into account a flow of costs over time (period of pro- duction) which, in economics, requires knowing the rate of interest as a basis for equating values in different time periods. In case (ii) knowing the value of (profit from) a stream of future output (over a period of production) means calculating the net present value and so discounting it at a rate of interest. Knowing the rate of interest is required in both cases. However, the rate of interest is the return on capital investment, which requires knowing the quantity of capital. So, the value of capital cannot be determined without knowing the stock of capital, that, for multiple forms of capital, becomes a value which cannot be known without the rate of interest, which is defined by already knowing the stock of capital, and so on … Neoclassical economists (aka Cambridge USA) then opted for naïve empiricism and claimed they could collect data and observe the rates of return in actual markets without explanation as to how, or from where, it is produced. Thus, in this tradition Dasgupta claims that: ‘[t]he yield on investment in produced capital is its marginal product’. Solow, whom Dasgupta cites as a major influence on his economics, has sought to justify this approach. Yet, the basic problem remains, the value of capital and, indeed, its definition, are left indeterminate and the empirical approach lacks validity. The alternative is to admit that neoclassical theory bears no relationship to reality, and capital investment is not about simplistic production functions specifying the rate of return to different factors (i.e. land, labour, capital) measured by disaggregated marginal products, but, rather, con- cerns institutional arrangements to capture surplus. Indeed, outside of economic textbooks, the contributions of the separate factors to output cannot be determined, let alone a marginal product attributed to each (i.e. what is due to labour vs. capital, say the farmer versus the tractor, let alone the land!). Rather than marginal productivity theory we might instead consider that profit is derived from the social power of those able to appropriate the technological achievements of society as a whole. They may be capitalists in market dominated economies or functionaries of the State in centrally planned economies. Under capitalism the key to power lies in gaining private property rights over resources, and this then lies at the heart of the debate over biodiversity. What is at stake is the legal right and economic authority to capture the surplus created by the production process. This is why classical political economy (as opposed to neoclassical economics) connected individuals’ dependence on the market for their livelihoods with social class, as the fundamental unit of analysis. Rather than marginal productivity theory we might instead consider that profit is derived from the social power of those able to appropriate the technological achievements of society as a whole. They may be capitalists in market dominated economies or functionaries of the State in centrally planned economies. Under capitalism the key to power lies in gaining private property rights over resources, and this then lies at the heart of the debate over biodiversity. What is at stake is the legal right and economic authority to capture the surplus created by the production process. This is why classical political economy (as opposed to neoclassical economics) connected individuals’ dependence on the market for their livelihoods with social class, as the fundamental unit of analysis. 30.2 Natural Capital (se renv on Spash/Dasgupta) Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) 30.3 Human Capital Spash The concept of human capital is heavily related to productivity in a wage labour economy. People who can be more productive have more value and those who live longer (i.e. the young) can be productive for longer, and so have more value, than others. The two main elements, health and education, must be converted into monetary values to oper- ationalize the human capital approach. Dasgupta (2021, p. 256) notes that: ‘The value of a statistical life (VSL), [is] a concept central to the meaning and measurement of human capital’. The idea is that monetary values can be placed on human life without specifying the people who will actually lose their lives as a result of a public policy decision. There are two main methods for assessing the risk of death or VSL. First, an indi- vidual may be directly asked their willingness-to-pay to avoid a risk or their willingness-to-accept compensation for incurring a risk. CVM surveys have been commonly applied but also been severely criticized. The other main alternative is to use measures related to earnings, a revealed preference method, technically termed a hedonic wage approach. This might, for example, use actual wage differentials in jobs with a range of risks. The definition of human capital as productive wealth could be understood as framing a govern- ment’s relations with its citizens primarily through the lens of their economic contribution, as if a human resources department, ensuring good health and education to the extent that it contrib- utes to productivity. This implies allocating resources according to the expected payback, e.g. prior- itizing young healthy adults. This productivist logic led some economists to justify eugenics. There are also long standing racist associations with references to lazy indigenous peoples by colonizing Europeans, and classist associations as in the history of removing common rights to force the poor into wage labour relations so they could become productive. The apparently simple case of investment in education also quickly runs into trouble. Financial returns neither require being educated nor does education bring financial returns per se. Under capitalism it is business, banking and finance that ‘makes money’ not just being educated. Health (mortality/morbidity) as a capital investment is even worse. Producing money numbers here requires the conjuring trick of talking about abstracted non-real people who are represented as ‘statistical lives’, under the VSL. For example, the results are used in transportation assessment to decide upon road building programmes and the installation of safety equipment. However, the public rejection of this approach is exposed when there is a train crash, people are killed and the public discover the lack of safety equipment is due to the calculation that it cost more than the expected fatalities times the VSL. Politicians rarely defend the numbers in such circumstances, although their transport departments may continue to use them on a daily basis. Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) 30.4 Social Capital Spash Social capital is defined as mutual trust and associated norms of reciprocity that enable people to engage with one another. Taken together, trust in others, confidence in government to deliver and in markets to function well, and the institutional arrangements that enable people to engage with one another for mutual benefit, is called social capital – a concept central to the economics of biodiversity. While mainstream economics commonly views society as comprising three classes of institutions (households, firms, government), ‘the idea of social capital illuminates a fourth class, comprising communities and civil society’ Dasgupta’s understanding here, and his capitalist reductionism, appear quite limiting. Social capital must be optimized for several reasons. First, he believes trust and economic growth are positively related so that more cooperation improves efficient allocation of resources and so increases wealth. Second, civic engagement and membership in associations discipline governments and improve governance. Third, communities and civil society are regarded as essential for controlling Nature conservation and restoration programmes initiated by government or national/international NGOs. Bringing together a range of actors – governments, NGOs and ‘increasingly’ private firms – is advocated to build local institutions to engage people in collective action and set rules. This is necessary because ‘beliefs do not appear out of nowhere’. Accordingly, this will be an institutional process: ‘That helps to align beliefs’ The danger of blanket calls to ‘align beliefs’, Diversity of opinions, different stake- holders perspective, ‘misaligned beliefs’ and public debates are what democracy is about. Aligning beliefs is more inline with totalitarianism. Absolute trust in government by all is also neither likely nor something to be ‘optimised’ via investment. Promoting such ‘social capital’ might easily be instru- mentalised to silence critical voices, blame civil society for being uncooperative, depoliticize issues and dismiss genuine concerns – class struggle, power relations, value conflicts. Civil society is also divided. The concept and promotion of social capital by The Review appears fuzzy, double-edged and dangerous for democracy. Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) "],["money.html", "31 Money 31.1 Money Creation 31.2 Inflation 31.3 Inflation as Restructuring 31.4 Modern Monetary Theory (MMT) 31.5 Milton’s Money", " 31 Money Money is formalized debt. Informal debt, ie. favour-based, or gift economies, CAN be ecologically sustainable. But by formalizing this kind of social obligation and cohesion, we necessarily commoditize our social ecologies. Formalization leads to ecological unsustainability. (BichlerNitzan tweet) Money is, above all, a subtle device for linking the present to the future (Keynes) 31.1 Money Creation Addiction Bank of England ‘addicted’ to creating money, say peers. The Bank of England risks becoming addicted to creating money and needs to come clean about how it plans to unwind its £895bn bond-buying programme, the House of Lords has warned. Guardian McLeay (2014) Money Creation in the Modern Economy (pdf) The value of money depends on not creating too much of them relative to future value creation. (Pengenes verdi henger på at det ikke lages for mye av dem i forhold til fremtidig verdiskaping.) Bankplassen: Hvordan skapes penger (‘How money is created’ in Norwegian) 31.2 Inflation Monetarist theory, which came to dominate economic thinking in the 1980s and the decades that followed, holds that rapid money supply growth is the cause of inflation. The theory, however, fails an actual test of the available evidence. In our review of 47 countries, generally from 1960 forward, we found that more often than not high inflation does not follow rapid money supply growth, and in contrast to this, high inflation has occurred frequently when it has not been preceded by rapid money supply growth. There are several implications. The most relevant of these seems to be that the current efforts of central banks to engender inflation are unlikely to be successful. Based on our examination of countries that together constitute 91 percent of world GDP, we suggest that high inflation has infrequently followed rapid money supply growth, and in contrast to this, high inflation has occurred often when it has not been preceded by rapid money supply growth. The U.S. economy may well experience some increase in inflation in the coming year, but if it does, it is likely it will be due to factors other than monetary policy. Money Growth Inflation 31.2.1 FED Policy and Inflation When you view money supply and velocity together, one notices they tend to offset each other. However, we highlight the late 1970s, the last highly inflationary period, to show a period they did not counteract each other. The Fed and Treasury are playing a dangerous game. The numbers we discuss above are massive and dwarf anything seen in American history. If consumers start spending their savings, and the government keeps borrowing and spending unprecedented amounts, velocity can pick up rapidly. We leave you with a vital question to better understand the prospects for inflation. Lebowitz Lebowitz and Freeze 31.3 Inflation as Restructuring Fix In the hands of economists, the idea of an ‘average price level’ is, to echo Joan Robinson, “a powerful instrument of miseducation”. The trouble is that averages are a mathematical identity — they are true by definition. I can calculate the average of any conceivable set of numbers. But that doesn’t mean my calculation will be informative. That’s because averages define a central tendency, yet do not indicate if this tendency actual exists. The idea that averages should be reported together with a measure of variation is a basic part of empirical science. And yet when economists study inflation, this practice is conspicuously absent. Economists won’t call it an ‘average’. They’ll call it a ‘price index’. Most people will assume that the movement of the average price indicates a strong central tendency. In other words, they’ll assume that inflation is uniform. Figure: Price change in the real world. The black line shows the change in the US consumer price index since January 1, 2020. The colored lines show the indexed price of all the individual commodities tracked by the CPI. Many commodities are tracked in multiple locations. Inflation is never uniform … it is always differential. And that makes it highly significant. Inflation restructures the social order, producing winners and losers. It is this restructuring that is the most important aspect of inflation. And yet it is this feature that economists almost completely ignore. If price change varies wildly by commodity (as it does in the real world), then the movement of the average price tells you little (if anything) about the movement of individual prices. And that means the money supply tells you little (if anything) about real-world inflation. Actually-existing societies produce many commodities whose prices do not change uniformly. And that creates a problem. It means that the quantity of production, \\(Q\\), is hopelessly ambiguous. Having seen that price change varies greatly between commodities, you might wonder why this matters. Well, it matters because it means that inflation is not purely a ‘monetary’ phenomenon. Inflation redistributes income. Nitzan and Bichler have discovered, for instance, that inflation systematically benefits big business. Notice how this evidence changes your view of inflation. It makes it hard to blame government for the problem. You see, if big business is systematically benefiting from inflation, it implies that these big corporations are raising prices faster than everyone else. In other words, it is oligopolies that are driving inflation. Inflation is a power struggle over who can raise prices the fastest. Fix (2021) The Truth About Inflation 31.4 Modern Monetary Theory (MMT) MMT and Austrian economics are mirror images. MMT wants soft money to redistribute wealth to middle class. Austrians want hard money to maintain value of middle class savings. Dominant capitals, in control of state, ignore both and switch between soft and hard to maintain power. (Ian Wright) Culbreath The common conservative objection to government spending is not only based on a faulty economic model, but it also serves as a mental obstacle to using government capacity for purposes conservatives are supposed to care about. Conservatives should re-examine the economics that underlie the tendency on the right to condemn any instance of large-scale government spending. The policy conversation could then shift away from whether government spending is economically justifiable and towards what kinds of spending are productive and worthwhile. According to MMT, no sovereign state that issues its own currency can ever run out of money. This is because it simply “prints” its money in order to pay for whatever operations it deems necessary. The government doesn’t need to go out and find the money it needs for such spending; it needs no fundraisers, no sales, and not even taxes, to fund its operations. It simply spends into existence the money that it needs for its operations. unlike any private entity, such as a household or a corporation, the government does not need to worry about its own solvency, since it can never go insolvent—it can never run out of money. Thus, the government does not need to take any of the measures that private entities take to accumulate savings or make a profit. It will always be able to “spend into existence” whatever money it needs to pay for its operations. The difference between the federal government and a household is thus not merely a difference in degree (one is bigger than the other). It is a difference in kind. It follows that taxes do not amount to the accumulation of wealth for the government. Rather, they amount to no more than the extinction of money from the economy. Like God, the government giveth, and the government taketh away; the government spends money into the economy by printing it and removes money from the economy by taxing it. In terms of the pure quantity of government spending, there is no limit except inflation to what the government may spend. Inflation only occurs, at least in any damaging degree, under certain conditions. Foremost among those conditions is full employment. When the economy is at full employment, which it rarely is, then the overall purchasing power within the economy may be considered to be at full capacity. At this point, an injection of more money into the economy might result in inflation, since it would likely push demand to outstrip supply, thereby causing prices to soar higher and purchasing power to decline at a dangerous rate. However, even this danger could be averted to a significant degree if the government aimed its spending not only at stimulating demand but also at stimulating the production of consumer goods. In fact, the U.S. government has a long history of doing exactly this, as the extensive research of economists like Mariana Mazzucato demonstrates quite compellingly. If there is a risk of inflation from government spending, it would not be because government spending automatically produces inflation. Rather, it might be due to the fact that the U.S. government has ever since the Reagan era failed to make a priority of stimulating both demand and production through the implementation of a deliberate industrial policy. This only highlights further the importance of well-planned spending by the government. Furthermore, the government has many other tools at its disposal for controlling inflation. Not only does the Federal Reserve play a central role here, but even taxes can be levied by the government as a way of controlling inflation. When demand begins to outstrip supply, the government may very well consider imposing taxes where this might effectively limit inflation. Of course, not just any taxation would do this; the government needs to ensure that its taxes target entities or classes who spend enough money to affect consumer prices. Taxing the extremely wealthy, for example, may not be the best way to control inflation, since wealthy people tend to save more than they spend. while there are many good reasons for the government to impose taxes, especially to control inflation, funding government expenditure is not one of those reasons. Indeed, the idea that taxes should fund the government’s expenditure by “plugging its deficits” could prove to have damaging effects on the economy as a whole, if actually put into practice. As MMT theorists are fond of pointing out, what we call the government’s deficit actually amounts to nothing other than the people’s surplus. As long as the government (public sector) spends more into the economy (private sector) than it taxes out (the definition of deficit spending), the economy itself will enjoy a money supply large enough to stimulate productivity and growth. By contrast, if the government were to tax more out of the economy than it spent into it, the economy would suffer from its own deficit, a risk of deflation, and a potential crisis of underconsumption. In other words, the deficit by itself is not a bad thing, and it is here to stay. Culbreath (2021) MMT for Conservatives 31.5 Milton’s Money Fix Like a good neoclassical economist, Friedman grounds his theory in an accounting identity — one that relates the quantity of money \\(M\\) to the average price level \\(P\\): \\[MV=PT\\] In this identity, \\(V\\) is the ‘velocity of money’ — the rate that money changes hands. And \\(T\\) is an index of the ‘real value’ of all transactions. The nice thing about this accounting identity is that it is true by definition. So if you tie a theory of inflation to it, your ‘predictions’ will always work. The problem, pointed out by critics, is that this identity tells us nothing about causation. It could be that printing too much money causes prices to rise. Or it could be that rising prices drive people to borrow (and hence ‘create’) more money. Fix (2021) The Truth About Inflation "],["markets.html", "32 Markets 32.1 Market Primacy 32.2 Markets as Entropy Maximisers 32.3 Competition Bargaining", " 32 Markets Markets as one of the fundamental institutions of capitalism function precisely because fragmented actors come together to compete. However, these decentralized encounters are based on a (financial) infrastructure that must be as frictionless as possible, i.e. centralized. In received theory, the market is a flat institution, comprising numerous buyers and sellers whose actions are independent of each other and whose size is too small to individually alter the overall outcome. In the actual world, however, the market is anything but flat. What should the Biden administration have done to overwhelm supply chain bottlenecks early on in the crunch? What should the administration be doing now? I think many of us imagined that we live in a world where there’s a wizard behind the box. That there’s actually somebody in charge of all of this, and that that somebody must have made a mistake. And of course, it must be the president of the United States. But that’s not actually the world that we live in. It’s a market-based system. We’re lucky to live in an economy that’s built on the principles of free enterprise, and so while it’s easy to cast blame and point fingers at the administration, we have to recognize that they’re not really in charge of all of these things. They didn’t create this situation and I’m not 100% convinced that they’re the ones that are going to be best equipped to solve the problem… As much as we love the idea of a free enterprise system, the reality is that markets often fail…if you’re trying to address a market failure, you want to have a single person or team in charge that can dictate terms to all the different market actors. 32.1 Market Primacy Austin Today’s market primacy may be detrimental. With markets privileged – and non-market institutions discredited – government has been unable to correct the market’s omission of so many recognized externalities, let alone advance non-market regulations or prohibitions to protect our ecology. We conceived of Homo Economicus and built a logical model of the world around that conception and have ever since been trying to live up – live down, really – to that self-image. We have been striving to make our behaviour fit a simple model rather than adjusting our models to a new comprehension of our complex behaviour. Effectively, a subplot of our broader mind-culture co-evolution has been a ‘mind-market coevolution’, in which human minds have made markets have shaped minds. Central to that model – and to our current faith in markets – is the left-brain inspired idea that society can be reduced to rational individual ‘agents’ endowed with entirely independent preferences who exchange parts of the world in a market system that has the magical power to ‘add everything back up’ to arrive at the best of all possible worlds. It is a seductive vision – magically self-coherent and entirely insulated from any external limitations. It sounds exactly like the sort of place that the reductionist left brain would wish to inhabit. Two centuries after Carlyle, people formerly members of communities have become ‘human capital’ summable into ‘social capital’ wholes. To allow the market mechanism to be sole director of the fate of human beings and their natural environment… would result in the demolition of society. (Karl Polanyi) Where Einstein said you cannot solve problems with the same sort of thinking that created them, what McGilchrist effectively says is that you cannot solve problems with the same brain hemisphere that created them. What I suggest from a lowlier perch, and what Kumar invited his LSE hosts to consider, is that we are unlikely to solve problems that have arisen from economic primacy with thinking that upholds economic primacy. Our market-centric culture has granted primacy to the economist, not the ecologist, which may be the wrong way around. Of course, we do have to ‘manage our house’ (economics), but we must also be mindful of the state of Nature’s house (ecology) and of how the management of our house at the local level affects Nature’s house at the global level. Market advocates have sometimes likened the market to an ‘intelligence’. But if it is intelligent, it is very much intelligent in the way the left brain is intelligent: with a limited view of the world – a ‘part world’ – that it nonetheless believes is whole. McGilchrist has remarked: ‘the left brain doesn’t know what the left brain doesn’t know.’ By the same token, the market doesn’t know what the market doesn’t know, but we have told ourselves it is all-knowing: ‘markets are the solution, government is the problem’. Our challenge is not to build a sustainable economy but to develop a sustainable culture that has an economy. If a left-brained culture has enabled – and been reinforced by – the ascendancy of the private-enabling market over non-market institutions within human self-organization, then we might attain a more sustainable culture by revitalizing the institutions left behind by that ascent. Indeed, before the ‘neoliberalism’ that has been ascendant for the duration of the ‘Great Acceleration’, there was an ’embedded liberalism’ in which market and non-market institutions were more finely balanced. Critically, the potential benefit of such a rebalancing is to reinvigorate those institutions that can complement the markets by standing up to them! The real value of government is its potential not to amplify market forces but to modulate them. The market is not only self-regulating, but also susceptible to positive reinforcement loops that can become runaway problems. Austin (2021) The Matrix of the Emissary - Market Primacy and The Sustainability Crisis 32.2 Markets as Entropy Maximisers Markets are randomising machines, they maximise entropy, and this fact alone is sufficient to explain some of the inequality we observe. Market transactions involve a transfer of monetary value. After any transaction one party may have more or less money than before. It’s quite easy to write a short simulation program that takes a large collection of individuals that start with equal amounts of money. We then pick two individuals at random. One is the buyer. We randomly choose a proportion of their money to spend. The seller gets that money. We then repeat, and pick another two individuals at random. And we keep doing this forever. After a short period, we can then measure the distribution of money across individuals. And we find, once again, the exponential distribution. Most individuals have very little money, and a small number have a great deal. So the activity of market exchange is acting just like the cocktail shaker: its mixing everything up, randomising things, and maximising the entropy of the system. You might think that this model of money exchange is far too simple to tell us anything about real markets. But you’d be wrong. Remarkably, we observe the exponential distribution in actual economies. The exponential is a great fit to the bottom 80% of the wealth distribution, which is the vast majority of the population. And this holds true for whichever capitalist country we look at. The fact that 80% of the wealth distribution of actual economies follows an exponential law is a very astonishing regularity. We might think that differences in wealth must arise from accidents of birth or personal virtue. But the principle of entropy maximisation tells us there’s a much more important causal factor at work. We quickly get extreme income inequality even in an economy with identical individuals with identical initial endowments of money. The points is that markets are randomising machines, they maximise entropy, and this fact alone is sufficient to explain some of the inequality we observe. So the anarchy of the market is the primary and essential cause of economic inequality. But why doesn’t the exponential law fit the entire wealth distribution? What about the remaining 20% of rich people? 32.2.1 The social relations of production as constraints Ian Wright 32.3 Competition Bargaining Tankus The central question this paper tackles are: what institutional mechanisms coordinate between different market actors to produce market prices and how does law, particularly competition law, shape these institutional mechanisms? This issue is surprisingly neglected because of the intuition built up by textbook microeconomics that market actors will coordinate markets without explicit governance institutions. Supposedly, they’re to do this automatically: simply acting in their own narrow self-interest. We tackle this myth head on by tackling the markets which are often claimed as “real world approximations” of textbook “perfect competition” markets: chartered exchange markets. While these markets have many buyers and sellers, in most other ways they diverge wildly from textbook market behavior. The vast majority of those buyers and sellers are not producers or consumers of the product — if there is a currently produced product at all — but market specialists (e.g. brokers and dealers) who’s explicit purpose is to smooth out market volatility. Recognizing these markets as jointly governed by chartered exchanges and financial firms helps us realise that governance architecture reliably manages all types of markets. The main type of price market focused on our paper are prices which are administered by business enterprises i.e. administered prices. Activities that happen “within” businesses are often illegal when conducted outside of businesses. It may seem obvious that a firm can set the price on the product it sells. But don’t forget the same conduct, conducted by the same individuals, using the same tools would be illegal if shared. This is known in competition law as the “per se rule” against “price fixing”. This differential treatment between firms and non-firms — or larger firms and smaller firms — is the firm exemption. This differential treatment is worsened by the fact that current competition law gives a relatively free hand to large businesses dictating terms to their suppliers or customers. These are known as vertical restraints. As an aside, in “industrial organization”, businesses which you interact with but don’t undertake the same activities are referred to as “vertical” because they are either “behind” you, or in “front” of you. Meanwhile your direct competitors are “horizontal” to you — because you are in the same business. Ergo coordinating with your direct competitors is referred to as horizontal coordination. Not all markets are vertically governed. Meanwhile the vast, vast majority of markets are not completely owned by one firm. Thus, vertical restraints and the firm exemption cannot explain all market governance in contemporary markets. And if it can’t, we are still left with a missing explanation for how markets are governed horizontally when explicit horizontal coordination is illegal. Posed in these terms, the answer is obvious. Horizontal implicit coordination between direct competitors is legal. This type of market governance, known popularly as price leadership, requires sufficiently concentrated firms to make coordinating implicitly viable. Or at least make the appearance of implicit coordination plausible. Because only firms can be price leaders, and a number of relatively concentrated price followers are likely required to maintain market order, this form of market governance is an extension of the firm exemption — and built on top of it. Most obviously, a cartel of the same size and market share could not be a price leader in the world of the firm exemption. Price leadership exemption. This is a really important point to emphasize. Price leadership by concentrated, hierarchical business enterprises is the most common form of market governance. That’s because competition law is designed in such a way that no other form of market governance can be conducted legally. At the same time, the price leadership exemption facilitates explicit price coordination among large concentrated firms, since they are presumed to be engaging in price leadership. Monetary policy, and to a certain extent fiscal policy, is obsessed with price stability. But only the type of price stability that appears in aggregate price indices. Our competition laws don’t value price stability. Some might respond to this paper by arguing that cracking down on price leadership will finally produce “competitive prices” and “true competition”. I don’t think that is the case. I think that is simply a recipe for more price instability and uncertainty, without very many benefits. Tankus (2021) Competion as Collective Bargaining (paper pdf) "],["demand.html", "33 Demand 33.1 Human Needs 33.2 Aggregate Demand Function", " 33 Demand 33.1 Human Needs I remember being fascinated when I discovered Manfred Max-Neef’s matrix of human needs. It’s such a rich description of the human animal that defies reduction to a single dimension. It puts homo economicus to shame. (Blair Fix) Manfred Max-Neef (2007) Development and Human Needs (pdf) 33.2 Aggregate Demand Function Since 1976, Robert Lucas—he of the confidence that the “problem of depression prevention has been solved”—has dominated the development of mainstream macroeconomics with the proposition that good macroeconomic theory could only be developed from microeconomic foundations. Arguing that “the structure of an econometric model consists of optimal decision rules of economic agents” (Lucas, 1976, p. 13), Lucas insisted that to be valid, a macroeconomic model had to be derived from the microeconomic theory of the behaviour of utility-maximizing consumers and profit-maximizing firms. In fact, Lucas’s methodological precept—that macro level phenomena can and in fact must be derived from micro-level foundations—had been invalidated before he stated it. As long ago as 1953 (Gorman, 1953), mathematical economists posed the question of whether what microeconomic theory predicted about the behaviour of an isolated consumer applied at the level of the market. They concluded, reluctantly, that it did not: Market demand functions need not satisfy in any way the classical restrictions which characterize consumer demand functions… The importance of the above results is clear: strong restrictions are needed in order to justify the hypothesis that a market demand function has the characteristics of a consumer demand function. Only in special cases can an economy be expected to act as an ‘idealized consumer’. The utility hypothesis tells us nothing about market demand unless it is augmented by additional requirements.’ (Shafer and Sonnenschein, 1993, p. 671-72) What they showed was that if you took two or more consumers with different tastes and different income sources, consuming two or more goods whose relative consumption levels changed as incomes rose (because some goods are luxuries and others are necessities), then the resulting market demand curves could have almost any shape at all. They didn’t have to slope downwards, as economics textbooks asserted they did. This doesn’t mean that demand for an actual commodity in an actual economy will fall if its price falls, rather than rise. It means instead that this empirical regularity must be due to features that the model of a single consumer’s behaviour omits. The obvious candidate for the key missing feature is the distribution of income between consumers, which will change when prices change. The individual demand curve is derived by assuming that relative prices can change without affecting the consumer’s income. This assumption can’t be made when you consider all of society—which you must do when aggregating individual demand to derive a market demand curve—because changing relative prices will change relative incomes as well. Since changes in relative prices change the distribution of income, and therefore the distribution of demand between different markets, demand for a good may fall when its price falls, because the price fall reduces the income of its customers more than the lower relative price boosts demand. The sensible reaction to this discovery is that individual demand functions can be grouped only if changing relative prices won’t substantially change income distribution within the group. Alan Kirman proposed such a response almost 3 decades ago: If we are to progress further we may well be forced to theories in terms of groups who have collectively coherent behavior. Thus demand and expenditure functions if they are to be set against reality must be defined at some reasonably high level of aggregation. The idea that we should start at the level of the isolated individual is one which we may well have to abandon. (Kirman, 1989, p. 138) Unfortunately, the reaction of the mainstream was less enlightened: rather than accepting this discovery, they looked for conditions under which it could be ignored. These conditions are absurd—they amount to assuming that all individuals and all commodities are identical. But the desire to maintain the mainstream methodology of constructing macro-level models by simply extrapolating from individual level models won out over realism. Macroeconomics cannot be derived from microeconomics. Steve Keen "],["inequality.html", "34 Inequality", " 34 Inequality Wright Stop talking about inequality, start talking about exploitation. Ian Wright (2017) The Social Architecture of Capitalism "],["inflation-1.html", "35 Inflation 35.1 Fiscal Theory of Price Level 35.2 Price Control", " 35 Inflation Inflation is about power, not money. 35.1 Fiscal Theory of Price Level Cochrane Abstract I introduce and summarize the fiscal theory of the price level. Fiscal theory states that the price level adjusts so that the real value of government debt equals the present value of real primary surpluses. Monetary policy remains important. The central bank can set an interest rate target, which determines expected inflation, and then innovations to the present value of surpluses pick unexpected inflation. Fiscal theory is a frictionless supply and demand foundation, on which we can add interesting in- gredients. Long-term debt is an important buffer and allows a higher interest rate to lower inflation without a fiscal shock. An s-shaped surplus process and time-varying interest rate are crucial to fitting data. One can easily integrate fiscal theory with standard new-Keynesian macroeconomic models. The models are observationally equivalent. That equivalence is a feature not a bug. It opens the door to easy transla- tion. It focuses our attention on direct information about government policy rather than statistical tests. It shows how to fix the current generation of fiscal theory mod- els to describe the whole sample, and better, not just periods of undesirable high inflation. Fiscal theory overturns many traditional doctrines of monetary policy. It accounts for the stability of inflation at the zero bound. Fiscal theory offers a warn- ing that containing a new inflation will be harder, as interest costs on a large debt and the fiscal costs of debt revaluation will be larger. Cochrane Memo Cochrane (2021) The Fiscal Theory of the Price Level: An Introduction and Overview (pdf) 35.2 Price Control Weber A critical factor that is driving up prices remains largely overlooked: an explosion in profits. Large corporations with market power have used supply problems as an opportunity to increase prices and scoop windfall profits. Today economists are divided into two camps on the inflation question: team Transitory argues we ought not to worry about inflation since it will soon go away. Team Stagflation urges for fiscal restraint and a raise in interest rates. But there is a third option: the government could target the specific prices that drive inflation instead of moving to austerity which risks a recession. As long as bottlenecks make it impossible for supply to meet demand, price controls for important goods should be continued to prevent prices from shooting up. The role of price controls would be “strategic”. It will not stop inflation, but it gains the time for the measures that do. Weber (2021) We have a powerful weapon to fight inflation: price controls. It’s time we consider it Krugman I am not a free-market zealot. But this is truly stupid. Krugman on Weber (Twitter Thread) Paul Krugman (???) Deleting, with extreme apologies, my tweet about Isabella Weber on price controls. No excuses. It’s always wrong to use that tone against anyone arguing in good faith, no matter how much you disagree — especially when there’s so much bad faith out there. Comment by Jonathan McCarty: (???) is against price controls for consumer goods, but supportive when it comes to the cost of labor, rent and money. So gov’t is too stupid to set the price of bread but smart enough to set the price of money? Bread shortages are not okay, but housing shortages are? Galbraith In The Guardian, Weber provides careful parallels to the spring of 1946, when Paul Samuelson – Krugman’s own chief mentor – signed a letter to The New York Times urging continued price controls, given ongoing bottlenecks and temporary shortages – precisely today’s situation… The point of strategic price control, then and now, was to prevent an outbreak of inflation, followed by loss of purchasing power and confidence. A further purpose now, not relevant yet in 1946, is to forestall counterproductive hikes in interest rates by the Federal Reserve… Krugman’s tweets, by contrast, are the trite repetition of textbook banalities. Kelton on behalf of Galbraith(Twitter Thread Galbraith 2001 So what is modern economics about? It seems to be, mainly, about itself Thirty years ago, Friedman-style monetarists wiped out all alternative theories of inflation. The ideas of “cost push” and “wage-price spirals,” on which the successful anti-inflation strategies of the 1960s had been based, disappeared. To this day, there exist no alternatives for fighting inflation, except higher interest rates, recession, and unemployment. These are the hard measures, the brutal measures, for which we have the monetarists to thank. Galbraith (2001) How the Economists Got It Wrong Smith Price controls: Simple theory If there’s one thing you should know about macroeconomics, it’s this: Convincing evidence is really really hard to come by, so people end up relying a lot on theory and making a lot of assumptions. Price controls are no different. So we can’t just point at evidence for whether price controls are good or bad; we have to think about how we believe the economy works. The basic theory of competitive supply and demand says that price ceilings cause shortages. Here’s the graph showing the theoretical gap between how much people want and how much they get when government caps the price of something: The basic logic here isn’t complicated. Government declares that milk shall be super-cheap. People say “Oooh, milk is super cheap!” and rush out to buy milk. The shelves empty out and there’s no more milk. The people who were late to the store can’t find any milk, and they get mad. The end. But this perfectly competitive model is often a bad description of reality. Sometimes, as we’ve seen with minimum wage, price controls don’t distort markets by a noticeable amount. In that case, the model we want to think about is more like a monopoly model. When there’s monopoly power in the economy, a price ceiling can actually move the price toward what it ought to be, and relieve shortages instead of exacerbating them. Monopolies make goods more expensive and limit the amount people can consume; a modest price ceiling can make goods less expensive while also making them more abundant. But does this make any sense when talking about inflation? Monopoly models like the one in the picture above are static, long-term equilibrium models; they don’t say much about the rate of change. It’s probably not plausible that monopoly power would change significantly in the course of one year due to supply bottlenecks. In other words, as Matt Bruenig points out, if powerful companies could have jacked up prices before now they would have done so; if their ability to jack up prices has increased, it’s probably not because they’ve suddenly become much more powerful. An economy with lots of monopoly power in various markets might have steeper supply curves in those markets, which in turn might make aggregate supply steeper, which would make inflation tend to be higher. But if this is how the economy works, would price controls in various markets reduce inflation at a time like now? Probably not, no. Go back and notice that in the monopoly model, the price ceiling doesn’t actually change the supply curve. Even if there are monopolies in each market, that doesn’t mean the macroeconomy overall acts like a monopolized market. There’s no one company that has a monopoly over aggregate production. So price controls, macroeconomically, are likely to reduce inflation only at the cost of causing a recession. That would be a bad idea; sure, we’d beat inflation, but we’d throw a ton of people out of work. If that’s what we want to do, we might as well use monetary policy. Simple theory suggests that that enacting economy-wide price controls just to bring inflation back down to 2% is not worth the damage it’ll cause. But simple theories like AD-AS aren’t always sufficient for determining policy. Real macroeconomies have a lot more going on. One possibility is that if price controls do cause empty shelves — as they will if they’re strong enough to overcome the amount of monopoly power in the economy — that this will cause people to engage in hoarding behavior. Hoarding could be especially bad. It would boost demand (because everyone is trying to hoard), which will lead to even more inflation, causing the government to respond with even more price controls, etc. That would be a very unpleasant spiral, even beyond the hardships and unfairness created by hoarding. This possibility of a price-control-inflation spiral has occurred to economists, but it’s very hard to measure. Many economists theorize that inflation is, at least sometimes, determined by people’s beliefs about monetary policy. If people think the government (especially the central bank) doesn’t care that much about fighting inflation, then they’ll raise prices now in anticipation of future cost increases, causing fear of inflation to become a self-fulfilling prophecy. This is one leading explanation for the high inflation of the 1970s — the oil shocks caused some prices to rise and the Fed didn’t respond, which convinced people that the Fed didn’t care that much about inflation, which caused inflation to spiral upward much more than the oil shocks should have caused just by themselves. So if price controls became the government’s primary tool for inflation-fighting — as Kelton suggests — it could send a very dangerous signal. It could convince the public that the government isn’t willing to use monetary policy to do the job. Evidence Argentina: price controls have only a small and temporary effect on inflation that reverses itself as soon as the controls are lifted. Second, contrary to common beliefs, we find that controlled goods are consistently available for sale. Third, firms compensate for price controls by introducing new product varieties at higher prices, thereby increasing price dispersion within narrow categories. Overall, our results show that targeted price controls are just as ineffective as more traditional forms of price controls in reducing aggregate inflation. Evicence Venezuela The utter failure of Venezuelan price controls should also serve as a reminder that there are real-world factors that don’t appear in macroeconomic models — for example, the black market. In the U.S., a vigorous, comprehensive regime of price controls would undoubtedly cause people to turn to cryptocurrency, and to technologies like the dark web, to evade the controls. History is hard to interpret, theory involves lots of assumptions, and macroeconomists have been largely derelict in their duty of studying inflation in recent decades (though I predict this will change quickly now). There are multiple obvious downsides and potentially catastrophic possible downsides. We don’t know for certain that price controls can’t work as an inflation-fighting tool. Smith (2021) Why price controls are a bad tool for fighting inflation Tooze As Eric Levitz makes clear in his excellent write-up of the debate, whether you find Weber’s op-ed convincing or not, there is a serious position to be argued with. The effort to assert the monopoly of conventional inflation-fighting disarms us. One could make a strong case for more stringent controls throughout the American health-care system. And price controls are themselves just one of many unorthodox approaches to inflation management. Reducing the monopoly power of price-gouging firms, channeling credit to sectors where demand outstrips supply, forcing (or strongly encouraging) workers to save a fraction of their paychecks, and direct public investment in expanded production are others. All of these measures have the potential for negative side effects and unintended consequences. But the same can be said of raising interest rates. If policymakers reflexively presume the wisdom of conventional tools, and dismiss the potential of unorthodox ones, we will all pay the price. I am impressed by recent BIS work which shows the common factor in recent price movements declining in significance. The question becomes which instruments might usefully address which drivers of which price increases. Weber starts by stressing rising profit margins as an important driver of general inflation. On that score I find the critique by BLS-economist and Substacker Joseph Politano wholly persuasive. It just isn’t likely that a general surge in profit margins is doing the damage here. Likewise, I find Politano’s breakdown of the sectoral logic of inflation highly persuasive as well as his skepticism towards price controls as a means of addressing inflation in energy prices, for instance. There is no doubt a case for driving down the price of pharmaceuticals in the US. Rent controls may be part of housing-policy trade-off in some cities. The meat lobby has an anti-trust case to answer. But I see little advantage in packing an array of discrete measures using existing instruments under the (deliberately) provocative rubric of “price controls”. I don’t think it is pejorative to describe the use of the term “price controls” as provocative. I take it to be the purpose of this language to provoke debate and break open the confines of conventional discourse. But as desirable as that kind of heterodox challenge may be in general terms, we will be kidding ourselves if we imagine that such measures are a “powerful weapon” to fight the spike in prices in 2022. Tooze (2022) Inflation &amp; Price Controls "],["innovation.html", "36 Innovation 36.1 US vs Scandinavia", " 36 Innovation 36.1 US vs Scandinavia Smith on Acemoglu In 2012, Daron Acemoglu, James Robinson, and Thierry Verdier came out with a paper about the different “varieties of capitalism”. The basic idea was that Scandinavia’s more safety net discouraged entrepreneurship, while America’s relative lack of government support forced people to be risk-takers, and that this explained America’s greater rate of innovation. This is the kind of theory economists tend to like, because it emphasizes tradeoffs, and because it tells a story that allows economists to place themselves in the political center, charting the optimal middle path between the kind-hearted Democrats who want to give out free stuff and the exacting Republicans who want to force people to work for their supper. But other economists and bloggers immediately started noting problems with the thesis — most importantly, the fact that the Nordic countries are generally more innovative than the U.S. by many measures. Those countries are small, so you don’t hear about their innovations as much, but they really punch above their weight. Acemoglu et al. were trying to explain a “fact” that didn’t really exist. Smith (2021) Cutthroat capitalism vs. cuddly capitalism "],["debt-and-interest.html", "37 Debt and Interest 37.1 Unpayable debt in a Stationary Economy", " 37 Debt and Interest 37.1 Unpayable debt in a Stationary Economy Hartley Abstract Under what circumstances are interest-bearing loans compatible with an economy without much growth? The question is becoming increasingly important given a tendency towards declining growth in industrialised economies and increasing evidence that continued growth is incompatible with environmental sustainability. Previous theoretical work suggests that when interest-bearing loans compound, this results in exponentially growing debts that are impossible to repay in the absence of economic growth. We here examine ten historical cases to assess support for this finding. We find that interest-bearing loans have typically resulted in unpayable debts in these non- and slow-growing economies. We further identify four broad category of measures to prevent or alleviate the problem of unpayable debts, and show how they have been employed in the past. Our Appendix compiles sources of debt regulation from across the world over five millennia. Hartley Memo Compound interest debt-based money is incompatible with a stationary economy but interest bearing debt-based money does not necessarily imply compound interest. Positive interest rates do not systematically lead to exponentially growing deposits, because taxation and consumption out of wealth and income can dampen the positive feedback loop of compound interest. Our starting point for this paper, then, is the longstanding body of literature which suggests that when interest compounds it can result in exponentially growing debts that are unpayable in the absence of economic growth. This body of theory has been developed to analyse modern economies, with the particular aim of better understanding what may happen if today’s economies stop growing. Rome, for example, had significant levels of financial intermediation and credit creation, with one recent comparative analysis concluding “that financial institutions in the early Roman Empire were better than those of eighteenth- century France and Holland. They were similar to those in eighteenth-century London and probably better than those available elsewhere in England” What particularly motivates us here is a desire to un­ derstand the consequences of positive interest in the absence of growth, and also to shed light on how these societies tried to mitigate the potential negative effects of interest-bearing loans. The charging of interest in the absence of substantial economic growth was accompanied by notable levels of unpayable debt in seven out of our ten cases. In these seven cases, there is evidence that in different periods this resulted in debtor dispossession and indenture, and at least some degree of social upheaval or revolt. The more extended lending is, the more individual problems of indebtedness are likely to translate into a bigger social problem. One might also argue that if lending markets worked efficiently, real interest rates in modern economies should converge towards the rate of real economic activity, which would suggest that real interest rates in a non-growing economy should tend towards zero. Current theories that suggest interest-bearing loans may become problematic in the absence of substantial growth have significant empirical support when tested against historical cases. Hartley and Kallis (2021) Interest-bearing loans and unpayable debts in slow-growing economies: Insights from ten historical cases (pdf) (SI pdf) "],["phillips-curve.html", "38 Phillips Curve", " 38 Phillips Curve Ratner Abstract Is the Phillips curve dead? If so, who killed it? Conventional wisdom has it that the sound monetary policy since the 1980s not only conquered the Great Inflation, but also buried the Phillips curve itself. This paper provides an alternative explanation: labor market policies that have eroded worker bargaining power might have been the source of the demise of the Phillips curve. We develop what we call the “Kaleckian Phillips curve”, the slope of which is determined by the bargaining power of trade unions. We show that a nearly 90 percent reduction in inflation volatility is possible even without any changes in monetary policy when the economy transitions from equal shares of power between workers and firms to a new balance in which firms dominate. In addition, we show that the decline of trade union power reduces the share of monopoly rents appropriated by workers, and thus helps explain the secular decline of labor share, and the rise of profit share. We provide time series and cross sectional evidence. Ratner (2022) Who killed the Phillips Curve (pdf) (pdf Slides) "],["productivity.html", "39 Productivity 39.1 Productivity-Pay Gap 39.2 IPR Stagnation 39.3 TFP", " 39 Productivity 39.1 Productivity-Pay Gap Using prices to aggregate ‘output’ leads to bizarre problems. On the one hand, it causes ‘productivity’ to be equivalent to average hourly income. This means that any connection between ‘productivity’ and wages is circular. On the other hand, the same decision causes ‘productivity’ to be ambiguous. Our measure of ‘productivity’ depends on arbitrary choices about how to adjust for price change. As a result, productivity trends (like the one in Figure 1) are riddled with uncertainty. ‘Productivity’ is used by both major schools of economic thought. Neoclassical economists use productivity to claim that the distribution of income is just. They argue that in a competitive economy, workers get what they produce. Marxists, in contrast, use productivity to claim that the distribution of income is unjust. They argue that in a capitalist economy, workers receive less than they produce (because capitalists extract a surplus). What’s interesting is that these two opposing theories commit the same sin. They define productivity in terms of income. Neoclassical economists do so explicitly, as I’ve described in this post. Marxists do so implicitly because they haven’t developed their own system of national accounts. Instead, Marxists who do empirical work use neoclassical measures of productivity. The result of this circular definition is that the analysis of productivity is a sleight of hand. ‘Productivity’ is just income relabelled. The ‘productivity-pay gap’ is a textbook example of this relabelling. It claims to show a growing gap between what workers ‘produce’ and what they get paid. But workers’ ‘productivity’ is actually measured in terms of income — the average hourly income. Blair Fix: Debunking Productivity Blair Fix: Productivity does not explain income Productive individuals, productive society? In the 1990s, geneticist William Muir conducted experiments on chickens to see what would improve egg-laying productivity. In one trial, he did exactly what the eugenicists recommend – he let only the most productive hens reproduce. The results were disastrous. Egg-laying productivity did not increase. It plummeted. Why? Because the resulting breed of hens was psychopathic. Instead of producing eggs, these “uber-hens” fought amongst themselves, sometimes to the death. The reason this experiment did not work is that egg-laying productivity is not an isolated property of the individual hen. It is a joint property of the hen and her social environment. In Muir’s experiment, the most productive hens laid more eggs not because they were innately more productive, but because they suppressed the productivity of less dominant chickens. By selecting for individual productivity, Muir had inadvertently bred for social dominance. The result was a breed of bully chicken that could not tolerate others. The lesson here is that in social animals, traits that can be measured among individuals (like productivity) may not actually be traits of the individual. Instead, they are joint traits of both the individual and their social environment. Here is evolutionary biologist David Sloan Wilson reflecting on this fact: “Muir’s experiments … challenge what it means for a trait to be regarded as an individual trait. If by ‘individual trait’ we mean a trait that can be measured in an individual, then egg productivity in hens qualifies. You just count the number of eggs that emerge from the hind end of a hen. If by “individual trait” we mean the process that resulted in the trait, then egg productivity in hens does not qualify. Instead, it is a social trait that depends not only on the properties of the individual hen but also on the properties of the hen’s social environment”. Blair Fix: Human Capital Theory RWER95 (pdf) 39.2 IPR Stagnation Schwartz Abstract Explanations for slow global growth (secular stagnation) correctly focus on income inequality and wage formation but are incomplete. They ignore the source of wages and fail to ask why a rising profit share has not produced more investment. Older but essential insights on stagnation from Keynes, Schumpeter and Veblen complement orthodox and post-Keynesian analyses to generate a more robust explanation based on the distributional conflict over profit among firms. These thinkers highlight the importance of corporate profit strategy and organizational structure for investment behavior. A politically mediated process of strategic interaction has transformed the old Fordist dual industrial structure into a tripartite structure composed of high profit volume firms with monopolies based on intellectual property rights (IPRs), physical capital-intensive firms protected by an investment barrier to entry, and low profit volume labor-intensive firms. Profit data from Compustat and Orbis show that IPR-based firms have a lower marginal propensity to invest. Other firms with smaller profit volumes forego investment from fear of creating excess capacity in a slow growth environment. High profit firms also tend to pay higher wages, creating income inequality. Changes in antitrust, employment and intellectual property law can remedy this situation. Schwartz (2021) Global secular stagnation and the rise of intellectual property monopoly (Paywall) 39.3 TFP Each period of productivity growth is bit slower than the last, meaning that the exponential growth rate is slowing down. Adjusting for the changing utilitization of capital and labor the growth rate of “true” TFP has stagnated even more than the above graph would suggest. In other words, we’ve been using our machines and buildings and stuff more intensively, disguising some of the true TFP slowdown. TFP is not the same thing as technology. The word “technology”, as we commonly understand it, includes stuff like computer chips, car engines, and procedures for making cement. Economists would broaden that definition to include things like business management techniques. But even with that broad definition, there’s plenty of stuff that can affect TFP that most of us would agree does not represent actual technology. For example: If the government adds a bunch of burdensome regulations or taxes, that reduces TFP. If people’s education level stops increasing, that lowers TFP growth. If the population gets older, that can reduce TFP (since older workers are, on average, less productive). If people spend more time goofing off at work, that can reduce measured TFP (since we’re overestimating the amount of labor input). If people stop moving from less productive places to more productive places (for example, if housing restrictions drive up rents and keep workers away from superstar cities), that reduces TFP. If a few big companies become more dominant, that can lower TFP, either via monopoly/monopsony power, or just by reducing dynamism in the economy. If demand shifts from sectors where technology is progressing rapidly (for example, manufacturing) to sectors where it’s progressing slowly (e.g. services), that can reduce TFP growth, even if the rate of technological innovation in each sector remains exactly the same. Basically we’re seeing a whole lot of things happen that tend to reduce TFP growth but that have nothing to do with slowing technological progress! We can invent economically useful stuff just as brilliantly as in the past, but if the above stuff happens, TFP will still slow down. In fact, in a recent book called “Fully Grown: Why a Stagnant Economy Is a Sign of Success”, the brilliant growth economist Dietrich Vollrath — whose excellent blog you should absolutely read — argues that most of the slowdown in TFP comes from slowing educational attainment, lower geographic mobility and economic dynamism, and and the shift from goods to services. Tyler Cowen’s 2011 book The Great Stagnation (the most subtle and circumspect of the stagnationist books), he identifies non-technological factors as contributing to the stagnation, and he predicts that both technology and productivity growth will bounce back. Stagnationists would be well-advised to read that book. Noah Smith "],["economic-regulation.html", "40 Economic Regulation 40.1 Climate protection impact on economic growth", " 40 Economic Regulation 40.1 Climate protection impact on economic growth Mudge (This article is part of a series in which DW is debunking myths surrounding climate change. Read also: Part 1 — Is global warming merely a natural cycle? Part 2 — Is half a degree of warming really such a big deal? Part 3 — Is China the main climate change culprit? Part 4 — Climate protection: Can I make a difference? ) The first major environmental protection rules hark back to the 1970s. Since then, a debate has raged about their potentially damaging impact on economic growth and competitiveness. One train of thought (Dechezleprêtre (2017)) says countries that adhere less stringently to environmental policies have a production and trade advantage over those nations that are taking climate action measures to reduce emissions. The concern in those countries is that their own emission-heavy industries will be put at a competitive disadvantage. This so-called pollution haven hypothesis predicts that if competing companies diverge only regarding the severity of environmental regulations they face, then those that are bound by relatively stricter measures will lose competitiveness. On the other hand, the so-called Porter hypothesis concludes that more stringent climate rules should encourage investment in developing new pollution-saving technologies. If these technologies lead to energy savings, they may help in turn to offset some of the climate protection costs. Then, there is also the issue of how much it might cost if we fail to mitigate climate impacts. Is GDP the only valid indicator? At first glance, using GDP as a measurement tool is an obvious choice to provide a cost-benefit analysis. The question is to what extent it provides an adequate measure of growth and prosperity. “It is the most developed indicator. I wouldn’t say that we should move away from that, but many of these damages that are associated with climate change are not internalized. This means that we as a global society will probably have costs due to lost biodiversity, for example, which are not directly reflected in the GDP,” Wilfried Rickels, director of the Global Commons and Climate Research Center at the Kiel Institute for the World Economy, told DW. Figure: From OECD - Obviously this is once again Nordhaus unvalid calculations (DH) Environmental protection itself contributes to economic growth As modern economies move toward a so-called resource-efficient and circular economy (RE-CE), there are concerns that — in the short term, at least — jobs will be lost across various sectors of the economy and that job creation will be minimal. However, an OECD report notes that it is important to distinguish between different sectors. Most jobs over the next two decades are projected to be created within the construction industry, and renewable power generation and services; while manufacturing sectors, agriculture, food production and fossil-fuel based power are expected to record job losses. The overriding question is how to balance economic growth with cutting carbon emissions, and ultimately, achieving climate neutrality. At this year’s World Economic Forum in Davos, Johan Rockström, director of the Potsdam Institute for Climate Impact Research, pointed to that contradiction. “It’s difficult to see if the current GDP-based model of economic growth can go hand in hand with rapid cutting of emissions,” he said. Mudge (2021) Does climate protection stifle economic growth? (Deutsche Welle) Dechezleprêtre Ever since the first major environmental regulations were enacted in the 1970s, there has been much debate about their potential impacts on the competitiveness of affected firms. Businesses and policy makers fear that in a world that is increasingly characterized by the integration of trade and capital flows, large asymmetries in the stringency of environmental policies could shift pollution-intensive production capacity toward countries or regions with less stringent regula- tion, altering the spatial distribution of industrial production and the subsequent international trade flows. This has caused concern, particularly among countries that are leading the action against climate change, because their efforts to achieve deep emission reductions could put their own pollution-intensive producers at a competitive disadvantage in the global economy. There are two different views in the environmental economics literature on the effects of asymmetric policies on the performance of companies competing in the same market: the pollution haven hypothesis and the Porter hypothesis. The pollution haven hypothesis, which is based on trade theory, predicts that more stringent environmental policies will increase compliance costs and, over time, shift pollution-intensive production toward low abatement cost regions, creating pollution havens and causing policy-induced pollution leakage. This is a particularly troubling problem for global pollutants such as carbon dioxide, because it means that on top of the economic impacts on domestic firms, abatement efforts will be offset to some extent by increasing emissions in other regions. In contrast, the Porter hypothesis (Porter and van der Linde (1995) Toward a new conception of the environment–competitiveness relationship. Journal of Economic Perspectives 9(4):97–118 ) argues that more stringent environmental policies can actually have a net positive effect on the competitiveness of regulated firms because such policies promote cost-cutting efficiency improvements, which in turn reduce or completely offset regulatory costs, and foster innovation in new technologies that may help firms achieve international technological leadership and expand market share. Some 20 years ago, in their review of the literature on the competitiveness impacts of envi- ronmental regulation in the United States, Jaffe et al. (1995) concluded that “there is relatively little evidence to support the hypothesis that environmental regulations have had a large adverse effect on competitiveness.” Since then, through hundreds of studies that have used ever larger datasets with increasingly fine levels of disaggregation, employing up-to-date econometric techniques, and covering a wider set of countries, this conclusion has only become more robust. This article has reviewed the recent empirical literature on the impacts of environmental regulations on firms’ competitiveness, as measured by trade, industry location, employment, productivity, and innovation. The cost burden of environmental policies has often been found to be very small. The recent evidence shows that taking the lead in implementing ambitious environmental policies can lead to small, statistically significant adverse effects on trade, employment, plant location, and productivity in the short run, particularly in pollu- tion- and energy-intensive sectors. However, the scale of these impacts is small compared with other determinants of trade and investment location choices such as transport costs, proximity to demand, quality of local workers, availability of raw materials, sunk capital costs, and agglomeration. Moreover, the effects tend to be concentrated on a subset of sectors for which environmental and energy regulatory costs are significant—a small group of basic industrial sectors characterized by very energy-intensive production processes, limited ability to fully pass through pollution abatement costs to consumers (whether due to regulation or international competition), and a lack of innovation and investment capacity to advance new production processes (Sato et al., 2015a). For these subsectors, where pollution leakage and competitiveness issues represent a genuine risk, a critical avenue for future research is to assess and evaluate the various policy options available to prevent adverse impacts on trade and investment without dampening the incentives to develop cleaner processes and products. This article has also shown that there is strong evidence that environmental regulations induce innovation activity in cleaner technologies. Thus far the benefits from these innova- tions do not appear to be large enough to outweigh the costs of regulations for the regulated entities. Of course, this does not preclude the ability of environmental regulations to foster the development of global leaders in innovation, but it does suggest that the evidence for the most controversial interpretation of the Porter hypothesis (i.e., that environmental regula- tions can lead to an increase in firms’ competitiveness) is lacking. As regulatory designs and combinations continue to be explored, further research will be needed to identify the com- binations of research and development and environmental policies that best encourage in- novation in green technologies. This review raises the question of why the effects of environmental regulations on inter- national industry relocation have been found to be so small and narrow given the strong concerns about competitiveness in public policy circles. One explanation could be that reg- ulated companies have an incentive to overstate the potential competitiveness impacts of regulations as a strategy to lobby against stringent policies by attributing unpopular off- shoring decisions to public policy rather than to underlying economic factors such as the shifting locus of supply and demand in global manufacturing or decreasing transport costs. An alternative explanation for the lack of empirical support for the large pollution haven effects discussed in the literature is that environmental policy is endogenous, i.e., governments strategically set stringency levels to be low (high) where there is a high (low) risk of competitiveness distortions. This argument suggests that competitiveness concerns could trigger a “race to the bottom” in global environmental protection efforts. To avoid such an outcome, further research is needed to accurately measure and monitor the competitiveness effects of environmental regulations to help ensure that policy is based on robust evidence. Dechezleprêtre (2017) The Impacts of Environmental Regulations on Competitiveness (pdf) "],["savings.html", "41 Savings", " 41 Savings Stropoli While many economists think more saving leads to productive investment, Sufi, Princeton’s Atif Mian, and Harvard’s Ludwig Straub make a different argument. They find that these savings are largely unproductive, being remade by the financial system into household and government debt. And their research outlines a cycle whereby the savings of the top 1 percent fuel the debt and dissavings of the lower 90 percent, which in turn leads to more savings at the top. From the 1980s through 2007, the top 1 percent financed a large portion of the overall rise in household debt for the lower 90 percent, according to the researchers. And as the rich have accumulated capital, the less wealthy have accumulated fewer assets, which means they experience less financial stability overall. Thus, the work argues, the savings glut of the rich, and its role in financing unproductive debt and dissavings of the nonrich, leads to instability not only for the less economically privileged but also for the broad economy. Mian and Sufi argue in 2018 research that a rapid flow of foreign funds into the US triggered a credit-supply expansion that boosted household debt, which they say was a major factor in igniting the financial crisis. From 1982 to 2016, the glut of the US rich was, on average, 60–75 percent of the size of the global glut. And at times in the 1990s and 2010s, the amount rich Americans put away even exceeded the global glut. Credit Suisse’s 2020 wealth report finds that the US has about 20 million millionaires, 40 percent of the global total. Meanwhile, the Billionaire Census 2020 from Wealth-X, which provides information and insight on the world’s wealthiest individuals, finds the US has about 28 percent of the world’s billionaires, who hold a 36 percent share of global billionaire wealth. The world now has a record 2,755 billionaires, according to Forbes. The top 1 percent of households in the US have just as much influence as emerging-market economies in fueling the debt of the bottom 90 percent. More savings, less investment Ideally, all those savings would be channeled into productive investments such as research and development, or practical equipment, or new roads, or even new yachts—investments that would promote growth in the economy. However, from 2000 through 2016, the average annual savings of the top 1 percent exceeded average annual net domestic investment as a percentage of GDP. While the rich saved more, investment in productive assets declined. Those savings were put to use financing both household and government debt. Between 2000 and 2016, they find that claims on household and government debt account for nearly two-thirds of the rise in asset accumulation of the top 1 percent in the US. In the 25 years leading up to the 2008–09 financial crisis the top 1 percent financed almost a third of the rise in household debt owed by the bottom 90 percent. In the years since the crisis, and since the housing bubble burst, the savings of the rich have gone more toward government debt Not all household and government debt is unproductive, of course. More than one entrepreneur has financed a startup on a credit card or with a personal loan. However, much household debt goes toward instruments such as mortgages and home equity loans, which can be used speculatively, in which case they are less productive than, say, investments in manufacturing plants or technology. Thus, the researchers argue that mortgages, while enabling homeownership, can also help perpetuate a cycle of wealth inequality. The rich are seeking returns on their excess savings because, as Sufi says, many of them “just cannot spend all the money they make.” The US government, by providing tax breaks on debt interest, and by encouraging banks to lend via debt financing, promotes less-productive investment Nonfinancial corporations have increased their holdings of money market funds and time deposits by 10 percentage points of national income since 1995. Since the early 2000s, the amount of corporate saving not invested in new capital has increasingly accumulated as cash. Say a corporation issues equity to the wealthy, but instead of spending the proceeds on research or equipment, puts that money into a time deposit at a bank, which in turn uses it to fund a mortgage for a less-affluent household. This is how the rich become lenders. Some politicians, economists, and pundits say that people are borrowing (and consuming) irresponsibly. But banks with all that cash on hand work to expand the credit market and realize returns on the savings glut of the rich. The bottom 90 percent are being convinced to borrow more and more, through lower interest rates, easier credit, and more advertising. From the 1980s through 2007, the net amount of household debt that the top 1 percent held as a financial asset rose by 15 percentage points of national income, while at the same time the amount of household debt that the bottom 90 percent owed as a liability rose by 40 percentage points. The so-called accumulated dissavings of the bottom 90 percent from 1983 to 2015, relative to the average level from 1973 to 1982, was over twice the national income, the researchers say. The debt trap As the savings of the rich go toward the borrowing of the nonrich, there may be a GDP boost in the short run, as it does encourage consumption. But the debt becomes a drag on future demand. The cycle of unproductive debt makes it hard for consumer demand to support full employment in the economy, and it ultimately forces central banks to lower interest rates. While lower rates may strengthen demand for a time, consistently low rates may be problematic. Persistent low demand can foster a high-debt liquidity trap—or debt trap—in which economies are stuck in long periods of sluggish growth. Stropoli (2021) How the 1 percent’s savings buried the middle class in debt Inequality Wealth among the superrich has been fueled by a marriage of in-demand skills, globalization, and technology—the combination of which are allowing businesses to scale up as never before. Skills, say many economists, are critical to the modern economy. As the US economy grows, jobs are going unfilled as companies scramble to find skilled people to hire. There’s a flip side to this: as certain skills have become scarce, this has raised the amount companies are willing to pay people who have them. The situation has similarly raised the amount of profits skilled company owners can make, and technology and globalization are further magnifying the value of in-demand skills. If this is true, the 0.01 percent are most likely benefiting from what economists call “skill-biased technological change”—the increasing return on certain skills in an economy driven by technology and globalization Under this well-established theory, a shortage of in-demand skills raises the value of those skills in rapidly expanding markets, and new technology helps some workers’ productivity grow much more than others’, exacerbating inequality. In the Information Age, the change has been particularly pronounced. “In business, you can use technology to do things you couldn’t do 30 years ago,” says Steve Kaplan. “You can scale your business using technology, and you can use people in India and China and all over the world—you couldn’t do that as effectively 30 years ago.” This, he argues, has been spectacularly positive for poorer people in developing countries. In 1990, the World Bank estimated that roughly 35 percent of the world lived in extreme poverty. Today, less than 11 percent of the world’s population is so impoverished. And it has been good for wealthy residents of developed countries. For them, the result has taken the form of the “superstar” or “winner-take-all” phenomenon, first identified in a landmark 1981 paper by the late Sherwin Rosen, who taught at the University of Chicago. “In certain kinds of economic activity there is concentration of output among a few individuals,” wrote Rosen. “Relatively small numbers of people earn enormous amounts of money and dominate the activities in which they engage.” Technology allows a hedge fund to be able to manage $20 billion and invest it,” says Steve Kaplan. “I don’t think people had the systems and information to do that 20 to 30 years ago. Now they have the systems and the information to do that. That technological change is here and is not going away. If anything, it’s getting stronger.” Gold (2021) The 000.1 Pct "],["wealth.html", "42 Wealth", " 42 Wealth "],["climate-economics.html", "43 Climate Economics 43.1 A Blocking Neoclassical Framework 43.2 Long-term Economic effects of Climate Change 43.3 Carbon Tax 43.4 Finnish Carbon Tax 43.5 Instrument Choice Delays 43.6 SSPs 43.7 Guard Rail Economics 43.8 Tipping Points 43.9 Keynesian Decarbonization 43.10 Discount Rate 43.11 Geoff Man on Nordhaus", " 43 Climate Economics 43.1 A Blocking Neoclassical Framework Brookes and Wagner With its fixation on equilibrium thinking and an exclusive focus on market factors that can be precisely measured, the neoclassical orthodoxy in economics is fundamentally unequipped to deal with today’s biggest problems. Change within the discipline is underway, but it cannot come fast enough. The economics discipline has failed to understand the climate crisis – let alone provide effective policy solutions for it – because most economists tend to divide problems into small, manageable pieces. Rational people, they are wont to say, think at the margin. What matters is not the average or totality of one’s actions but rather the very next step, weighed against the immediate alternatives.Such thinking is indeed rational for small discrete problems. Compartmentalization is necessary for managing competing demands on one’s time and attention. But marginal thinking is inadequate for an all-consuming problem touching every aspect of society.Economists also tend to equate rationality with precision. The discipline’s power over public discourse and policymaking lies in its implicit claim that those who cannot compute precise benefits and costs are somehow irrational. This allows economists – and their models – to ignore pervasive climate risks and uncertainties, including the possibility of climatic tipping points and societal responses to them. And when one considers economists’ fixation with equilibrium models, the mismatch between the climate challenge and the discipline’s current tools becomes too glaring to ignore.Yes, a return to equilibrium – getting “back to normal” – is an all-too-human preference. But it is precisely the opposite of what is needed – rapidly phasing out fossil fuels – to stabilize the world’s climate.These limitations are reflected in benefit-cost analyses of cutting emissions of carbon dioxide and other greenhouse gases. The traditional thinking suggests a go-slow path for cutting CO2. The logic seems compelling: the cost of damage caused by climate change, after all, is incurred in the future, while the costs of climate action occur today. The Nobel prize-winning verdict is that we should delay necessary investment in a low-carbon economy to avoid hurting the current high-carbon economy. The very structure of academic economics all but guarantees that marginal thinking continues to dominate. The most effective way to introduce new ideas into the peer-reviewed academic literature is to follow something akin to an 80/20-rule: stick to the established script for the most part; but try to push the envelope by probing one dubious assumption at a time. Needless to say, this makes it extremely difficult to change the overall frame of reference, even when those who helped establish the standard view are looking well beyond it themselves. Because equilibrium thinking underpins the traditional climate-economic models that were developed in the 1990s, these models assume that there are tradeoffs between climate action and economic growth. They imagine a world where the economy simply glides along a Panglossian path of progress. Climate policy might still be worthwhile, but only if we are willing to accept costs that will throw the economy off its chosen path.Against the backdrop of this traditional view, recent pronouncements by the International Monetary Fund and the International Energy Agency are nothing short of revolutionary. Both institutions have now concluded that ambitious climate action leads to higher growth and more jobs even in the near term.The logic is straightforward: climate policies create many more jobs in clean-energy sectors than are lost in fossil-fuel sectors, reminding us that investment is the flipside of cost. That is why the proposal for a $2 trillion infrastructure package in the United States could be expected to spur higher net economic activity and employment. Perhaps more surprising is the finding that carbon pricing alone appears to reduce emissions without hurting jobs or overall economic growth. The problem with carbon taxes or emissions trading is that real-world policies are not reducing emissions fast enough and therefore will need to be buttressed by regulation. The framework of neoclassical economics is still blocking progress. The discipline is long overdue for its own tipping point toward new modes of thinking commensurate with the climate challenge. Brookes and Wagner (2021) Economics needs a Climate Revolution 43.2 Long-term Economic effects of Climate Change Kahn Abstract We study the long-term impact of climate change on economic activity across countries, using a stochast ic growth model where labour productivity is affected by country-specific climate variables—defined as deviations of temperature and precipitation from their historical norms. Using a panel data set of 174 countries over the years 1960 to 2014, we find that per-capita real output growth is adversely affected by persistent changes in the temperature above or below its historical norm, but we do not obtain any statistically significant effects for changes in precipitation. Our counterfactual analysis suggests th at a persistent increase in average global temperature by 0.04°C per year, in the absence of mitigation policies, reduces world real GDP per capita by 7.22 percent by 2100. On the other hand, abiding by the Paris Agreement, thereby limiting the temperature increase to 0.01°C per annum, reduces the loss subst antially to 1.07 percent. These effects vary significantly across countries. We also provide supplement ary evidence using data on a sample of 48 U.S. states between 1963 and 2016, and show that climate chan ge has a long-lasting adverse impact on real output in various states and economic sectors, and on labo r productivity and employment. Kahn Memo By using deviations of climate variables from their respective historical norms, while allowing for nonlinearity, we avoid the econometric pitfalls associated with the use of trended variables, such as temperature, in output growth equations. As it is well known, and is also documented in our paper, temperature has been trending upward strongly in almost all countries in the world, and its use as a regressor in a growth regression can lead to spurious estimates. To measure the damage caused by climate change, economists have sought to quantify how aggregate economic growth is being a¤ected by rising temperatures and changes in rainfall patterns; see a recent survey by Dell et al. (2014) The literature which attempts to quantify the e¤ects of climate change (temperature, pre- cipitation, storms, and other aspects of the weather) on economic performance (agricultural production, labour productivity, commodity prices, health, con‡ict, and economic growth) is relatively recent and mainly concerned with short-run e¤ects Moreover, there are a number of grounds on which the econometric evidence of the e¤ects of climate change on growth may be questioned. Firstly, the literature relies primarily on the cross-sectional approach and as such does not take into account the time dimension of the data (i.e., assumes that the observed relationship across countries holds over time as well) and is also subject to the endogeneity (reverse causality) problem given the possible feedback e¤ects from changes in output growth onto the climate variable. Secondly, the …xed e¤ects (FE) estimators used in more recent panel-data studies im- plicitly assume that climate variables are strictly exogenous, and thus rule out any reverse causality from economic growth to rising average temperatures. n his computable general equilibrium work, Nordhaus accounts for the fact that faster economic activity increases the stock of greenhouse gas (GHG) emis- sions and thereby the average temperature. At the same time, rising average temperature could reduce real economic activity. This equilibrium approach has important implications for the econometric speci…cation of climate change–economic growth relationship. In fact, recent studies on climate science provide strong evidence that the main cause of contemporary global warming is the release of greenhouse gases to the atmosphere by human activities. Consequently, when estimating the impact of climate change on economic growth, temperature (T it ) may not be considered as strictly exogenous, but merely weakly exogenous/predetermined to income growth; in other words economic growth in the past might have feedback e¤ects on future temperature. While it is well known that the FE estimator su¤ers from small-T bias in dynamic panels with N (the cross-section dimension) larger than T (the time series dimension). This bias exists regardless of whether the lags of the dependent variable are included or not, so long as one or more regressor is not strictly exogenous. In such cases, inference based on the standard FE estimator will be invalid and can result in large size distortions unless N=T ! 0, as N; T ! 1 jointly. Therefore, caution must be exercised when interpreting the results from studies that use the standard FE estimators in the climate change–economic growth literature given that N is often larger than T . Thirdly, econometric speci…cations of the climate change–macroeconomic relation are often written in terms of real GDP per capita growth and the level of temperature, T it , and in some cases also T it 2 ; see, for instance, Dell et al. (2012) and Burke et al. (2015). But if T it is trended, which is the case in almost all countries in the world (see Section 3.1), inclusion of T it in the regression will induce a quadratic trend in equilibrium log per capita output (or equivalently a linear trend in per capita output growth) which is not desirable and can bias the estimates of the growth–climate change equation. Finally, another major drawback of this literature is that the econometric speci…cations of the climate change–growth relation are generally not derived from or based on a theoretical growth model. Either an ad hoc approach is used, where real income growth is regressed on a number of arbitrarily–chosen variables, or a theoretical model is developed but not put to a rigorous empirical test. We contribute to the climate change–economic growth literature along the following di- mensions. Firstly, we extend the stochastic single-country growth models of Merton (1975), Brock and Mirman (1972), and Binder and Pesaran (1999) to N countries sharing a common technology but di¤erent climate conditions. Our theoretical model postulates that labour productivity in each country is a¤ected by a common technological factor and country- specific climate variables, which we take to be average temperature, T it , and precipitation, P it , in addition to other country-specific idiosyncratic shocks. As long as T it and P it remain close to their respective historical norms (regarded as technologically neutral), they are not expected to a¤ect labour productivity. However, if climate variables deviate from their his- torical norms, the e¤ects on labour productivity could be positive or negative, depending on the region under consideration. For example, in a historically cold region, a rise in temper- ature above its historical norm might result in higher labour productivity, whilst for a dry region, a fall in precipitation below its historical norms is likely to have adverse e¤ects on labour productivity. 2 Secondly, contrary to much of the literature which is mainly concerned with short-term growth e¤ects, we explicitly model and test the long-run growth e¤ects of persistent increases in temperature. Thirdly, we use the half-panel Jackknife FE (HPJ-FE) estimator proposed in Chudik et al. (2018) to deal with the possible bias and size distortion of the commonly-used FE estimator (given that T it is weakly exogenous). When the time imension of the panel is moderate relative to N , the HPJ-FE estimator e¤ectively corrects the Nickel-type bias if regressors are weakly exogenous, and is robust to possible feedback e¤ects from aggregate economic activity to the climate variables. Our results suggest that a persistent change in the climate has a long-term negative e¤ect on per capita GDP growth. Our empirical findings apply equally to poor or rich, and hot or cold countries. We show that an increase in average global temperature of 0:04 C per year— corresponding to the Repre- sentative Concentration Pathway (RCP) 8.5 scenario (see Figure 1), which assumes higher greenhouse gas emissions in the absence of mitigation policies— reduces world’s real GDP per capita by 7:22 percent by 2100. Limiting the increase to 0.01 C per annum, which corre- sponds to the December 2015 Paris Agreement, reduces the output loss substantially to 1:07 percent. To put our results into perspective, the conclusions one might draw from most of the existing climate change–macroeconomy literature are the following: (i) when a poor (hot) country is 1 C warmer than usual, its income growth falls by 1–2 percentage points in the short- to medium-term; (ii) when a rich (temperate) country is 1 C warmer than usual, there is little impact on its economic activity; and (iii) the GDP e¤ect of increases in average temperatures (with or without adaptation and/or mitigation policies) is relatively small— a few percent decline in the level of GDP per capita over the next century (see, Figure 2). In contrast, our counterfactual estimates suggest that all regions (cold or hot, and rich or poor) would experience a relatively large fall in GDP per capita by 2100 in the absence of climate change policies (i.e., the RCP 8.5 scenario). However, the size of these income e¤ects varies across countries depending on the projected paths of temperatures. Burke et al. (2015) consider an alternative panel specification that adds quadratic climate variables to the equation and detect: (i) non-linearity in the relationship; (ii) di¤erential impact on rich versus poor countries; and (iii) noisy medium-term growth e¤ects— their higher lag order (between 1 and 5) estimates reported in Supplementary Table S2, show that only 3 out of 18 estimates are statistically significant. Overall, apart from the econometric shortcomings of existing studies, robust evidence for the long-run growth e¤ects of climate change are nonexistent in the literature. However, our results show that an increase in temperature above its historical norm is associated with lower economic growth in the long run— suggesting that the welfare e¤ects of climate change are signi…cantly underestimated in the literature. Therefore, our findings call for a more forceful policy response to climate change. Kahn (2019) LONG-TERM MACROECONOMIC EFFECTS OF CLIMATE CHANGE: A CROSS-COUNTRY ANALYSIS (pdf) Dell Abstract A rapidly growing body of research applies panel methods to examine how temperature, precipitation, and windstorms influence economic outcomes. These studies focus on changes in weather realizations over time within a given spatial area and demonstrate impacts on agricultural output, industrial output, labor productivity, energy demand, health, conflict, and economic growth, among other outcomes. By harnessing exogenous variation over time within a given spatial unit, these studies help credibly identify (i) the breadth of channels linking weather and the economy, (ii) heterogeneous treatment effects across different types of locations, and (iii) nonlinear effects of weather variables. This paper reviews the new literature with two purposes. First, we summarize recent work, providing a guide to its methodologies, datasets, and findings. Second, we consider applications of the new literature, including insights for the “damage function” within models that seek to assess the potential economic effects of future climate change. Dell Memo The difficulty in identifying causative effects from cross-sectional evidence has posed substantial and long-standing challenges for understanding the historical, contemporary, and future economic consequences of climate and climate change. In the last few years, there has been a wave of new empirical research that takes a different approach. These new studies use panel methodologies, exploiting high-frequency (e.g., year-to-year) changes in temperature, precipitation, and other climatic variables to identify these variables’ economic effects. As nomenclature, this new literature uses “weather variation” to describe shorter-run temporal variation. The word climate is reserved for the distribution of outcomes, which may be summarized by averages over several decades, while weather describes a particular realization from that distribution and can provide substantial variability. The primary advantage of the new lit- erature is identification. By exploiting exogenous variation in weather outcomes over time within a given spatial area, these methods can causatively identify effects of temperature, precipitation, and windstorm variation on numerous outcomes, including agricultural output, energy demand, labor productivity, mortality, industrial output, exports, conflict, migration, and economic growth. This literature has thus provided a host of new results about the ways in which the realizations of temperature, precipita- tion, storms, and other aspects of the weather affect the economy. This literature has important implications for the “damage function” in climate change models. The opportunity here is to bring causative identification to the damage functions, elucidating the set of important climate–economy channels and their functional forms. The challenge lies in bridging from the evidentiary basis of short-run weather effects to thinking about longer-run effects of changes in the distribution of weather, which may be either larger (e.g., due to intensification effects) or smaller (e.g., due to adaptation) than the short-run impacts. While certain climate change aspects are difficult to assess, we examine a number of empirical methodologies that can help bridge toward longer-run effects while maintaining careful identification. climate studies often seek to estimate the contemporaneous effect of temperature on economic activity for the purpose of assessing the potential impacts of forecasted temperature changes over the next several decades. The ­ cross-sectional relationship, which represents a very long-run equilibrium, may incorporate processes that are too slow to accurately inform the time scale of interest, or it may include historical processes (such as colonialism) that will not repeat themselves in modern times. To the extent that one is interested in iso- lating the impact of climatic variables such as temperature—apart from the many other factors that they are correlated with and have influenced over the very long run—a different approach is to use longitudinal data to investigate the effects of weather shocks. This approach, which is the focus of this review, has emerged in recent years and emphasizes variation over time within a given spatial entity. The literature uses a nomenclature of “weather varia- tion” for shorter-run temporal variation, as opposed to “climate variation,” where the word climate is used to describe the distribution of outcomes while weather refers to a particular realization from that distribution. A related issue is the inclusion of lags of the dependent variable, ​ y​ it ​ . Including these lags biases coefficient estimates in short panel models, 4 yet excluding the lagged dependent variable may also bias the estimates if it is an important part of the data-generating pro- cess. While what comprises a “short” panel will depend on the data-generating process, Monte Carlo experiments suggest that the bias can be nonnegligible with panel lengths of T = 10 or even T = 15. A further implementation question involves the appropriate functional form for the weather variables. One common approach measures ​ C ​ it ​ in “levels” (e.g., degrees Celsius for temperature or millime- ters for precipitation). In the panel set up, the identification thus comes from devia- tions in levels from the mean. 7 Another common approach, aimed at revealing nonlinear effects, considers the frequencies at which the weather realizations fall into different bins. A different approach emphasizes “anom- alies,” where the weather variable is cal- culated as its level difference from the within-spatial-area mean and divided by ­ the within-spatial-area standard deviation. The first part—the difference in mean—is already captured in a broad sense by the panel model. The second part—scaling by the standard deviation—takes a particular view of the underlying climate–economy model where level changes matter not in an absolute sense but in proportion to an area’s usual variation. Alternatively, outcome-specific approaches may be preferred where existing research provides guidance. For example, knowledge of biological processes in agriculture sug- gest refined temperature measures such as “degree-days” for crop growth, possibly with crop-specific thresholds. s a general rule, imposing specific func- tional forms on the data, such as crop degree- days, is useful to the extent that one has confidence in the specific model of the pro- cess that translates weather to economic out- comes. The more agnostic about the model, the more general the researcher would like to be about the functional form. There are two notable interpretative issues with the panel models that, while not calling into question the experimental validity of the regression design, do raise questions about their external validity for processes such as global warming. One interpretive challenge is whether and how the effects of medium- or long-run changes in climatic variables will differ from the effects of short-run fluctuations. A second issue is that panel models, in focusing on idiosyncratic local variation, also neutral- ize broader variation that may be of poten- tial interest, including general equilibrium effects that spill across spatial borders or are global in nature, like effects on commodity prices. Data There are currently four principal types of weather data: ground station data, gridded data, satellite data, and reanalysis data. The most basic type of data are from ground stations, which typically directly observe tem- perature, precipitation, and other weather variables such as wind speed and direction, humidity, and barometric pressure. Gridded data provide more complete coverage by interpolating station information over a grid. Satellite data use satellite-based readings to infer various weather variables. Finally, reanalysis data combine information from ground stations, satellites, weather balloons, and other inputs with a climate model to estimate weather variables across a grid. Different interpolation schemes can produce different estimates, particularly in short time periods and particularly for precipitation. Precipitation has a far greater spatial variation than temperature, especially in rugged areas, and thus is more difficult to interpolate. While satellite data can provide important weather information for areas with a limited ground network, satellite data are not necessarily a panacea. Satellites were launched relatively recently, so their data does not extend back nearly as far historically as other ­datasets. Furthermore, an individual ground station is more accurate than the satellite data for that particular location, in part because satellites do not directly measure temperature or precipitation, but rather make inferences from electromag- netic reflectivity in various wavelength bands. Lastly, a s ­ atellite-based series is not drawn from a single satellite, but rather from a series of satellites. Sensors have changed subtly over the years and, within a particular satellite, corrections are needed due to subtle changes in the satellite’s orbit over time and other factors. The key difference between reanalysis and gridded data is that, rather than use a statistical procedure to interpolate between observations, a climate model is used. One approach is to aggregate spatially; that is, to overlay administrative or other boundaries with the gridded weather dataset and take a simple area-weighted average of weather variables within the administrative unit, which can be done easily using GIS soft- ware. However, this approach will lead large areas with little economic activity and sparse populations (such as deserts, rain forests, or the Arctic) to dominate the weather aver- ages of large spatial units such as the United States, Russia, and Brazil. A second approach is, therefore, to aggregate using a fixed set of population weights, so that the relevant concept is the average weather experienced by a person in the administrative area, not the average weather experienced by a place. Overall, the studies discussed in this sec- tion document that temperature, precipi- tation, and extreme weather events exert economically meaningful and statistically significant influences on a variety of eco- nomic outcomes. These impacts illustrate the multifaceted nature of the ­ weather– economy relationship, with numerous appli- cations for understanding historical, present, and future economic outcomes and possible policy responses. For example, the effects of weather variables on mortality rates, labor productivity, energy demand, and agricul- tural output can inform investments and policy design around public health, air-con- ditioning, energy infrastructure, and agricul- tural technologies. Moreover, these studies can help inform classic issues of economic development, especially the role of geo- graphic features in influencing development paths. Finally, these analyses may inform estimates of the economic costs of future climatic change. The possibility of future climatic change has been a primary motive for the recent, rapid growth of this literature. Results Cross-country empirical analyses show a strong negative relationship between hot cli- mates and income per capita. Panel studies exploit the exogeneity of cross-time weather variation, allowing for causative identification. In a world sample from 1950 to 2003, Dell, Jones, and Olken (2012) examine how annual variation in temperature and precipitation affects per capita income. They show that being 1°C warmer in a given year reduces per capita income by 1.4 percent, but only in poor countries. Moreover, estimating a model with lags of temperature, they find that this large effect is not reversed once the temperature shock is over, suggesting that temperature is affecting growth rates, not just income levels. 22 Growth effects, which compound over time, have potentially first-order consequences for the scale of eco- nomic damages over the longer run, greatly exceeding level effects on income, and are thus an important area for further modeling and research. While the production function is often calibrated through the use of experimental data, it has been criticized for not realistically modeling real farmer behavior in real settings. For example, many studies do not allow farmers to adopt new crops when the temperature input into the production function changes, nor do they allow farmers to switch their cultivated land to livestock or nonfarm use. To address these concerns, Mendelsohn, Nordhaus, and Shaw (1994) developed a second approach, which they called the Ricardian approach, that instead used cross-sectional regressions with land values to recover the net impacts of climate on agri- cultural productivity. By analyzing farm land prices as a function of climate and a host of other characteristics, they estimated that the impacts of climate change would be much smaller than those estimated by the production function approach and might even be positive. In estimating a cross-sectional relationship like equation (2) for irrigated areas, which transport water from other locations, the localized climate is not the key determinant of production. Understanding nonlinearities becomes important when con- sidering the impact of global climate change because a right-shift in the distribution of average temperature causes a disproportionate increase in the number of very hot days. The possibility of adaptation was a major argument for the approach of Mendelsohn, Nordhaus, and Shaw (1994), since presumably, changes in land values would incorporate future adaptation effects. Modern lab experiments have investi- gated the impact of temperature on pro- ductivity. Subjects are typically randomly assigned to rooms of varying temperatures and asked to perform cognitive and physical tasks. Examples of tasks shown to respond adversely to hot temperatures in laboratory settings include estimation of time, vigilance, and higher cognitive functions, such as men- tal arithmetic and simulated flight. Observational and experimental studies also show a strong relationship between tem- perature and the productivity of factory, call center, and office workers, as well as students. Within the range of temperatures from 22–29oC, each additional oC is associated with a reduc- tion of about 1.8 percent in labor produc- tivity. The relationship is complex and find that other aspects (e.g., humidity, amount of outdoor air, carbon dioxide levels) have complex inter- actions with ­temperature. A meta-analysis of these studies concludes that increasing temperature from 23 to 30oC reduces productivity by about 9 percent. Industrial output using aggregated data center approximately on a 2 percent output loss per 1°C. Large effects of windstorms on industrial production. Effects of precipitation on industrial output appear slight, although only one study looks at extremely heavy precipitation and in that case finds modest negative effects. Energy The literature has looked extensively at how climatic variables, in particular temper- ature, influence energy consumption. This relationship, which has received renewed attention in light of potential climate change, has long been important for the design of electricity systems, where demand varies with climate and weather. Understanding temperature effects matters for the energy consequences per se and for potential feed- back loops, incorporated into some climatic models, where energy demand influences greenhouse gas ­emissions, which in turn affects future energy demand. A clear U-shape relationship between energy demand and temperature, with an extra day below 10oF or above 90oF raising annual energy demand by 0.3–0.4 percent. These panel-data papers, in using tempera- ture bins, depart from a prior practice of using “heating degree days” (HDD) and “cool- ing degree days” (CDD), which count the number of days below and above a threshold temperature, with each day weighted by its temperature difference from the threshold. This degree-days approach misses the con- vexity found in the ­nonparametric approach, where extreme temperatures provoke much stronger energy demand increases. The con- vexity of the U-shape appears important both in getting the energy demand estimation cor- rect and in light of climate change models, which show an increasing number of very hot days. Partly for this reason, Deschênes and Greenstone (2011) and Auffhammer and Aroonruengsawat (2011) find that the net effect of warming over the twenty-first century is likely to increase energy demand substantially, ceteris paribus, with these studies estimating 11 percent and 3 percent demand increases respectively. Trade and Innovation Trade can, in principle, dampen or exacerbate local effects of productivity losses. Another potentially first-order adaptation mechanism is innovation. The unusual identification opportunity provided by weather shocks has allowed a rigorous analysis of weather–economy link- ages, and implications for breadth, hetero- geneity, and functional forms. While much work remains in developing a detailed under- standing of the underlying mechanisms, especially for macroeconomic and politi- cal economy outcomes, the new literature shows that weather variation has substantive effects in contemporary periods. This begins to suggest policy targets, whether the goal is preventing substantial economic damages or protecting public health and security. From short to long run: Econometrics While tem- perature changes over the next thirty years will plausibly be within this range (recall the IPCC middle estimates were between 1.8–3.1oC by 2100), the ninety-fifth percentile estimate is warming of 7oC by 2100. If the impacts of climatic variables are linear throughout this range, then extrapolation is not an issue per se. However, if there are nonlinearities that are different from those operating within historical experience, one cannot directly extrapolate from equation (3) to climate scenarios far outside this range. This issue suggests a limited capacity for panel models to provide quantitative estimates of damages from extreme warming. hese issues highlight that, even though panel models of the form of equation (3) correctly identify the causal effect of weather shocks on contemporaneous economic out- comes, they may not estimate the structural equation of interest for understanding the likely effects of future global climate change. Moreover, even leaving aside the potential of catastrophic climate scenarios, such as rapid sea-level rise or the release of methane from melting permafrost that could greatly increase global temperature, the panel esti- mates are neither obviously an upper bound nor a lower bound for the effect of climate change. If the adaptation force dominates, then the effects of weather shocks will tend to be larger than the effects of climate change; if the intensification force dominates, then the effects of weather shocks will tend to be smaller than the effects of climate change. Longer-difference estimates are perhaps the closest empirical analogue to the structural equation of interest for climate change. To the extent that adaptation requires forward-looking investments, adap- tation choices will depend not only on the underlying damage functions and adaptation possibilities, but also on agents’ expectations. Responses will depend on whether agents both were aware of the change in average temperature, and whether they perceived it to be a permanent change or just an accumulation of idiosyncratic shocks. The challenge is that economies are chang- ing and the longer the time difference taken in (8), the further back in time the analysis goes (by necessity), and the further removed from present-day economic conditions the analysis becomes. To the extent that differ- ent economies presented very different standards of living, technologies, and institutions through the twentieth century, one may still make headway by examining historical heterogeneous treatment effects along various dimensions of economic development. On the other hand, the future presumably prom- ises new technologies and other features that may pull economies outside the range of historical experiences, calling for caution in drawing sharp conclusions from increasingly historical studies. Long-run studies illustrate that factor reallocation may be an important mechanism. IAMs Our focus is on the damage function, the component of IAMs that specifies how increased temperatures affect economic activity. IAMs used for economic policy analysis typi- cally include four broad components: 1) a model projecting the path for greenhouse gas (GHG) emissions; 2) a model mapping GHG emissions into climatic change; 3) a damage function that calculates the economic costs of climatic change, and; 4) a social welfare function for aggregating damages over time and potentially across space. All IAMs must make a wide variety of modeling choices, with large uncertainties remaining across each component. The possibility of positive feedback loops implies that mod- eled climate change predictions are right- skewed; in other words, there are “fat tail” probabilities for massive climatic change in the next century. IAMs must specify a social welfare function that discounts the future path of consumption. The concavity of the utility function. This property influences not only how one weighs future versus current generations, but also how one weighs rich versus poor economies at a single point in time. Different IAMs model the climate-dam- age function in somewhat different ways. For example, the DICE/RICE models use a Cobb–Douglas production function with capital and labor as inputs, multi- plied by TFP, which grows at a constant, exogenously specified rate. Output is then reduced by the climate-damage function. For example, in the DICE model, the damage function is \\[D(T) = frac{1}{1+Pi_1 T + Pi-2 T^2}\\] DICE calibrates the π parameters to match cross-sectional estimates of climate damages reviewed in Tol (2009). In the FUND model, rather than spec- ify an aggregate damage function directly, climate damages are calculated at the region-by-sector level and aggregated up; ­ that is, FUND posits separate models for agriculture, forestry, energy consumption, and health (deaths from infectious, cardio- vascular, and respiratory disease), while also considering water resources, extreme storm damage, sea level rise, and the value for eco- systems, with potentially separate regional parameters for each of these models. An important challenge with the current damage functions is that, for the most part, they do not incorporate the type of rigor- ous empirical evidence on climate damages reviewed here. In a recent review of IAMs, when discussing the calibration of the D(T) function, Pindyck (2013) writes “the choice of values for these parameters is essentially guesswork. The usual approach is to select values such that [D(T)] for T in the range of 2°C to 4°C is consistent with common wis- dom regarding the damages that are likely to occur for small to moderate increases in temperature. . . . The bottom line here is that the damage functions used in most IAMs are completely made up, with no theoretical or empirical foundation.” The implications of the econometric evidence discussed here can be thought of in two respects: how we model and calibrate the climate-damage function at a point in time, and how the climate-damage function evolves over time. A key modeling choice for the dam- age function is whether climate affects the level of output or the growth path of output. The main IAMs assume that the impact of climate is on the level of output only with the growth of total-factor productivity continuing exogenously. Because growth effects, even small ones, will ultimately dominate even large-level effects, ruling out growth effects substantially limits the possible economic damages these models allow. An alternative way of specifying the dam- age function is to allow climate to affect the long-run growth rate directly. Understanding the functional form through which climate affects economic output is critical. While it is hard to know definitively the correct functional form for the loss func- tion, even small impacts on productivity growth could, over time, swamp effects on the level of output. Building IAMs is a challenging exercise with enormous uncertainty. We are optimistic that the damage function can be substantially informed by the recent wave of new empirical research, which has begun to provide key insights. Integrating across the many studies reviewed, several broad themes emerge. First, there is a wide range of channels through which weather shocks affect eco- nomic outcomes. Shocks, especially temper- ature, affect agricultural output, industrial output, energy demand, labor productivity, health, conflict, political stability, and eco- nomic growth. Labor productivity effects alone may suggest potentially economywide mechanisms. Moreover, the magnitudes of the effects are often substantive. An inter- esting linkage appears across studies of labor productivity, industrial output, and economic growth, where estimates converge around a 1–2 percent loss per 1°C in poor countries. Second, the panel studies provide an emerging set of key insights about functional forms. Effects are often not simple linear functions independent of context. High sensitivity to extreme temperatures, but little or no sensitivity to temperature changes within moderate temperature ranges. International and internal trade effects, including studies of how integrated mar- kets both mute and transmit shocks. Panel methodologies can also study medium-run and longer-run changes directly. Keeping in mind that countries have warmed substan- tially on average in the last several decades, with substantial variance within and across countries, there is ample capacity to study medium-run changes. The recent warm- ing rate is also very similar to that predicted by many climate models through at least the middle of the current century. Noting that climate change is not about a perma- nent climate shock, but rather about a sto- chastic warming process along an upward trend, recent historical experience, which has occurred on such a stochastic warming trajectory, provides a highly relevant set- ting to understand warming effects. Dell (2014) What Do We Learn from the Weather? The New Climate–Economy Literature (pdf) 43.3 Carbon Tax Roberts Carbon taxes are an almost perfectly terrible policy from the perspective of political economy. They make costs visible to everyone, while the benefits are diffuse and indirect. They create many enemies, but have almost no support outside the climate movement itself. 43.3.1 Fee and Dividend More to the point, because there have been so few fee-and-dividend policies implemented in the real world, there’s been very little field testing of the public’s actual response to it. A new paper in the journal Nature Climate Change by political scientists Matto Mildenberger look at public opinion in the places where carbon fee-and-dividend policies have been implemented. It turns out there are only two. Switzerland established a rebate program in 2008. The carbon tax reached 96 Swiss francs (about $105) per tonne in 2018; about two-thirds of the revenue is rebated on a per-capita basis, with everyone (including children) receiving an equal share. Canada established a rebate program in 2019 as part of its national carbon-pricing strategy. So far, the scheme covers four of 10 provinces, with more than half of the national population. The price was initially set at 20 Canadian dollars (about $16 U.S.) a tonne, rising to CA$50 by 2022; recently the government released a new schedule that would target CA$170 by 2030. The refund, or Climate Action Incentive Payment, is based on the number of adults and children in the household, with a 10 percent boost for rural households. It is highly progressive; 80 percent of households get more back than they pay. The Nature Climate Change paper looks at public opinion in both countries. In Canada, it draws on a longitudinal study, which surveyed the same residents — “from five provinces, two subject to the federal carbon tax (Saskatchewan and Ontario), one with provincial emissions trading (Quebec), and two with provincial carbon taxes (British Columbia and Alberta)” — five times from February 2019 through May 2020, during which time the scheme was proposed, debated, passed, and implemented. In Switzerland, the paper draws on a survey of 1,050 Swiss residents in December 2019. Only 12 percent of Swiss respondents know that part of the carbon revenue is refunded; 85 percent did not know they’d gotten a refund at all. Canadians remain confused and in many cases ignorant about carbon refunds. You might think, well, the problem is how these countries administer their refunds. In Canada, it’s a line on your tax return. In Switzerland, it’s a discount on your health insurance premiums. Both are clearly marked, but lots of people don’t exactly scrutinize those documents and keep track of every line item. In short, the available evidence suggests that carbon refunds don’t do much to reshape public opinion on carbon taxes, even among voters with accurate information about the refund they receive. Roberts (2022) Do dividends make carbon taxes more popular? Apparently not 43.4 Finnish Carbon Tax Mideksa Abstract Finland introduced the planet’s first carbon tax in 1990 to experiment with, to most economists, the best policy to reverse carbon emissions. I estimate the causal effect of taxing carbon on Finnish emissions using the Synthetic Control Approach (Abadie, 2021). The results suggest that taxing carbon reduces emissions by big margins. Finnish emissions are 16% lower in 1995, 25% lower in 2000, and 30% lower in 2004 than emissions in the counterfactual consistent with carbon taxes whose value increasing by 20 fold in 1990 - 2005. The estimates suggest that the carbon tax’s abatement elasticity is about 9%. Mideksa Memo Despite the conceptual foundation behind using a carbon tax to reverse carbon emissions being strong, its empirical foundation remains arguably weaker. The supporting evidence for the effectiveness of taxing carbon is missing, first, because few countries have taxed carbon: even fewer empirical studies of the causal effect on emissions. Besides, in the countries that have taxed carbon, the policy-induced data generating process has been too complex to lend itself to causal identification. Syntetic Control Approach What is the causal effect on CO 2 emissions of the Finnish taxes on carbon? One can think about this question and identify the causal effect by conducting a randomized control trial: some regions, chosen randomly, tax carbon while the remaining regions serve as a control group. Yet, countries tax carbon in all regions let alone with randomization. One thus needs a second-best alternative to randomization: the synthetic control approach to estimate the causal effect of the Finnish carbon tax since 1990. The synthetic control method adopts a data-driven approach in choosing the best comparison unit and allows falsification tests in assessing its sensitivity In estimating the effect of the Finnish carbon tax using synthetic control, I focus on emissions from the transportation sector for the following reasons. First, there is a problem of ruined-control in the countries without a carbon tax. The problem arises when Finland imposes the carbon tax, some inputs can be imported from (or exported to) other countries. In other words, the aggregate emissions in nations without a carbon tax could be affected by the Finnish carbon tax when such countries trade with Finland. This is a concrete problem, for example, when it comes to per capita emissions, which is a contaminated measure for a small open economy like Finland. However, transportation services are internationally non-tradable, the ruined-control effect due to international trade is limited. The structure of energy production and use in transport activities is similar across countries. This eases the task of constructing a valid comparison unit to Finland from the set of other countries. I focus on the transport sector due to the availability of data for predictors that allow comparability across countries. The estimated gap between the actual and the counterfactual emissions implies that the carbon tax reduces emissions considerably. Finnish emissions are 16% lower in 1995, 25% lower in 2000, and 31% lower in 2005 relative to the counterfactual. Rising impact over time goes in line with the increasing intensity of the CO 2 tax per ton of CO 2 over time (i.e., increased by 20 fold in the treatment period). The estimated emissions reductions came from stabilizing the Finnish emissions at the 1990 level relative to sharply rising emissions in the countries lacking a carbon tax. The estimated emissions reductions are consistent with the decline in Finnish gasoline, and diesel, consumption after 1990. Moreover, Finnish passenger transport activities and the number of vehicles have decreased to a new trend after 1990. I estimate the carbon tax elasticity of emissions reductions directly by using the real carbon tax data and its estimated impact. The geometric mean of the annual carbon tax elasticity of emissions reduction values is −9%. The countries in the donor pool adopt arguably similar transportation technologies as Finland. Limiting the donor pool to countries whose emissions are driven by a similar structural process as that of Finland serves as a reasonable potential comparison unit. The counterfactual trajectory of emissions emerges as a convex combination of emissions of six countries. These countries, with corresponding weight in a bracket, are the United Kingdom (43.20%), Turkey (18.40%), New Zealand (15.90%), Luxembourg (10.20%), Switzerland (9.40%), and the United States of America (2.90%). A useful parameter to summarize the effect of carbon tax is elasticity. Since the number of observations is very small, the OLS based estimation of elasticity is sensitive and its application is conceptually questionable. The initial values of annual elasticity are higher consistent with abundant possibilities of low-hanging abatement options. While the values of annual elasticity oscillate, the arithmetic and geometric mean values are 0.099 and 0.086 respectively. eople expect taxes to be permanent and long term while variations in fuel prices could be short-term. In response, people adjust both on the intensive and the extensive margins when facing carbon taxes whereas mostly on intensive margins for temporary fluctuations in gasoline prices. Third, carbon taxes carry a signal from a society that urge for reducing carbon emissions, a signal absent in temporary variations in gasoline prices. The identifying assumption underlying the synthetic control approach is that emissions in synthetic Finland serve as a valid counterfactual. For example, if countries price carbon indirectly through energy and fuel taxes, the identifying assumption calls for a similar evolution of such variables both in Finland and synthetic Finland. To take into account implicit carbon prices, I exclude countries that have raised their fuels taxes from the donor pool. I perform a back-dating test by introducing hypothetical carbon taxes in 1986, 1987, and 1988. Besides, I drop one predictor at a time to re-estimate emissions in synthetic Finland. While one recognizes that the Finnish recession in 1990 – 1993 could have played some role in reducing emissions, it is unclear if the recession is the central explanation for the observed emissions reductions in 1993 – 2005. Similarly, the high elasticity estimates may reflect the low-hanging fruits. The case for a carbon tax, when compared with auctioned quotas, is far from unanimous. The reservation goes back, at least, to Buchanan (1969, p. 175) – who aimed at contributing the project of “dismantling of the Pigovian tradition in applied economics, defined here as the emphasis on internalizing externalities through the imposition of corrective taxes and subsidies.” The current concerns include the difficulty of meeting a given emissions target (Harris and Pizer, 2020) and the ramifications for risk externality (Mideksa, 2020), the political difficulty of imposing a tax and changing it over time (Slemord and Bakija, 2017), the challenge of enforcing a tax and avoiding evasion in countries with a weak fiscal capacity (Acemoglu, 2005; Besley and Persson, 2009), the ease for allowing exceptions and loopholes and for undermining a tax through subsidies to complementary inputs, and other factors covered in Sterner and Coria (2012) and Stavins (2020). These factors, in addition to carbon leakage, can explain why there was no break in the trend of Finnish industrial CO 2 emissions around 1990. Focusing on political economy considerations, Brooks and Keohane (2020, p. 20) explain the point as follows. “In general, the environmental community is focused on ensuring emissions reductions, while the regulated industry is focused on limiting costs. The former’s strong preference for such environmental certainty helps to explain why existing market-based climate policies are overwhelmingly quantity-based; the latter’s insistence on some degree of cost certainty helps to explain why these policies generally also include price containment mechanisms.” 10 Emissions target can be unmet due to the uncertainty from business cycles or MIT shocks like pandemics. Other things being the same, carbon taxes deliver higher abatements during recessions and lower abatements during booms, relative to auctioned quotas. Since economies tend to have more periods of a boom than periods of a recession (e.g., see Figure 2 in Rebelo, 2005), some stakeholders (e.g., environmentalists that Brooks and Keohane (2020) refer to) worry that taxes can fail to meet emissions targets. Nevertheless, the Finnish experience in 1990 – 2005 suggests something else: a well-crafted carbon tax induces meaningful emissions reductions both in booms and recessions in the early phases of decarbonization. Thus, the Finnish experience does not support the idea that carbon taxes are excuses to continue emitting more. Mideksa (2021) Finnish Carbon Tax (pdf) 43.5 Instrument Choice Delays Roberts The capture of the climate policy debate by carbon-price-obsessed economists in the late 20th century helped send national and international climate policy down a multi-decade cul-de-sac in which very little was accomplished and much precious time was wasted. Roberts (2021) A rant about economist pundits, and other things, but mostly economist pundits Boyd Using the case of emissions trading, this Article investigates how the instrument choice debate has impoverished our conception of government and limited our capacity to respond to the climate crisis. The central claim is that the overly abstract theory of instrument choice that has underwritten widespread enthusiasm for emissions trading and other forms of carbon pricing over the last three decades has led to a sharply diminished view of public engagement and government problem solving. Boyd (2021) The Poverty of Theory: Public Problems, Instrument Choice, and the Climate Emergency 43.6 SSPs Buhaug Abstract The recently developed Shared Socioeconomic Pathways (SSPs) have enabled researchers to explore coupled human–nature dynamics in new and more complex ways. Despite their wide applicability and unquestionable advantage over earlier scenarios, the utility of the SSPs for conducting societal impact assessments is impaired by shortcomings in the underlying economic growth projections. In particular, the assumed economic convergence and absence of major growth disruptions break with historical growth trajectories in the developing world. The consequence is that the SSP portfolio becomes too narrow, with an overly optimistic lower band of growth projections. This is not a trivial concern, since resulting impact assessments are likely to underestimate the full human and material costs of climate change, especially for the poorest and most vulnerable societies. In response, we propose that future quantifications of the SSPs should incorporate the likelihood of growth disruptions, informed by scenarios of the relevant political contexts that historically have been important in curbing growth. How will climate change shape societies in coming decades, and what steps could be taken to avoid the gravest consequences? The recently developed Shared Socioeconomic Pathways (SSP) framework, which plays an integral role in the ongoing Intergovernmental Panel on Climate Change (IPCC) Sixth Assessment cycle, constitutes the most comprehensive attempt to date to model societal development consistent with different climate change scenarios (O’Neill et al. 2014; Riahi et al. 2017). The SSPs span a range of alternative futures, determined by assumptions about challenges to climate change mitigation and adaptation. Four pathways (SSP1, SSP3–SSP5) capture the four possible combinations of low versus high barriers to adaptation and mitigation, whereas the fifth (SSP2) represents a middle-of-the-road pathway. Central drivers of these challenges include changes in demographic, economic, technological, social, political, and environmental factors. The SSPs serve two key functions: to provide “a basis for integrated scenarios of emissions and land use” and to facilitate “climate impact, adaptation and vulnerability analyses” (O’Neill et al. 2017, 169). There is some tension between these functions, since the former is determined mostly by the development trajectories of large economies and major greenhouse gas (GHG) emitters, whereas the latter is much more sensitive to future development in low-income countries and the world’s poor. In other words, there is little overlap between the countries that contribute the most to anthropogenic climate change and those that are the most vulnerable to its impacts (Althor et al. 2016). Presently, the SSP framework appears better suited to fulfilling the first task than the second. In this research note, we show that existing quantifications of the SSPs, despite their wide applicability and unquestionable advantage over earlier scenario exercises, have clear limitations for researchers seeking to conduct societal adaptation and impact assessments because of shortcomings in the economic growth models underlying the SSPs. In particular, the assumption of growth convergence, whereby poorer countries gradually catch up with wealthy economies as long as educational attainment improves, and the related assumption of a future without major growth disruptions break with historical development trajectories. The result is an overly narrow and optimistic range of projected development outcomes. In response, we encourage revising or expanding the SSPs to incorporate growth projections that are sensitive to the underlying political and security contexts. Assumptions about such conditions are already embedded in the narratives that accompany the quantified SSPs (O’Neill et al. 2014, 2017), but presently, they exist in isolation from the growth projections. By bringing the political context explicitly into the quantitative scenarios, the SSP modeling community would help the IPCC get one step closer to achieving its objective: “to provide governments at all levels with scientific information that they can use to develop climate policies.” Buhaug Memo The quantiﬁcation of the SSPs consists of end-of-century population and urbanization projections, including changes in fertility and education ( Jiang and O’Neill 2017; KC and Lutz 2017), as well as three alternative projections of growth in gross domestic product (GDP), developed by modeling teams at the Organisation for Economic Co-operation and Development (OECD) (Dellink et al. 2017), the International Institute for Applied Systems Analysis (IIASA) (Cuaresma 2017), and the Potsdam Institute for Climate Impact Re- search (PIK) (Leimbach et al. 2017), respectively. Governance and security de- velopments are not part of the quantitative scenarios. Instead, aggregate descriptions of the regional and global political contexts are embedded in the qualitative storylines that accompany the SSPs. All three SSP teams modeling future economic growth adopted the aug- mented Solow growth model. A central feature of the Solow model is the convergence mechanism: that development is associated with diminishing marginal returns on invest- ments, such that it is cheaper and more viable for less advanced societies to absorb inventions from the technology frontier than for advanced societies to develop new technology. The augmented model assumes that the rate of convergence is conditional on human capital. If the convergence mechanism is a major driving factor of growth, we should observe a steady narrowing of the income gap between developed and developing countries as time pro- gresses, assuming that the populations in developing countries become increas- ingly educated. In the real world, economic convergence has been much less pronounced, despite signiﬁcant educational improvements in poor countries. Despite the historical prevalence of growth disruptions and a good scientiﬁc un- derstanding of important structural drivers, the economic growth models in the SSP portfolio abstain from incorporating negative shocks. For most countries, most of the time, this is not a problem, and the augmented Solow model has been shown to perform well in predicting welfare growth for the countries that account for the vast majority of global GDP (Mankiw et al. 1992). For the same reason, the SSPs are well suited to evaluating implications of alternative societal development trajectories for global GHG emissions and climate change mitiga- tion challenges. However, such models tend to return overly optimistic projec- tions in the long term, especially for countries at greater risk of experiencing growth disruptions. Known barriers to growth, such as resource dependence, war, and lack of good governance structures—all of which are part of the SSP narratives are unaccounted for in the GDP projections. The consequence is that the range of futures provided through the quanti- ﬁed SSP framework becomes too narrow and covers too small of an area of the conceivable probability space due to an overly optimistic lower band of growth projections. Of particular concern is the fact that the coun- tries for which the GDP projections will ﬁt the least well (i.e., developing countries with a history of recurring growth disruptions) are the very same countries where vulnerability to climate change is considered the highest and for whom sound adaptation and impact assessments may be most in demand. [Attempts to use the SSPs] are thus at risk of overestimating future production and security improvements and underestimating the relative cost of choosing a development pathway akin to regional rivalry (SSP3) or inequality (SSP4) over sustainable development (SSP1). Although modelers need high- growth scenarios, and there are reasons to remain optimistic about long-term development in many of today’s poor countries, sound and comprehensive impact assessments also require projections that are at least as pessimistic as the recent past. Buhaug (2019) On Growth Projections in the Shared Socioeconomic Pathways (pdf) 43.7 Guard Rail Economics Stern We risk loss of life in the hundreds of millions or billions; because we do not know what the “carrying capacity” of a world of 4 or 5 o C might be. It could be much lower than the 9-10 billion or so expected towards the end of the century. It is hard to understand or put numbers on the potential devastation and agony around the process of loss of life that could be involved. It is difficult, in particular, to argue that an expected utility approach captures the issues at stake in a plausible way. In my view, a direct risk-assessment looking across possible consequences and a guard-rail approach is more thoughtful, reasoned, broad-ranging and robust. And it is clearly seen as a reasonable and rational approach by the body-politic. A new form of Growth The necessary rapid change across the whole system, just described, can be a story of growth, indeed the only sustainable story of growth. In the shorter term, the necessary investments can boost demand in a world where planned savings exceed planned investments (with sluggish demand and low real interest rates). In the short and medium term it is full of innovation, investment, discovery, and new ways of doing things. It can be more efficient; and much cleaner. It can create cities where we can move and breathe, and ecosystems which are robust and fruitful. It is potentially a very attractive, different way of doing things, relative to past dirty models, with so many gains across the different dimensions of well-being. But that does not mean that it is easy. It does mean that it is sensible, it does mean that it is attractive, and it is within our grasp. We have to change radically and, particularly, invest and innovate strongly to get there. That is the challenge. But there can be a real payoff in terms of a much better form of growth. Can it be done? The answer is ‘yes’ and in particular there are four forces at this current moment which are particularly favourable to moving quickly and on scale: low interest rates, rapid technological change (see section 2.4), international understandings coming together (including the UNFCCC, COP21, the Paris agreement of 2015 and more than 100 countries covering 61% of emissions committing to net- zero by mid-century (Black, et al., 2021)), and pressure from the young people of the world to change (for example, Fridays for the Future and strong activity in the universities of the world). Investment Strong, internationally coordinated investment should be at centre stage, right through from recovery from the COVID pandemic to transformational growth and the drive to a net-zero economy. What kind of orders of magnitude of investment do we need to make? To bring through the new ways of doing things and the new technologies required to make that happen, we have to increase investment by around 2-3 percentage points of GDP across the world, relative to the previous decade. Policy These increases in investment, will require strong policy and a positive investment climate, including the functioning of relevant governmental institutions. Further, the many relevant market failures (see section 7b) and the urgency of change indicate the necessity of a whole range of policy instruments. Carbon pricing will be important, but alone it will not be enough. Complementary policies, including city design, regulation and standards, and investments in R&amp;D, will also be needed. Finance Getting the right kind of finance, in the right place, at the right time is not easy. Mobilising private sector finance, at scale, will be critical. But there will also be a need for development finance and concessional finance to support the activities that do not quickly generate strong revenue streams or have high risks. The international financial institutions, especially the multilateral development banks, and including the IMF, have a crucial role to play. How economics must change An assessment of what the current situation demands of us, particularly for this decade, was set out. That requires changing our ways of producing and consuming, rapidly and fundamentally, and creating the investment, innovation, sets of policies, and the finance that could foster and support the change. How can we bring our economics to bear in a way that informs those very real and urgent problems? How can we use economic analysis to tell us as much as it possibly can about why to do this, how to do this, and the methods and policy instruments we should use? I will focus, in terms of broad analytical approaches, on where we are in the economics discipline on climate change and argue that it is time for change in the economics of climate change and, in some respects, economics generally. Our subject does have much to offer in applying our existing tools and in developing new perspectives and analyses, but we must be innovative and, as a profession, engage much more strongly on this, the biggest issue of our times. A starting point is the important set of insights of economists Alfred Marshall and Arthur Pigou. At the end of the 19 th century, Marshall (Marshall, 1890) drew attention to the potential difference between marginal private cost and marginal social cost. Thirty years later, Pigou (Pigou, 1920) argued for a tax, equal to the difference between the marginal private cost and the marginal social cost, to correct for an externality, where that is the source of the difference 11 . Around 60 or 70 years ago, Ronald Coase began considering these concepts in a different way, emphasising institutional arrangements (Coase, 1960). He spoke of allocating property rights and establishing markets so that there could be trade in externalities. James Meade - his work ‘Trade and Welfare’ (Meade, 1955) was a landmark - also wrote very insightfully about the theory of externalities, including integrating externalities into the theory of reform, bringing in distributional issues and looking at general equilibrium in multi-good models. Coming forward further, and looking at applications 30 or so years ago, David Pearce, for example, was writing ‘Blueprint for a Green Economy’, emphasising how the Pigouvian idea could be implemented (Pearce et al., 1989). The modelling of climate change began with Bill Nordhaus’ important and admirable paper ‘To slow or not to slow?’, published in the Economic Journal in 1991 (Nordhaus, 1991) and Bill Cline published his book ‘The Economics of Global Warming’ in 1992 (Cline, 1992). Nordhaus’s question, recognising that there could be potential dangers from climate change and that emissions arose from activities around producing and consuming, was ‘should we grow a little less fast than we might have envisaged before we thought about climate change?’. He proceeded in a sensible way, taking an emerging problem and applying the standard tools of economics: first the Pigouvian story of marginal social costs, marginal private costs, and taxing for the externality; second on growth, he used the framework of a standard exogenous growth model and considered the impact of climate change largely in terms of small perturbations around the underlying growth path(s). That was a sensible early contribution for the economics of climate change. Over the following 10-15 years, it became more and more clear that climate change is not a marginal problem. We are dealing with a challenge involving huge potential disruptions, which requires very radical changes in our production systems and ways of consuming. That simply cannot be picked up by assuming a fairly standard underlying model of exogenous growth and, within that model, portraying climate change in terms of marginal damages of just a few percent of GDP. Nordhaus’ DICE model launched a major literature on integrated assessment models (IAMs), and their scope has been expanded. But the basic underlying features of optimisation of explicit, calibrated social welfare functions, underlying exogenous growth and aggregation (usually to one good) impose severe limitation on their ability to illuminate two basic questions. The first is how to approach analytically the challenge of managing immense risk, which could involve loss of life on a massive scale. The second is how to chart and guide a response to this challenge which will involve fundamental structural change across a whole complex economy. These two issues are at the core of economic policy on climate. The basic structure of IAMs, I shall argue, even with the many advances and mutations that have been offered, is not of a form which can tackle these two questions in any satisfactory way. There is a problem in the profession, which goes beyond the way IAMs are structured and specified, associated with an inability or unwillingness to move much beyond the static Pigouvian or 20 th century approach to externalities in analysing the challenges of climate change. Many discussions of policy suggest that “economic theory says” that policy should be overwhelmingly about a carbon price. That “theory says” that the carbon price is the most effective route is simply wrong and involves a number of mistakes. The first mistake is that there is a whole collection of market failures and market absences of great relevance beyond the greenhouse gas externality (see section 7). The second is that under the temperature target or guardrail approach (see section1), the choice of carbon prices is focused on its role, in combination with other policies, in incentivising paths which achieve the overall target (such as net-zero emissions by mid-century to fit with the temperature target) with as much economic advantage as possible. Such prices are not simply the marginal social cost as in Pigou (see discussion of Stern- Stiglitz Commission below, this section). Third, where the risks of moving too slowly are potentially very large and there are increasing returns to scale and fixed costs in key industries, then regulations can help reduce uncertainty and bring down costs (e.g. Weitzman, 1974). Fourth, many consumers, producers, cities, and countries, recognise the obligation to act, and are not blinkered, narrow optimisers with a view of utility focused only on their own consumption. Fifth, much of the challenge of action is how to promote collaboration and act together. This poses a whole set of important questions around institutions and actions for mutual support. This is an immense challenge concerning risk, values, dynamics and collaboration, and the narrow Pigouvian model, useful though it is,is very far from the whole story. Failure of IAMs There is an underlying one-good growth model where emissions depend on output, accumulated emissions cause temperature increase and climate change, and emissions can be reduced by incurring costs. However, much of this literature, which has dominated so much work on the economics of climate change, has been misleading and biased against strong action, because climate damage specifications are implausibly low and costs of action implausibly high, and subject to diminishing returns. Most standard IAMs also embody diminishing returns to scale and increasing marginal costs of action to reduce emissions, plus modest rates of technical progress (relative to those experienced in the last decade or so). These features are very problematic because we have already seen how important increasing returns to scale and very rapid change in technology are in this context. By embodying diminishing returns and modest technical progress, the IAMs systematically overstate the costs of climate action. Further, they distort the theory of policy which is much more complex when we have increasing returns to scale; particularly in the context of risk. Standard optimising policy models which focus on “marginal cost equals marginal benefit” are far more tractable with diminishing returns and increasing marginal costs to action, but by choosing model assumptions primarily for tractability and convenience, we risk severely disturbing the policy discussion at issue. There are deeper problems with the general approach of maximising a social welfare function (for example, based on expected utility) in the presence of extreme risk, which cannot be corrected by adjusting functions and parameters. Standard utility or welfare functions at the heart of the IAMs cannot capture adequately the nature and scale of the risks from climate change. Impacts which can involve deaths of billions are not easily captured in the standard social welfare functions, which we used in most IAMs (and more broadly), involving aggregation of individual utility functions. Indeed, as Weitzman argued (Weitzman, 2009, 2012) standard approaches quickly run into problems of utility functions going to minus infinity. There can be arbitrary “fixes”, for example by putting bounds on utility, but it is an indication that the model has lost touch with the problem. Just as with the social welfare function aspect of IAMs, there is a deeper question on the production side of the modelling. The policy challenge, as we have seen, involves generating rapid and major change in key complex systems, including energy, transport, cities and land, over a very short period. Simple “cost” functions for emissions reductions, even if made more realistic, do not get to grips with the real policy challenges of how to make these changes. We are in a world with many market imperfections, with major risks, requiring fundamental systemic change, and where optimisation is difficult to define, let alone achieve. There is no serious ethical argument in favour of pure-time discounting. There is little point in looking for ethical values relevant to social discounting in capital markets, because capital markets: (i) do not reflect ethical social decisions; (ii) they embody expectations and views about risk that are hard to identify; and (iii) they involve many imperfections. Nevertheless, one often seems to hear the mistaken argument that social preferences can be derived from these markets. Economic analyses of climate change must first capture extreme risk, including possible large-scale and unforeseeable consequences. Second, they should recognise that many key markets have critically important failures (beyond that of the GHG externality), that crucial markets may be absent, and that there are limits on the ability of government to “correct” these market failures or absences. Third, they should embody rapid technical and systemic change, often in very large and complex systems such as cities, energy, transport, and land use, and allow for increasing returns to scale. Fourth, they should examine rapid changes in (endogenously determined) beliefs and preferences; and fifth, take into account distributive impacts and risks, both at a moment in time and over time, and including those associated with structural change. All of this will unavoidably involve explicit analysis and discussion of value judgements. These components, or sets of questions, are difficult to incorporate in standard integrated assessment modelling, but are at the core of the issues around understanding policy towards climate change. We must deepen our economic analysis to incorporate them. We should also recognise that questions embodied in, or similar to, these components arise in many other parts of economics, where major risks and fundamental change are at the core of the challenge under examination. Given that governments are made up of complex compromises and coalitions, are limited in information and capabilities, and are not necessarily long lasting, we must recognise in our analysis that there are limits on their ability or willingness to “correct” for market failures and absent markets. Governments cannot fully commit to future actions in a credible way. The GHG failure is top of our list of market failures. And carbon pricing has a critical role to play in tackling that market failure. However, we can see, from thinking about different aspects of market and government failures, that the policy question is much richer than carbon pricing alone. Regulatory policies, alongside carbon pricing, could be more efficient and effective than carbon pricing alone. The need for new approaches to economic analysis of climate change raises an enormously rich research agenda. At the same time, action on scale is urgent. The necessary transformation of the economy relies critically on changing key systems: energy, cities, transport, land use. These large and complex systems cannot be changed by fiddling with just one parameter, a whole set of policies will be required to foster change. For example, you would not sensibly attempt to redesign a city to reduce congestion and pollution just via a carbon price. Most elements of economics come into the challenge of climate change. It is time for change in economics. Stern (2021) A time for action on climate change and a time for change in economics (pdf) 43.8 Tipping Points Dietz Dietz (2021) Economic impacts of tipping points in the climate system (pdf) [SI (pdf)(pdf/Dietz_2021_Economic_Tipping_SI.pdf) Keen on Dietz This was an invited talk to the Oxford Department of International Development “Climate Change and the Challenges of Development Lecture Series”, on my criticisms of the application of neoclassical economics to climate change. I focused on the new paper by Dietz et al. that allegedly calculates the economic costs of tipping points: Dietz, S., J. Rising, T. Stoerk and G. Wagner (2021). “Economic impacts of tipping points in the climate system.” Proceedings of the National Academy of Sciences 118(34): e2103081118. Upon closer examination, this papers fails to consider tipping points in any credible way, and this is obvious in its incredible claim (in the original sense of the “not credible”), that: “Tipping points reduce global consumption per capita by around 1% upon 3°C warming and by around 1.4% upon 6°C warming&quot; This is ridiculous: the tipping points they consider are: Arctic summer sea ice, The Greenland Ice Sheet, The West Antarctic Ice Sheet, The Atlantic Meridional Overturning Circulation (“Gulf Stream”), The Amazon Rainforest, The Indian Monsoon, Permafrost, and Ocean methane hydrates. If all 8 of these tripped–especially with a temperature 3-6°C above pre-industrial levels–we would be experiencing a climate utterly unlike anything Earth has seen for tens of millions of years. The thought that this would just reduce global consumption by just 1.4%–compared to what it would be if none of these tipping points were triggered–doesn’t pass what Nobel Laureate Robert Solow once called “the smell test”: “every proposition has to pass a smell test: Does it really make sense?”. I show why this paper stinks in Solow’s sense (slides here). Keen (2021) From Economic Fantasy to Ecological Reality on Climate Change 43.9 Keynesian Decarbonization Mason In the Keynesian vision, the economy is imagined as aa system of monetary production rather than real exchange, with the binding constraints being not scarce resources, but demand and, more broadly, coordination. Economic activity is coordination- and demand-constrained, not real resource-constrained. Production is an active, transformative process, not just a combining of existing resources or factors. Money is a distinct object, not just a representative of some material quantity; the interest rate is the price of liquidity, not of saving. These premises have a number of implications for climate policy. Decarbonization will be experienced as an economic boom. There is no international coordination problem There is no tradeoff between decarbonization and current living standards. Price based measures cannot be the main tools for decarbonization. Central bank support for decarbonization must take the form active credit policy. Sustained low interest rates will ease the climate transition. There is no link between the climate crisis and financial crisis. There is no problem of getting private investors to finance decarbonization. We face a political conflict involving climate and growth, this will come not because decarbonization requires accepting a lower level of growth, but because it will entail faster economic growth than existing institutions can handle. Today’s neoliberal macroeconomic model depends on limiting economic growth as a way of managing distributional conflicts. Rapid growth under decarbonization will be accompanied by disproportionate rise in wages and the power of workers. Rapid decarbonization will require considerably more centralized coordination than is usual in today’s advanced economies. If there is a fundamental conflict between capitalism and sustainability, I suggest, it is not because the drive for endless accumulation in money terms implies or requires an endless increase in material throughputs. Rather, it is because capitalism treats the collective processes of social production as the private property of individuals. (Even the language of “externalities” implicitly assumes that the normal case is one where production process involves no one but those linked by contractual money payments.) Treatment of our collective activity to transform the world as if it belonged exclusively to whoever holds the relevant property rights, is a fundamental obstacle to redirecting that activity in a rational way. Resistance on these grounds to a coordinated response to the climate crisis will be partly political and ideologically, but also concrete and organizational. Mason (2021) Climate Policy from a Keynesian Perspective 43.10 Discount Rate Bichler Nitzan With this discount rate, today’s present value (PV0) of $100 worth of climate cost incurred one hundred years from now (n=100) is approximately $10. But there is nothing to prevent Nordhaus from using a different rate. A slightly higher rate of 3% (dashed series), for example, will cause today’s present value to drop by one half, to a mere $5, give or take. And Nordhaus doesn’t have to stop there. He can go the Full Monty, push the discount rate up to 4.7% (solid series), and reduce the present value to a paltry $1. Blessed are the wonders of compound interest. The nice thing about these discount-rate ‘adjustments’ is that, unlike the commotion stirred by debates over the actual cost of climate change, here there are no messy quarrels with scientists, no raised eyebrows from journalists and no outcries from the cheated public. Only contented politicians and delighted capitalists. And that is exactly the route chosen by William D. Nordhaus. In his 2007 ‘Review of the Stern Review on the Economics of Climate Change (pdf)’, he mocked Lord Nicolas Stern’s assumption of a low discount rate of 1.4%, suggesting we should instead discount the future by his favourite rate of 6%. And that mockery succeeded wonderfully. By leveraging the capitalization ritual in the name of profit and glory, Nordhaus managed to not only help investors minimize the apparent cost of climate change, but also win the Economics Nobel Prize as the white knight of nothing less than . . . ‘sustainable global economic growth’! Who says you can’t eat your cake and have it too? If there is a civilizational lesson from this fiasco – assuming we still have time for such lessons – it is that we need to bar the capitalized fantasy of never-ending growth from any discussion regarding the ecological future of humanity. Bichler Nitzan (2018) The Nordhaus Racket 43.11 Geoff Man on Nordhaus Mann The ​ American economist William Nordhaus opened his Nobel Prize speech in 2018 with a slide of the painting El Coloso, traditionally attributed to Goya and completed sometime between 1808 and 1812. Like Goya’s better-known images of the Madrid uprising of 2 May 1808 and the bloody retribution that came after, the painting depicts the calamitous violence of the Peninsular War, which followed Napoleon’s invasion of Spain. But while Goya’s intentions are clear in El Dos and El Tres de Mayo, it is much less obvious what is going on in El Coloso. A giant man, shrouded in mist, looms over the hills. In the dark valley below, people and animals are caught in desperate flight. Only a single donkey remains still, unflustered. The giant is half-turned away from us and from the refugees, his fist raised, ready to fight. But his eyes appear to be closed. Who or what presents the threat isn’t visible to us. Is the colossus protecting the people, or is it him they fear? Does he symbolise the French armies wreaking havoc across the Spanish hills or Spain standing up to the invaders? If the giant is Spain, which Spain is he: the Bourbons demanding restoration or the liberal republicanism that flared briefly at the time Goya was painting? Nordhaus, whose speech was about the contribution he has made to the economics of climate change, didn’t engage with any of this. For him, El Coloso was simply a metaphor for global warming, which ‘menaces our planet and looms over our future’. It stands outside and above humanity, ‘the colossus of all environmental externalities’, before which any single actor – person, nation, region – is powerless. Nordhaus doesn’t claim to be an art historian, so perhaps he can be forgiven for not recognising that in the world of the colossus, there is no such thing as an ‘externality. Mann (2022) Check Your Spillover "],["resource-economics.html", "44 Resource Economics 44.1 Sustainability 44.2 Resource Extraction 44.3 Peak Theory 44.4 Energy Transition", " 44 Resource Economics 44.1 Sustainability 44.1.1 VML - Voluntary Market Led Sustainability Austin Though our roughly 6-decade response to global environmental challenges – since Rachel Carson and others alerted us in the 1960s – is a long time for an individual human being, from the perspective of the complex system of humankind, all our efforts to date constitute merely the first adaptive responses we have been able to implement in short order. The form of this emergency response has necessarily been constrained by the patterns of collective behaviour we had arrived at before recognizing our new context, for reasons that have nothing to do with the new context. It is a crisis precisely because it has not found us prepared – behaviourally, organizationally, even cognitively. Predominantly a Voluntary Market-led (VML) response What has been the nature of this first response? Of course, it has been multi-faceted, but as befits our market-centric modern society, it has come to be dominated by voluntary market-led strategies under various banners – ethical investing, socially responsible investing (SRI), corporate social responsibility (CSR), environmental, social and governance (ESG) initiatives, impact investing, divestment campaigns, reporting and disclosure frameworks, corporate engagement efforts, stakeholder advocacy and more. VML is most easily defined by what it opposes. ‘Voluntary’ denies, or at least strongly opposes, the need for enforceable regulations and policies to achieve sustainability goals. ‘Market-led’, implicitly upholds the idea that the key market dynamics of profit maximization and economic growth are not only not impediments to sustainability but critical drivers of the solution. VML emerged as our predominant adaptive strategy from the mid- 1990s, not out of any confidence that it would be a sufficient response, but because it was the only scalable response established Western socio-economic norms could tolerate at that time. In short, a major pattern of the last 50 years is that a long-gestating neoliberalism captured a fledgling environmentalism and VML’s ‘win-win’ proposition was about all the environmentalism we could muster. The question today is whether this VML adaptive response can generate ‘enough sustainability before it is too late’. While the VML meta-strategy has certainly delivered gains that would not otherwise have occurred – in accelerated green innovations, widespread awareness, and incremental behaviour change – after 25 years, it is becoming apparent VML cannot generate ‘enough sustainability in time’, which is increasingly the only interpretation of sustainability that matters. Basically, our first response strategy does not seem to be working. Figure: The ‘Fix that Fails’ Archetype. A top loop ‘balances’ an initial problem. (‘The problem leads to more of a fix that leads to less of the problem’). However, it is offset by a second, delayed, loop that only reinforces the initial problem (‘The fix leads to consequences that lead to more of the problem’). If VML is the ‘first response’ of a complex system to an abrupt new awareness of context, certain concepts developed to understand complex systems can help us identify the type and extent of response VML has been and what its limitations now point to. What I will call ‘systems thinking’ – sometimes also ‘complexity science’ or the study of ‘complex adaptive systems’ – is now inexorably and beneficially on the rise as a counterpoint to the reductionism that has underpinned Western thinking for the last 350 years. Systems thinking invites two high-level conclusions about the VML strategy – helpfully both alliterative. First, the tool of Causal Loop Diagrams suggests that VML is a ‘fix that fails’ – an ubiquitous pattern in which genuine ‘fix’ actions are offset or even completely overturned by unintended ‘fail’ consequences. More encouragingly, reflecting on the layered adaptive architecture of complex systems reveals that VML has been a possibly inevitable, not unhelpful ‘defence at first depth’. Complex systems adapt by working through ‘fixes that fail’ until they land upon a deeper ‘fix that works’. It is within our capabilities to adapt in this way, but it implies we must graduate quickly from the initial shallow response of VML to deeper responses of policy and culture change that now offer the only realistic means of averting climate and ecological crisis in time. Because the nature of these deeper changes conflicts with certain premises of the VML strategy – particularly VML’s compromised ability to advocate for economic growth-reducing demand-side measures – initiating such changes requires the conscious giving up of certain beliefs and claims made for VML. As such, among the individuals who can most powerfully and credibly trigger the transition to deeper changes are precisely those individuals who have historically promoted VML strategies, whose public ‘change of mind’ might now constitute a disproportionately powerful signal of the limitations of our first response. Missing the physics for the finance. Plainer still, one can look through the economics to the underlying physical reality. A sustainability discourse conducted in business and economics terms continues to miss the physics for the finance. Our climate and biodiversity challenges are fundamentally driven by human transformation of matter and energy at a scale and pace that exceeds Nature’s capacity to absorb. In response, VML aims to transform the world more sustainably, but the building of a clean economy has simply become the new banner by which we accelerate our transformation of the physical world. We frame the build-out of a clean economy as ‘greener’, but the Earth just registers ‘yet more’ transformation of matter and energy overwhelming natural processes. VML denies that a large part of our sustainability response requires establishing a slower and gentler interaction with Nature to fall back into balance with its pace. Less, not more. Layered architecture of complex systems A universal feature of complex systems is that they emerge or self-organize as hierarchical or layered systems comprised of fast-responding surface adaptive capabilities underpinned by slower-moving capabilities. This results in complex systems responding to adaptive challenges in a layered or cascade fashion. Layered behavioural architecture seems to be Evolution’s elegant solution for handling the innate challenge facing all living things – from simple organisms to complex societies – namely the need for some rigidity to uphold and perpetuate existing beneficial behaviours and yet also some flexibility to respond to new circumstances. Both are beneficial but must be in tension. A complex system in adaptive crisis must find a ‘fix that works’ before it is too late. At the even lower level of ‘culture’, the predominant Western culture remains firmly anchored around consumerism and has most recently taken to celebrating people playing spaceship. With our heavy dependency on VML strategies to address the sustainability crisis, we are effectively defending at first depth only. Governance and culture are certainly more entrenched, slower moving forces, but it is when these gears begin to turn that society really starts to shift. As Brand puts it: ‘Fast gets all our attention, but slow has all the power.’ As the profundity of our sustainability crisis sinks in – ‘sinks in’– it is becoming clear that we must solve our sustainability challenge at a deeper level than initially thought. In a sense the challenge of our ‘race against time’ adaptive crisis is to now shift deeper and more powerful mechanisms of social change faster than perhaps ever before. To accelerate deeper change requires an emergency review of the – entirely pre-Anthropocenic – thinking implicit in our governance structures and cultural beliefs, how they got that way and how they might be different. It is layers again, not of our institutions, but of our prior cognition and reasoning that first shaped those institutions and is now reinforced by them. Western societies have iterated towards a market primacy of self-organization over the last two centuries, in the belief that it was the best means by which to advance human welfare. However, market primacy of human self-coordination is itself a fix that fails because the positive benefits of market-driven economic growth and innovation are undermined by lagging, unintended consequences not registered by the market system – ‘externalities’ – of a scale far greater than most economists and politicians have historically been willing to recognize, and than government and philanthropic efforts currently absorb. A study by Robert Costanza and colleagues estimated the monetary value of the ‘services’ provided free by the Earth’s ecosystem at $125 trillion in 2011, nearly twice the value of global GDP (gross domestic product). 54 Just from this one assessment of some ecosystem benefits, much more ‘value’ is unknown to the market than known. The same study estimated that annual ecosystem services had been depleted by $20 trillion since 1997, during which time conventionally measured real GDP increased by $29 trillion, for a net gain of $9 trillion While conventional global GDP grew by 3.5 percent per annum during the period, a fuller measure of ‘total wealth creation’ would have grown by only 0.3 percent to 1.7 percent per year – that is, ‘growth’ would have been at most half what we registered, at worst virtually non-existent. What we are effectively doing is counting the positive monetary growth of the market system’s ‘fix’ loop, while ignoring the unmonetized costs of the ‘fail’ loop. Scale of externalities not yet accepted Possibly the key driver of our sustainability crisis is that the dominant Western culture has not achieved sustained acceptance – distinct from mere theoretical admission – of the scale of market externalities and what that must imply for claims made about the superiority of market-led coordination. ‘We need more data’ seems to be the universal belief. No, we don’t. We have more than enough data. We need acceptance, which is qualitatively different to admission, such that we cannot simply disclose more and more data and expect to arrive at acceptance. he failure of economics to cultivate the sustained acceptance of externalities is increasingly becoming the most pertinent fact about the whole discipline. Not only did economics – the ‘science of markets’ – not encourage acceptance of externalities, but it also made strenuous effort to downplay or even trivialize them. The hope of early economists, subsequently bolstered by ideals of complete market theory, was that market systems would be self-regulating, removing or minimising the need for government regulation. And, certainly, an economy contains many balancing processes. If the demand for bread increases, the price of bread will rise inducing more supply so bringing the price back down again. The market contains a great many ‘self-regulating’ or rebalancing, loops. However, the market is not only self-regulating, but also susceptible to positive reinforcement loops that can become runaway problems. This was crystallized by Brian Arthur in 1990, when he identified that economic systems did not just exhibit ‘diminishing returns’ – or balancing loops – but also, quite commonly, ‘increasing returns’ – or reinforcing loops. Because neoliberalism has granted markets primacy, and because markets are vulnerable to large-scale runaway effects, neoliberalism is effectively a runaway feedback loop. Many of our biggest problems - global debt accumulation, wealth inequality, climate change and biodiversity loss – all exhibit runaway, vicious spiral, dynamics seemingly beyond the powers of the market to rein in. The erosion of the government’s capacity to modulate market forces is itself the consequence of certain powerful reinforcing loops intrinsic to neoliberal logic. In what might be called ‘Friedman’s Feedback Loop’, corporations’ ‘social responsibility to maximize profits’ has, over time, seen them spend large amounts of money lobbying government to change the rules to allow them to increase profits, providing them with more resources to lobby governments etc.… As this inexorable process of regulatory capture persists, a society progressively steers less by a sense of what is ‘good’ and more by what is ‘profitable’. Equally, in what might be called ‘Reagan’s Reinforcing Loop’, if perception spreads that ‘markets are the solution and government is the problem’, human talent will slowly but surely be drawn towards the private sector and away from public service. Our still extending – and so still incomplete – market system continues to annex new, previously uncommodified, realms, but in asymmetrical fashion. It opens new frontiers of profit but cordons off areas of potential cost. The argument for market primacy is predicated on the power of price signals to achieve a more efficient allocation of goods and services than might be achieved by the cumbersome and coercive ‘central planner’, but in practice, this power of price signals can now only be extended to generate new profits, not new costs. And so, we are denying ourselves the use of the market system and its price signals to tackle possibly the most critical scarcity problems we have ever identified – limited atmospheric capacity for greenhouse gas emissions and limited capacity of ecosystems to absorb or tolerate our activities. As such, one can turn the tables and ask: if we don’t need prices for the greenhouse gas emissions driving runaway climate change, and we can instead rely on people voluntarily to take the steps consistent with those prices without them being implemented, why bother having prices for anything at all? Why not just assume that people will always voluntarily behave in ways that collectively advance human welfare? Among the more effusive accolades made of the market system is that it is a form of ‘intelligence’. And while there is something to this in the autonomous way the market system marshals goods, services and human time and effort, it can only be a partial intelligence because the market has no inkling of the non-commoditized and non-priced world. A market-centric culture commits to follow where profit leads. Quite important, then, that either we calculate profit sustainably or we temper our market-centricity. At a yet deeper level, one can trace the threads of today’s market-acquiescing VML response to momentous cognitive developments of the 17 th century that shifted the Western world onto an entirely new cultural metaphor, which in turn paved the way for our excessive credulity in the power of markets, and our equating of market growth with moral improvement. Just as scientists rarely waste time questioning long-established axioms of their discipline, so our shared myths or metaphors warrant little comment or reflection because, after all, the view from within a culture is that it is so obviously the way the world is and must be! The essence of reductionism is that knowledge can best be acquired by breaking things down to pieces, learning how the pieces work and then ‘adding back up’ this knowledge to arrive at a greater comprehension of the whole. At the dawn of the Scientific Revolution, reductionism profoundly shaped our sense of what science even was. A scientific method was then ‘too hastily expanded’ into other fields of investigation, including – fatefully – those concerned with living, complex things. This had the inadvertent consequence of rendering other methods of investigation more suitable for living things as ‘non- scientific’ – a pejorative designation that has catastrophically held back our comprehension of the living world, including ourselves, and encouraged a dismissiveness of non-Western insights and traditions. However, the slowly-dawning meta-learning of the Scientific Revolution, spearheaded by the rise of systems thinking, is that as you move ‘up’ from inanimate objects to more complex systems – from ‘dead’ things to ‘living’ things – so reductionism gradually loses its power as an explanatory method, because the ‘add back up’ assumption breaks down. A core essence of systemism is that ‘the whole is more than – or different to – the sum of the parts’. ence as we move up, we repeatedly encounter new levels of organization whose behaviours cannot be fully anticipated even with complete knowledge of the parts. These ‘discontinuities’ in emergent complex systems represent new levels of complexity, requiring understanding in their own right. Most important of all – though we cannot get into it here – is that at some point on this upward journey, we encounter something we have come to call a ‘mind’, capable of deriving meaning from the world and forming expectations about it, both critical, but subjective, capacities of living things brutally excised from the reductionist ‘scientific’ view of the world. Essentially, the Western mind fell into a Valley of Reductionism, which was positively beneficial for physics and chemistry, but which has been a decidedly poor vantage point from which to understand living systems, including human society and global ecology. The problem is not that Smith or Mandeville were wrong about the market’s powers to transform greed into good, but that we came to believe that the market could fully capture and neutralize greed. But if markets are incomplete – if externalities exist – then markets do not capture and neutralize all the effects of greed, with the consequence that some greed slips through the market net and behaves like, well, plain old greed with excesses that destabilize the social and ecological system. We have lost sight of the earlier, intuited, understanding that admonitions against greed constituted an important balancing loop in the complex system of human society. Having given greed freer rein, we have gradually super-sized the impulse via the creation of corporations – ‘corporate persons’. We have a market system that does not ‘add everything back up’ and so cannot neutralize all greed, some of which spills out to drive inequality and destroy the ecosystem. At the same time, old cultural injunctions against greed have lost their potency. With incomplete markets and diminished capacity to appeal to individuals’ moral sense, we find ourselves systemically and institutionally induced to free ride upon each other and the social and ecological costs slowly accumulate in the background. Though we have tended to perceive our ecological challenges as dating from the mid-20 th Century, I believe we will not be able to solve them until we recognize them as a lagged response to profound cultural shifts dating back to (at least) the 17 th Century. That does not imply that we need to roll back the clock and reinstate a pre-market society, only that we need to re-assess each link of the chain of thought that brought us to this point to see whether each link still makes sense in the abruptly different context of the Anthropocene. If the context changes around a culture – as with the shock of the Anthropocene – a whole culture may end up displaying unconscious incompetency, by, say, polluting its own atmosphere, destroying its natural base, or creating social inequalities that slowly tear society apart. World as Nested Complex System Seeing the world as an Emergent Nested Complex System brings three key features of living systems into view that enable us to understand how complex systems might adapt in a crisis. Enabling constraints. The innate structure of a complex system is the counterintuitive idea of ‘enabling constraints’ or ‘constraints that deconstrain’. ‘Constraints that deconstrain’ expresses the idea that the pay-off to the constraint is a new, ‘higher’ space for the system to explore. The idea that a constraint is an enabler or, in reverse, that ‘freedom comes from constraint’ is counterintuitive but it pervades systems architecture and is everywhere once one starts to look for it. What pops out towards the top of the human stack of constraints are experiences we describe as ‘free’. Individual freedom is dependent on the underlying enabling constraint of a Rule of Law and prior norms of justice. What gives us our freedom is everyone else constraining themselves to respect our rights. The misleadingly named ‘free market’ is entirely dependent on the enabling constraint of legally enforceable property rights. Constraints are the sine qua non for the self-organization of an emergent system. They are the scaffolding that brings complex life up out of the primordial soup. Path dependency Complex systems simply have not had enough time in the history of the Universe to try all possible solutions to land on the optimal path, so each complex system constitutes a highly path-dependent exploration of a vast ‘possibility space’. One visualization of our sustainability crisis is that our human complex system has emerged in a context of an effectively limitless world with new frontiers always on hand, only to abruptly find itself in a finite and fragile world. Our emergent behaviour was not premised on this assumption and many ‘old’ ideas, habits, traditions and customs are no longer consistent with it, even though they are familiar and engrained. In some sense, the human ‘stack’ is now perceived to be in the wrong place relative to where it needs to be. Two directions of adaptation The emergence of each complex adaptive system represents an upward exploration within the space laid out by lower constraints, of which the base immoveable constraints are the laws of physics and upon which each successive layer establishes successive guardrails directing further upward movement. With the ‘extended order’ market system now on top of the human stack, we have found a mechanism that accelerates upward innovation via the spur of individual profit gain. New ideas are ‘market-tested’ to see what works and what does not. But the implication is that we are emerging upwards where profit directs. In contrast, there is a second type of learning which can be visualized as moving sideways, in an ‘unlearning and relearning’ process – or an ‘uninstall and reinstall’. It might also be termed ‘calibrative’ learning because it amounts to recalibrating some existing parts of the established structure in the face of new context. In a sense, a complex system is continually correcting for imbalance. Such learning is effortful and costly, because it is not just the ‘building on top’ of upward learning but the intentional breaking of a trusty habit or once-cherished custom, which may have been justified at an earlier time but is now deemed wrong or unhelpful. Part of the cost of sideways learning is the difficult crystallization of a psychic loss, or that what we may have been doing for years or decades is now ‘wrong’, by today’s contextual demands. The innate tension between VML strategies and policy and cultural changes is that they represent two fundamentally different directions of adaptive response. Many of our green solutions represent yet more complexity, not just in the design of more high-tech components – from intricacies of new batteries to advanced materials – but in the more complicated supply chains and two-way, intermittency-handling electricity grids behind the scenes. All the ‘clever’ sensors, smart meters and real-time demand management programs add up to more complexity. Overwhelmingly, our VML strategies are upward movements. The beneficial adaptive power that lower levels have over the higher levels is that they can reframe the space in which higher level complexity can take place. Hence, with its powers to set property rights, prices and regulations, government has the capacity to re-shape the entire ‘market space’. The real adaptive value of this power is to constrain or abolish activities currently taking place at the higher level, which are now deemed to be a threat to the system. The cost of lower change is why our first-choice strategy is invariably to try and avoid it. Policy and culture are prior to markets. The widespread inclination by VML practitioners to believe that any sustainability effort is better than none at all misses the tension that exists between VML and policy strategies. The natural enthusiasm for VML is in part a manifestation of the difficulty of calibrative learning. We are hoping that market-led innovation may obviate the need for wrenching uninstall and reinstall of property rights, laws and cultural norms, but it is looking less and less likely that will work. Yet, as a market-reaffirming movement, the execution of VML strategies is a daily reinforcement of market primacy, which postpones deeper level policy and cultural changes whose beneficial adaptive power is precisely that they can choke off certain market activities we can no longer tolerate, in a way that market-conforming VML strategies cannot. Where the market surpasses government is in its real-time facility to respond to ever- changing supply and demand signals. Markets are great at detail. An emergent nested view of human society suggests that a better conception of government is as a ‘central director’ not as a ‘central planner’. What a central director can do much more effectively than markets is work at a lower level of the system to redirect the innovation space in broad brush strokes by adjusting property rights. Granting that a ‘central director’ – preferably of capable, elected human beings – can see broader and further into the future than partially-sighted markets, would be to hitch a complementary intelligence to the market intelligence we largely steer by today. Re-legitimizing government’s role to establish new property rights – ‘you know, government might be a key part of the solution’ – should now be a central goal of any business with sustainability aspirations. It would effectively be to re-embed markets that have become, as Polanyi warned, too disembedded. It would be to make markets a tool of human culture, not human culture the by- product of markets. It will dawn on people that an immediate consequence of limits-respecting markets must be a slowdown in ‘growth’ as we have been measuring it because they would reflect new limits we have not been considering. In the real world of very incomplete markets, things of human value lie in two separate realms – the marketed domain and the non-marketed domain. Some of the growth of the marketed economy genuinely arises from human ingenuity and creativity unlocking better ideas and products from new combinations of inputs. This seems like ‘good’ growth, which ought to be celebrated and encouraged. However, other parts of monetized ‘growth’ arise from simply running down the stocks of what is valuable but in the non-marketed realm. This is the illusion of wealth creation based on registering the increase in marketed value, but not recording the decrease in unmarketed values. In contrast to growth from genuine ingenuity, this is ‘wealth’ conjured up by the Unmentionable Cost-Shifting Foot. Our measured economic ‘growth’ overall combines in unknown proportions a ‘creative growth’, which we want to encourage, and a ‘parasitic growth’, which we do not. I don’t know if the term ‘capitalism’ would survive the journey to this new cultural ground – or whether that would even be desirable. The specific problem with the term ‘capitalism’ is that its mere utterance upholds the entrenched view that markets have primacy over policy and culture, which just locks in the externality-denying capitalism we currently practice. Instead, the goal must be to have a market system that operates within a human cultural context that recognizes market externalities are real and significant. This wouldn’t be capitalism or socialism so much as a model of limits- respecting social coordination, for which I am not sure there is yet a name. Returning to the surface, we arrive not at a ‘sustainable economy’ but at the ‘economy of a sustainable culture’. We will have effectively internalized that sustainability is a property of the whole, not of the parts. Our current market primacy of self-coordination is rooted in thinking that is entirely pre-Anthropocenic. The great hope of the VML strategy was that there would be a business case for sustainability. But, if sustainability must mean ‘sustainable enough before it is too late’, the meta-learning is that there is just not much of a business case to rely on. The business case is simply too weak and compromised a force to promote enough change fast enough. Instead, the moral case for sustainability is going to have to carry most of the load from here. The shock of the Anthropocene profoundly challenges some of the core assumptions of the last 50 years – if not 300 years – of Western culture. The sustainability challenge is nothing less than our ability to transcend the neoliberal stage of development we had reached and prematurely thought might be the end of our quest for the ideal form of human self-organization. But, it has seen those that would defend today’s capitalism merely defending an externality-denying capitalism we cannot afford. To counter the usurpation of markets over culture, we now need a reverse usurpation whereby those who can see what has happened work to re-legitimize government and other cultural institutions to take the tough, moral decisions that are beyond the reach of corporations duty- and norm-bound to profit-maximize. The long-run goal for a sustainable business should be to help forge a sustainable culture. Austin (2021) Market-led Sustainability is a ‘Fix that Fails’… (pdf) 44.1.2 ‘Sustainable Growth’ - An Oxymoron Daly Memo It is impossible for the world economy to grow it’s way out of poverty and environmental degradation. Sustainable growth is impossible. In its physical dimanesions the economy is an open sub-system of the Earth ecosystem which is finite, nongrowing and materially closed. As the economic sub-system grows it incorporates an even greater proportion of the total ecosystem into itself and must reach a limit at 100 percent, if not before. Therefore its growth is not sustainable. The term “sustainable growth” when applied to the economy is a bad oxymoron - self-contradictory as prose, and unevocative as poetry. Sustainable Development (qualitatively) OK Even ‘green growth’ is not sustainable. In the past 200years we have developed a culture dependent on exponential growth for its economic stability. To delude ourselves into believing that growth is still possible and desirable if only welabel it ‘sustainable’ or ‘green’ will just delay the inevitable transition andmake it more painful. Precisely because quantitative and qualitative change are very different it is best to keep them separate and call them by different names. To grow means ‘to increase in size by the addition of material through assimilation or accreation’. To develop means ‘to expand or realize the potentialkities of; to bring gradually to a fuller, greater, or better state’. When somethings develops it gets different. The concept of optimal scale of the aggreagte economy relative to the ecosystem is totally absent from current macroeconomics. Microeconomics, which is almost entirely devoted to establishing the optimal scale of each micro level activity by equating costs and benefits at the margin, has neglected to inquire if there is not also an optimal scale for the aggreagte of all micro activities. Nonrenewable resources should be depleted at a rate equal to the rate of creation of renewable substitutes. Projects based on exploitation of nonrenewable resources should be paired with projects that develop renowable substitutes. The net rents from the nonrenewable extraction should be separated into an income component and a capital liquidation component. The capital component would be invested each year in building up a renewable substitute. The separation is made such that by the time the nonrenowable is exhausted, the substitute renewable asset will have been build up by investment and natural growth to the point where its sustanable yield is equal to the income ’component. The income component will have thereby become perpetual, thus justifying the name ’income’æ. which is by definition the maximum available for consumption while maintaining capital intact. Daly (1990) Sustainable Growth. An Impossibility Theorem (pdf) 44.2 Resource Extraction Bardi on Hubbert The well known “Hubbert curve” assumes that the production curve of a crude oil in a free market economy is “bell shaped” and symmetric. The model was first applied in the 1950s as a way of forecasting the production of crude oil in the US lower 48 states. Today, variants of the model are often used for describing the worldwide production of crude oil, which is supposed to reach a global production peak (“peak oil”) and to decline afterwards. The model has also been shown to be generally valid for mineral resources other than crude oil and also for slowly renewable biological resources such as whales. Despite its widespread use, Hubbert’s modelis sometimes criticized for being arbitrary and its underlying assumptions are rarely examined. In the present work, we use a simple model to generate the bell shaped curve curve using the smallest possible number of assumptions, taking also into account the “Energy Return to Energy Invested” (EROI or EROEI) parameter. We show that this model can reproduce several historical cases, even for resources other than crude oil, and provide a useful tool for understanding the general mechanisms of resource exploitation and the future of energy production in the world’s economy. Figure: Fitting of the data for oil discovery in Norway and of the number of wildcats. In this case, the number of wildcats is proportional to the capital used by the oil industry in the effort of discovering the resource, oil wells. Bardi (pdf) 44.3 Peak Theory Insofar as economic growth is driven by oil consumption growth, post-peak societies must adapt. Hubbert believed: Our principal constraints are cultural. During the last two centuries we have known nothing but exponential growth and in parallel we have evolved what amounts to an exponential-growth culture, a culture so heavily dependent upon the continuance of exponential growth for its stability that it is incapable of reckoning with problems of non growth. Wikipedia: Hubbert Peak Theory Hubbert ( ) Exponentital Growth as Transient Phenomenon in Human History (pdf) Fix I’ll close by returning to where I started: the Simon-Ehrlich wager. What’s important about this wager is that it conforms to our expectations about prices. Ehrlich bet money on the idea that resource scarcity will cause prices to rise. It’s an idea that most people find intuitive. Simon bet money on an equally intuitive idea — that resource abundance will cause prices to fall. Looking at the bet, you can see that it’s really about two distinct hypotheses. The first hypothesis is that we’re exhausting our natural resources. The second hypothesis is that prices will rise in response. What’s interesting is that most of the discussion about the Simon-Ehrlich wager conflates the two hypotheses. Because Ehrlich lost the bet, people assume that resource scarcity is not a problem. But that’s faulty logic. What’s also possible (and what all the evidence points towards), is that the price hypothesis is wrong. As we exhaust natural resources, their price does not explode. Instead, it collapses. Figure: Oil purchasing power in the real world … and projected future. Solid lines represent real-world trends for the growth of US nominal GDP per capita and the nominal price of oil. I’ve smoothed the data to more clearly show the long-term trend. Dashed lines continue the recent trend into the future. Even though Ehrlich lost his bet, his thinking remains widespread. Just look at peak-oil theory. Many peak-oil theorists think that as oil production declines, the price of oil will explode. But not everyone is convinced. The notable exception is the analyst Gail Tverberg. For years, Tverberg has been arguing that we’re headed for lower oil prices. (Here’s a thread of her writing on deflation.) But she doesn’t think prices will fall because of resource abundance. She’s a Malthusian much like Paul Ehrlich. Instead, Tverberg thinks we’re headed for a world where oil is scarce yet cheap. To many people, such a future makes little sense. But that’s because we can’t imagine a world in which incomes collapse. But Tverberg can. And so I propose a hypothetical bet for the future: Ehrlich vs. Tverberg. Both scientists assume that oil will get more scarce. But in the Ehrlich scenario, oil prices explode. In the Tverberg scenario, oil prices collapse. I once thought that the Ehrlich scenario was all but guaranteed. But today, my money’s on Tverberg. In the future, oil will be scarce and unaffordable. But I think it will also be cheap. Fix (2021) Scarce but Cheap and less affordable 44.4 Energy Transition Durand A regulatory, or market rational, state concerns itself with the form and procedures – the rules, if you will – of economic competition, but it doesn’t concern itself with substantive matters […] The developmental state, or plan- rational state, by contrast, has as its dominant feature precisely the setting of such substantive social and economic goals When one considers the economic challenges of restructuring economies to keep carbon emissions in line with the stabilization of the climate, this discussion acquires a new framing. Eﬀectiveness must take precedence over eﬃciency in reducing emissions. That means abandoning the fetish of the price mechanism in order to plan how the remaining dirty resources will be used in the service of clean infrastructure. Such planning must have international reach, since the greatest opportunities for energy-supply decarbonation are located in the Global South. Moreover, as transformation on the supply side will not be enough, demand-side transformations will also be essential to stay within planetary boundaries. Energy requirements for providing decent living standards to the global population can be drastically reduced, but in addition to the use of the most eﬃcient available technologies, this implies a radical transformation of consumption patterns, including political procedures to prioritize between competing consumption claims. Durand (2021) Energy Dilemma (NLR) (pdf) Tooze on Durand As an account of the 2021 energy crisis it is fundamentally misleading. It attributes far too much influence to climate policy and mistakes the basic dynamics of investment in the sector. Rather than the political trajectory of climate policy, the starting point for an analysis of developments in the energy sector in recent years should be the energy market price shock of the summer of 2014, when, between the summer of 2014 and 2016, an aggregate index of energy prices fell by two thirds. It is this collapse of global energy prices that has dictated both the patterns of investment in the global energy industry, and the balance between fuel types in electricity generation After the 2015 Paris accords, the ramping up of climate ambition did coincide with a slump in gas and oil investment. But, it was not the former that caused the latter. After 2014 investment plunged. The huge shock to energy prices also led to an adjustment in the pattern of energy use. If gas could be bought at rock bottom prices, thanks in part due to the abundance created by the American shale revolution, then flexible gas-fired power plants could replace coal-fired electricity generation. After 2014 the “dash for gas” was key to driving coal out of the power-chain. Coal was simply uncompetitive relative to gas. Thus, coal dies in Europe and the US in the years after the Paris conference of 2015. But the causal effect runs through the energy market. The fossil fuel sector did not so much retreat after 2014 as regroup. Exxon and co imagine a future for themselves as suppliers of feedstock to the global chemicals industry. Saudi Arabia and the Gulf producers know that they will be the global suppliers of last resort. As far as oil is concerned, in 2021 the energy dilemma is a non-factor. The current surge in oil prices is the result of a deliberate policy decision by OPEC and Russia to throttle production and allow prices to rise. The producers want to take profits to restore their cash balances and reward their investors for their patience since the shock of 2014. The idea of a secular retreat of fossil fuel investment under the sign of an “energy dilemma” simply misses the mark. Over the last fifteen years, whilst it was trumpeting its commitment first to the Kyoto climate protocol and then to the Paris agreements, the EU made a substantial physical and financial investment in integrating its system of gas supply with global energy markets. Since the early 2000s, the aim of EU gas policy has to been to create a “liberalised” gas market, that amongst other things would reduce its cold war-era dependence on supply and pricing through contracts with Russia. Furthermore, the EU has moved to pricing gas not on the basis of long-term supply contracts but by way of the spot market. Seen from this point of view, the huge capacity of pipelines and LNG terminals is not redundant. It is the physical infrastructure that has enabled Europe to play the global gas markets. Gas is less polluting than coal, but this strategy was not driven by the desire to minimize CO2 emissions, but to minimize energy costs and achieve energy security. The issue is not that post-Paris climate ambition depressed investment, narrowed margin of supply and created conditions for spike. That entirely exaggerates significance of green policy. What 2021 exposes is that the green push since 2015 has been enacted against the backdrop of a regime of low energy prices set by the price collapse in 2014. The lesson is not that the EU has been pushing green too hard, too fast. The lesson is that if China and the rest of Asia embark on a huge dash for gas, Europe’s investment in market-based gas import model is very high risk. The logic of diversifying away from Russia was good, until you ran into China. The solution is not less commitment to the energy transition but more. Tooze on Durand (2021) Chartbook #51: Explaining the energy dilemma of 2021- the 2014 shock and the global energy business. Pisani-Ferry A simple exploration of the essential mechanisms at work suggests that the transition to net zero will confront policymakers with serious macroeconomic difficulties. This transition is unlikely to be benign. Techno-optimism is no reason for overlooking transition cost Because of the accelerated pace of climate change and the magnitude of the effort involved in decarbonizing the economy, while at the same time investing in adaptation, the transition to net zero is likely to involve, over a 30-year period, major shifts in growth patterns. Effects will include - a significant negative supply shock, - an investment surge sizable enough to affect the global equilibrium interest rate, - large adverse consumer welfare effects, - distributional shifts, and - substantial pressure on public finances. If too gradual in the years to come, the transition is likely to prompt precipitous adjustments later. If too swift it is bound to entail large losses resulting from the accelerated obsolescence of existing capital stock and the limited availability of cost-reducing innovations. An increase of 2 percentage points in the investment- to-GDP ratio would more than reverse the decline in the world investment ratio between 1980–89 and 2010–19. The main message from a simple analysis is that while discussions of the relative roles of innovation and investment, or the desirable combination of price signals and regulation, remain important, it is high time to realize that climate policy is also macro policy. A better, more precise discussion on the macroeconomics of climate action is urgently needed. In this context, debates should focus more on identifying the mechanisms and choices involved in what is bound to be a challenging transition. Pisani-Ferry (2021) Climate Policy is Macroeconomic Policy, and the Implications Will Be Significant (Peterson Institute Policy Brief) (pdf) Christophers ‘Abstract’ The transition, and the ‘fossil fuels versus renewables’ question at its core, is about investment, not price. To be sure, investment decisions are themselves shaped by price, as indeed they are shaped by government policy and regulation – which, needless to say, can and to one extent or another will reshape the ‘purely economic forces’ acting on technology shifts and which, deployed in a highly interventionist way, would potentially render the energy transition something other than strictly ‘capitalist’ (i.e. market driven). But investment decisions are not determined by price. The nub of investment is proﬁt. Informed by this perspective, the article ventures a diﬀerent type of stock-taking of transition pro- spects than the IEA’s. If our focus should be not on price but on investment and proﬁt (a premise that the article seeks to justify), what can we say about current prospects? The article oﬀers one particular ‘cut’ at this question, focusing on the activities and investments of major Western fossil-fuel compa- nies – speciﬁcally, oil and gas producers. The pace and extent of the energy transition is as much about the winding down of fossil fuels as it is the ramping up of renewables. The main ﬁnding of the article is that for all the falling price of renewables such as solar, from the perspective of companies such as BP, Shell and Total the investment logic appears to remain weak. Christophers Memo The transition literature is not primarily ‘economic’ in nature. Rather, scholars have typically con- ceptualised the transition in terms of diﬀerent ‘socio-technical’ regimes comprising, in the work for instance of the inﬂuential Frank Geels, (a) networks of actors, (b) formal, normative and cognitive rules, and (c) material and technical elements. Successful transition, it is argued (e.g. Verbong and Geels 2007), requires positive and mutually-reinforcing developments in all three such domains. Economic factors represent just one of the multiple dimensions – alongside political, cultural and technical ones – on which clean-energy innovations compete with incumbent energy infrastructures. But, to the extent that the transition literature does concern economics, its focus, like the wider, public-facing, IEA-style discourse referenced earlier (and which at one level it undoubtedly informs), is squarely on price. To win out, renewables must be cheaper. It is perhaps unsurprising that it is widely believed that renewable energy needs to be cheaper than fossil fuels in order to comprehensively supplant them, for the received wisdom has long been that fossil fuels themselves originally became capitalism’s principal energy source on such an economic basis. Malm’s own Fossil Capital (2016) shatters that received wisdom, however. It does so in two ways. First, Malm meticulously demonstrates that the existing orthodoxy is belied by the facts. It is simply not true that water was scarce, in absolute terms or relative to emergent industrial requirements. Nor is it true that waterwheels could not generate as much power as steam. And, most signiﬁcantly of all, it is not true that steam was cheaper. On the contrary: water was, and remained, cheaper, mainly because it required no human labour to call forth its powers, whereas coal could only be transformed into an energy source through ‘massive’ inputs of costly human labour-power (p. 91). Second, Malm assembles a series of compelling alternative claims. He does so by treating energy transitions as what, under capitalism, they self-evidently are – phenomena crystallized through a series of active ‘investment decisions, sometimes with crucial input from certain governments but rarely through democratic deliberation’ (p. 268). Why, Malm asks, did early English cotton capitalists and then capitalists in other industrial branches decide to actively invest in steam? His answer is partly that steam was spatially advantageous. Unlike a waterwheel, a steam engine could be put up more-or-less anywhere, enabling the industrial capitalist to set up in the fast- growing northern towns where labour-power (not to mention other sources of agglomeration econ- omies) was concentrated, and many of the biggest of which happened to be located close to coal- mines. Water power was of course considerably less ﬂexible; ﬁrms had to go to it, source workers from elsewhere, and then invest in maintaining them – in the shape, most notably, of worker colo- nies, where the cotton mill-owner, lacking the luxury of being able to readily replace workers, was much more vulnerable to strike action. Steam was also temporally advantageous. Water’s irregularity of supply became a signiﬁcant problem in the context of the increasing demands of export markets. Furthermore, the ﬂexibilization of working conditions and long working days that were required to compensate for such irregularity and associated work downtime were substantially fettered by the 1833 Factory Act and later the 1847 Ten Hours Act. As Malm (p. 192) writes: ‘water followed its own clock – not that of the factory’. Steam-based production was much less aﬀected. Last but not least, steam ﬁtted much better in the brave new world of capitalist private property. Largescale, reservoir-based water-power schemes would perhaps have been preferable to steam for cotton capital at large, but such inherently collective arrangements fell foul of opposition from indi- vidual capitals who saw such schemes as a restraint on their independence and private property rights. Private property and water ‘did not mix well’; the latter, invested in at scale, required ‘com- plicated communal relationships’ (pp. 119-120). Coal and steam did not suﬀer the same ‘collective drawbacks’. For our purposes, in any event, the speciﬁc reasons for the victory of the steam engine and coal are less important than the more general implication of Malm’s account. Not only, he shows, was the early-nineteenth-century energy transition the combination of a series of investment decisions. But those decisions ultimately hinged not on price, but on proﬁt. The spatial and temporal advantages of steam consisted in the fact that that technology represented ‘a superior medium for extracting surplus wealth. To grasp the core dynamics of carbon capitalism, it is necessary to refer to ‘business decisions based on proﬁt margins’ and not ‘engineering, ecological or environmental concerns’. Political-economic scholars have not explicitly examined the shifting proﬁt nexus associated with diﬀerent energy generation technologies – the ways and extent to which, that is, these diﬀerent technologies enable capitalists to extract surplus value and to accumulate capital, and with what implications for transition prospects. This, then, is the focus of the present article. Around a decade ago, the world saw a step-change in levels of investment in renewable energy. Prior to 2010, total annual global new investment had never exceeded c. $180 billion; since 2010, it has never been lower than c. $235 billion The cost structure of low-carbon technologies such as solar and wind – the two energy sources that account for the vast bulk of renewables invest- ment in the past two decades – was and is a crucial factor. Both are highly capital intensive, which is to say that almost all the costs of energy production are incurred upfront: think of wind turbines. Here, notably, there is a signiﬁcant contrast with high-carbon alternatives, where, for say a coal or natural gas-ﬁred power plant, between 40 and 70 per cent of the costs are related to fuel and operating and maintenance expenses. The capital-intensive nature of solar and wind projects long heightened perceived investment risk and, as a result, project ﬁnancing costs. The economic constraints on the development of renewables were, as Gupta further noted, only compounded by the fact that gov- ernments worldwide continued to massively subsidise competing fossil-fuel energy sources, to the collective tune of an estimated $312 billion in 2009. ‘The high ﬁxed costs and low marginal costs of most low-carbon generators’ require ‘certainty of revenues’ in order to commit ﬁnancing. In the face of these perceived economic hurdles to renewables investment, governments in both the Global North and South, at various junctures, responded by introducing mechanisms of pro- motion and subsidisation. Stimulated by such government measures, capital responded with alacrity. Around the world, investment leapt, both in the technology deployed at renewable-energy plants and in the develop- ment and operation of solar parks and wind farms themselves. As it did so, costs fell dramatically, driven down by intensifying levels of competition. ‘The barriers to entry in the [renewables] sector’, as Nick Butler (2019) has observed, ‘are low – anyone can become an electricity producer and schemes that allow surpluses to be sold back into the grid are encouraging both businesses and households to build their own capacity’. In the context of the aforementioned lack of entry barriers and associated intense competition, producers accepted lower and lower prices, and utilities and other major oﬀ-takers eagerly encouraged and exploited their willingness to do so. Governments, meantime, saw the fall of production costs as reason to swiftly move to reduce or even remove the subsidies. Widely transitioning to awarding renewable-energy contracts on the basis of reverse auctions whereby, rather than buyers bidding prices up (as per traditional auction formats), sellers bid prices down. By 2017, zero-subsidy bids were recorded for the ﬁrst time in European oﬀshore wind auctions. Reverse auctions as a rule had become, ‘extremely competitive’. he terms of the private power-purchase agreements (PPA) through which generators frequently contract to sell speciﬁed volumes of electri- city were becoming markedly less attractive for renewables operators. Not only did prices on renewable-energy PPAs fall to record lows, but the period over which oﬀ-takers were willing to oﬀer generators ﬁxed prices – thus shielding them from the merchant-price risk of spot-market volatility – shrank. The upshot of all this is not hard to fathom. Proﬁts have been substantially squeezed. The key question, of course, is how far such investor willingness to accept lower returns will stretch. In May 2019, IEA data showed renewable energy deployments stagnating for the ﬁrst time since the turn of the century. ‘There is no lack of capital in the marketplace for good projects; there is, however, a lack of bankable projects to attract investment and fulﬁl today’s appetite for renewable energy projects’ In other words, in terms of project viability (IRENA’s ‘bankability’), renewable energy production was now in many ways back at square one. A decade or more after governments stepped in to stimu- late investment in a sector suﬀering from perceptions of excess investment-risk, things had come full circle. Industry participants fundamentally lack the market power to maintain price at signiﬁcantly above cost; and where the external government stimulus to investment was removed, the invest- ment case once again became marginal. Today, then, solar and wind-based energy generation for the most part are not attractive invest ment propositions. Shortened PPAs ‘mean that a project has a tighter window to hit its required returns’. Revenues arising after PPAs and their (typically) ﬁxed prices come to an end are referred to as ‘residual value’. It used to be the case that investors essentially ignored such value when assessing potential project returns. Not now, however. So parsimonious have PPAs become that renewable project sponsors reportedly are ‘relying on over half of their returns coming from the post-PPA period’ – an investment stance that, as Merchant notes, represents ‘a gamble on merchant power price forecasts that extend 15–20 years in the future’. The major players today in the renewable energy space are generally distancing themselves from the energy generation business per se, and focusing instead on technology manufacture and/or the development and servicing – but not the ownership and oper- ation – of generation plants. Generating electricity from solar, for now at least, is not where the money is to be made. There is no simplistic relation between energy prices and energy transition prospects. Low prices for renewable energy products can cer- tainly help drive the transition to carbon neutrality, but only if generators can deliver such low prices proﬁtably – if they cannot, or if the path to proﬁtability is not clear and compelling, the incentive to invest in renewable energy production will not be nearly substantial enough to drive investment on the scale that is ecologically necessary. BP looks for an IRR ‘of around 10%’. But its realistic assessment is that ‘expected returns’ on its renewable power projects will be in the 8–10 per cent range. That, of course, is higher than the returns – in the 4–8 percent range – that, as we saw earlier, existing operators are achieving. Is BP ignoring market realities, then? Not entirely. It accepts that ‘normal’ returns are lower – around 5–6 per cent. But it thinks it can lift returns to the 8–10 per cent level by virtue of three special ‘diﬀerentiating’ factors that it putatively brings to the table: operational and project expertise; integration; and structured ﬁnancing. The last of these stands out: elsewhere in the same document, ‘innovative ﬁnancing’ is identiﬁed as a key source of the ‘enhanced returns’ BP suggests it can possibly lift renewables returns to above 10 per cent if it utilises so-called ‘farm-down’ (otherwise known as asset rotation or build- sell-operate), which involves selling equity stakes in projects to outside investors during the pre-con- struction phase in order to free up capital for further projects. BP says its target for its renewables business is a return on average capital employed (ROACE) by 2030 of 8–10 per cent. This fundamentally is not, for now at least, a high-proﬁt business. They know, from experience, that wind and solar are not like oil and gas – that since, in Malm’s words, ‘the fuel is not hidden away in a separate chamber, but rather hangs like a fruit for anyone to pick, there is little surplus-value to extract in its production’ It has not helped the renewables’ case that, even now, they often remain visually as well as operationally and ﬁnancially peripheral at the oil and gas majors. The ﬂip side of the investment-calculus coin is the majors’ continuing robust investment in new hydrocarbon projects, which, by contrast, do still reliably oﬀer the returns they demand. This vast ongoing investment in oil and gas production creates huge, long-term inertia, locking the world into fossil-fuel energy landscapes for many, many years to come. All of this underlines the fact that unless the regulatory environment shifts in dramatic fashion, the world’s leading fossil-fuel producers, guided by the investment logics we have described, will long remain primarily fossil-fuel producers: they constitute fossilised capital, not mere fossil capital. This, ultimately, is the terrible paradox: to fund the transition to being something else (renewable energy producers), the oil and gas majors are relying heavily on what they currently are. The more negative market sentiment becomes, the more important the hydrocarbon business becomes. ‘The cash generated by hydrocarbons will be key to supporting [our] transition’, concedes BP. Surviving through, still less prospering from, the energy transition requires ‘allocating suﬃcient capital to our resilient hydrocarbons business to gen- erate sustainable cash ﬂow’. The sooner governments and regulators recognise this sobering reality, the sooner something substantive can be done about it. While renewable energies may now be widely competitive with fossil fuels on price, it is far from clear that they are competitive in relation to the producer proﬁts they aﬀord. Focusing on these companies is crucial for another reason. The energy transition is often pictured in terms of a transition in energy types. Of course, it is that; but it is not only that. It is necessarily also in signiﬁcant part a transition in the nature of a set of existing capitalist institutions. Investments that may appear logical from the perspective of economic theory and its bloodless and rootless agents and its ready availability of capital may be illogical from the perspective of worldly institutions that arguably now face an existential battle and for whom the question is not fossil fuels or renewables, but rather what mix of the two in the short and medium term will enable a long-term shift from the former to the latter that is maximally proﬁtable while also meeting complex, ﬂuctuating and overdetermined criteria of social, political and ecological tolerability. Christophers (2021) Fossilised Capital: Price and Profit in the Energy Transition (pdf) "],["demographics.html", "45 Demographics 45.1 China", " 45 Demographics 45.1 China Figure: China Population Pyramid 1950 2019 2050 Batson The working paper on demographics recently published by the People’s Bank of China is a pretty interesting document, and has gotten more than the usual amount of attention. It doesn’t read much like the cautious, dry and technical papers previously released by this august institution. There’s not much quantitative analysis or rigorous logical argument; it’s more like an extended op-ed, arguing vigorously that major demographic changes for China are coming and that the country needs to wake up to that fact and adapt quickly. This call to arms is well-timed. It seems likely that the much-delayed figures for China’s 2020 population census will confirm what many demographers have been saying for a while: that China’s fertility rate has been overstated, and therefore that its demographic transition and the aging of its population are going to happen even faster than standard forecasts project. Batson 2021 Demographics might change everything for China Hao Abstract: Since the industrial revolution, the death rate and birth rate have fallen successively, which has created a demographic transition and brought people to the world. Mouth explosion, demographic dividend, aging and declining birthrate. Developed countries, as pioneers in the transition, underestimate people The role of the population and the seriousness of aging and declining birthrates overestimate the importance of education technology, encourage childbirth, and improve the elderly. effect. Since the founding of the People’s Republic of my country, the population of our country has expanded from a rapid growth to a slowdown, and the population structure has grown from a pyramid to a long one. It is square, and our country’s population transition time is shorter, aging is faster, and declining birth rate is more serious. Our country must recognize The demographic situation of the Qing Dynasty has changed. It is necessary to realize that the demographic dividend was used comfortably at the time, and it is a debt that needs to be repaid afterwards; It is necessary to realize that population inertia is a huge force across generations, and its reactionary force will cause the population to change in the opposite direction; Realize that education and technological progress cannot compensate for the decline in population. To this end, we should fully liberalize and encourage childbirth, Really solve the difficulties of women in pregnancy, childbirth, nursery school, and school, comprehensively implement strategies, and work hard for a long time. Now we have a long-term plan for 2035 and a century-old goal. Memo: The transformation of the world population The four stages of demographic transition 2 Since the industrial revolution at the end of the eighteenth century and the beginning of the nineteenth century, 3 economic and social development has led to population deaths The birth rate and birth rate have successively declined, but due to the time lag between the two declines, the world has experienced “low growth (I) -Accelerated Growth (II)-Growth Slowdown (III)-Low Growth (IV)” four stages of population transformation. In the first stage (agricultural society before the industrial revolution is usually at this stage), productivity is underdeveloped, and the population is dead. The death rate is high, but in order to maintain the stability of the population size, the birth rate is usually high. This leads to the population age structure Pyramid shape, low dependency ratio for old age, high dependency ratio for children 4, slow economic growth. Phase II (initial and mid-stage of industrialization), with income growth, nutrition, hygiene and medical conditions Improvement, the mortality rate of the population declines rapidly, but the birth rate is usually difficult to follow. This leads to a rapid population size Increase, the population structure develops from a pyramid shape to a rectangular shape 5, that is, the decline in the mortality rate causes the elderly population to occupy The ratio and the proportion of the labor force have risen, pushing the top and middle of the pyramid to widen, but the birth rate has not decreased accordingly. Therefore, the bottom narrowing is not obvious. During this period, both the old-age dependency ratio and the child dependency ratio showed a downward trend. The growth rate is accelerating. In stage III (the middle and late stages of industrialization), the mortality rate of the population further decreased, but the rate of decline decreased. Contrary to what Malthus expected, the birth rate did not increase with the improvement of nutritional conditions, but decreased. (Google Translation) Hao (2021) Cognition and countermeasures about China’s population transition PBC WP2021/2 "],["development-economics.html", "46 Development Economics 46.1 Structural Reforms 46.2 TechFare 46.3 How Asia Works 46.4 Social Provisioning of Needs", " 46 Development Economics Trainer Thinking about development is dominated by a conventional conception which takes for granted the centrality of increasing production for sale, integration into the globalized market place, moving to more sophisticated technologies, and the goal of rising to affluent rich-world living standards. Basic criticisms of this conception of development are briefly summarized, firstly to do with the way it has primarily benefitted the rich and secondly regarding its grossly unsustainable resource implications. Global biophysical resource endowments prohibit its realization. There has been remarkably little thinking from conventional or critical sources on the goals and means which a sustainable alternative must take. The Simpler Way project is concerned to show the necessity for, and desirability and workability of, the development of mostly small scale, cooperative, highly self-sufficient and self- governing local economies focused on meeting basic needs, and not concerned with economic growth, globalization, competing in the global market place, or aspiring to rich-world “living standards”. It is argued that only some form of Simpler Way can enable satisfactory global development within sustainable resource and ecological limits. The major fault in most if not all previous development thinking has been failure to grasp the need for materially simple lifestyles and systems. Conventional development can be regarded as a form of legitimized plunder. Alternative, appropriate development … The simpler way The basic element in appropriate development is the small, highly self-sufficient and largely co-operative local economy. The transition can be a process of gradually building a new “Needs-Driven-Economy” underneath the old “Profit-Driven-Economy”. It can begin by a few coming together as a Community Development Cooperative to organize the provision of some neglected basic goods and services, for example by setting up community gardens, poultry co-ops or aged care rosters. Their long term goal would be to increase these cooperative, socially desirable non-market activities until they might largely replace the old economy. Trainer (2021) Third World Development RWER 95 (pdf) 46.1 Structural Reforms Braun on Draghi’s ‘Refrorm Thesis’ Draghi was a structural reformer avant la lettre: He uses the term “reform” exactly as it would come to be used in “structural reforms”. Except that the concept didn’t exist at the time. Draghi’s thesis fully articulates the theory that came to bring us structural reforms: A planner opting for short-run stimulus will never reach the optimal long-run path. By contrast, enforcing optimum long-run policies today will not have negative short-run consequences. Noting that “the common finding is a positive relationship between real wages and employment”, Draghi seeks to refute that finding, describing it an artifact of faulty methodological choices. Structural reformers must reject the idea of a positive wage-employment relationship because SRs are supposed to boost employment precisely via lower real wages. Braun - Twitter Thread on Mario Draghi’s Thesis Tooze on Draghi/Yellen new assignments It would be absurd to blame either Draghi or Yellen personally for the sequence of shifts and shocks that has destabilized capitalist democracies since the 1990s or the crisis of confidence these have triggered among centrist liberals. But as people of huge influence and as representatives of a class of experts who have ruled the roost for the last 30 years, they can hardly plead innocence either. It was on their watch that growth slowed, inequality between social classes and regions became ever deeper, and the risk of inflation tipped into that of deflation. It was on their watch that the financial system was allowed to become a flywheel of mass destruction. It was on their watch that the risks of climate change and pandemic threats went unaddressed. If broad-based growth cannot be restarted, the implications are alarming. Whereas the market revolutionaries of the 1970s and ’80s were radicals, squashing the last bastions of the old left and bulldozing organized labor out of the way, Draghi and Yellen came to the fore in the 1990s as managers of what is now known as the Great Moderation. Inheritors of the market revolution, committed to managing and improving the status quo, Draghi’s and Yellen’s march through the institutions has been glorious, but their careers have also been defined by constant adjustment to political and economic shocks that they did not foresee and could not control. These shocks have driven Yellen and Draghi to explore the political and economic boundaries of technocratic power. At MIT and Yale in the 1970s, they imbibed what was known as the neoclassical synthesis. The central idea was that though the microeconomics of markets were important, markets would function properly only so long as the macroeconomic environment was set correctly. Keynesianism and market economics were not opposites but complements. In the 1980s, Yellen played an important part in shaping the further development of the neoclassical synthesis known as New Keynesian economics. Working alongside the likes of Joseph Stiglitz and George Akerlof, she mapped how labor market imperfections could give rise to macroeconomic problems. Those rigidities in wages and prices, in turn, also enabled macroeconomic policy to work. It was because markets were slow to adjust that unexpected movements in interest rates, taxes, and government spending could have real effects. Draghi’s work at MIT was less intellectually generative than Yellen’s. But his dissertation is nevertheless revealing. It includes a chapter in which he describes how planners trying to manage an economy subject to short-run fluctuation are more successful if they focus on long-run goals. Long-range strategy, regardless of short-term cost, will do better than a hectic effort to optimize at every moment. Though they owe little to the Chicago school, it does not follow that Draghi and Yellen were not exponents of neoliberalism. On the contrary: They were strong advocates of markets. Competition and properly designed incentives were the recipe for productivity and growth. In the world economy, they favored the free capital movement and flexible exchange rates that defined the so-called Washington Consensus of the 1990s. It was Rudiger Dornbusch, the pope of international macroeconomics at MIT and one of Draghi’s chief mentors, who described the project of his generation as being the taming of “democratic money.” In the wake of the collapse of the Bretton Woods financial order and the U.S. dollar’s gold peg, the chief enemies of good economic governance were shortsighted trade unions pushing for higher wages and vote-chasing politicians. Once trade unions were curbed and politicians confined to their proper tasks, Friedmanite monetarists hoped that prices could be stabilized by mechanical monetary rules. But by the early 1980s, that had proved naive. For the MIT crowd, what keeping money safe from democracy amounted to was placing it under the control of competent experts credibly committed to providing markets with the stable framework they needed. The independent central bank was their institutional bastion. The global financial order developed by economic elites—from the 19th-century gold standard to the gold-pegged dollar of the Bretton Woods system to the worldwide preoccupation with independent central banks after Bretton Woods dissolved—has always involved imposing constraints on policymakers. In the 1980s, devices such as exchange rate pegs were all the rage in Asia as well as Europe for signaling self-discipline to financial markets. For all their inside status and expertise, neither Yellen nor Draghi gave any public sign of anticipating the crisis that was to come. The same was true for the vast majority of their cohort, whether MIT or Chicago. The scale of the systemic risk posed by the financial system of the advanced economies simply did not register until it was too late. The consistent failure to deliver adequate fiscal policy responses to the crisis after 2008 went against all the preconceptions of 1970s MIT-style macroeconomics. Where were the spendthrift politicians when you needed them? The fiscal undershoot by the Obama administration could perhaps be explained by miscalculation and Republican partisanship. But the fact that a centrist majority in the heart of Europe, faced with dangerous populist challenges from the left and right, would choose to die on the hill of budget balance was not part of the plan. It was up to the ECB to act. In 2015, to the horror of German conservatives, Draghi finally launched a QE program. This was a technical economic measure. But it had spectacular political effects. It enabled the European Council to play hardball with the radical left-wing government in Greece without causing the bond markets to panic. One might say it marked the Americanization of the ECB. Seven years on from the collapse of Lehman Brothers, a majority on the Fed board was swinging toward tightening. The point was not so much that the U.S. economy needed restraining as that they were deeply uncomfortable with interest rates remaining at zero. It stoked speculation in financial markets and gave the Fed nowhere to go if it needed to counter a downturn. Negative interest rates along the lines adopted by Japan were not something that the Fed wanted to contemplate. The basic framework of 1970s macroeconomics that framed Draghi and Yellen’s training and outlook, like that of the rest of their cohort, was that properly structured markets would take care of growth. Well-regulated financial systems were stable. The chief priority for economists was to educate and restrain politicians to ensure that inflation remained in check and public debts were sustainable. Financial instability is a mortal risk. For now, it is being held at bay. But the world saw as recently as March 2020 how rapidly even the largest financial market—the market for U.S. Treasurys—can be destabilized. To tame that risk, the Fed and the ECB, under Yellen’s and Draghi’s non-economist successors—Jerome Powell and Christine Lagarde, respectively—have adopted an astonishingly undogmatic and expansive approach to stabilization. The Italian political class is abdicating in favor of a retired, unelected official in his 70s. Faced with a decisive historical challenge—restarting growth after decades of stagnation—Italy’s political class has chosen to delegate executive power to someone who has never been elected to office. It is the ultimate victory of technocracy but also a do-or-die challenge. The truly strategic challenge facing progressive politics in the United States as in Europe is to find a new model of inclusive and environmentally sustainable economic growth In the 1990s, you didn’t need to be a naive exponent of the post-Cold War end-of-history argument to think that the direction of travel for global politics was clear. The future belonged to globalization and more-or-less regulated markets. The pace was set by the United States. That enabled technocratic governments to be organized around a division between immediate action and long-term payoff. That was the trade-off that Draghi evaluated in his MIT Ph.D. in the 1970s. The drama of Draghi and Yellen’s final act is that for both of them, and not just for personal reasons, the trade-off is no longer so clear-cut. If the short-term politics fail, the long-term game may not be winnable at all. “Whatever it takes” has never meant more than it does today. Tooze (2021) Draghi/Yellen - Can they control what comes next? 46.2 TechFare Bhagat Big Tech has long thrived on regulatory evasion and the exploitation of legal grey areas. In this literature, then, the tendency is to assume that it is an absence of state intervention that has underpinned the technology industry’s growing economic (and political) power. With our conception of techfare, however, we aim to push beyond these explorations of how Big Tech evades state control. Instead of focusing on state absences, we set out to highlight an equally significant dynamic: how the technology industry has become deeply entwined with the activities of the neoliberal state. As is well known, neoliberalism has yielded specific forms of state intervention to discipline and normalize the surplus population and to regulate social insecurity. Filling the void left by the retrenchment of social and welfare spending, these forms include Jamie Peck’s workfare, Susanne Soederberg’s debtfare, and Loïc Wacquant’s prisonfare. As the technology industry has inserted itself more deeply into consumer credit markets and surveillance activities, it has augmented both debtfare (which normalizes and encourages reliance on private sources of credit to augment wages and regulate social insecurity) and prisonfare (which criminalizes poverty through policies that extend the reach of the police, courts, jails, and prisons). And, as the two vignettes below show, it has done so in ways that not only support the ongoing efforts of the neoliberal state, but that also underpin the growth and profitability of Big Tech itself. The penetration of Big Tech into the realm of consumer finance has clear parallels with what Gabor and Brooks (2017) call the fintech-philanthropy-development nexus. Gabor and Brooks argue that fintech has accelerated the financial inclusion of the poor and enhanced financial institutions’ ability ‘to bank the unbanked. Big Tech, too, is adopting these logics of financial inclusion: the technology giants have vast stores of user data and trusting consumer bases that have allowed them to extend financial services globally. For instance, the total alternative credit model—a combination of fintech and lending by Big Tech companies—reached $800 billion in 2019. In Asia, Africa, and Latin America the presence of Big Tech credit grew rapidly, coinciding with the decline of fintech credit volumes due to market regulation in China. Our snapshots surrounding consumer finance and surveillance act as central examples of arenas where techfare augments extant modes of neoliberal regulation in the face of social insecurity. In aligning with debtfare, we are interested in how the vacuum left by welfare retrenchment and the decline of traditional financial actors has paved the way for Big Tech to become a player in consumer finance through new innovations on payday loans, credit cards, and other lending services that explicitly target low-income earners. In relation to prisonfare, we also highlight how Big Tech profits off of surveillance by extending the carceral state to the level of the neighbourhood and the household. Facial recognition is often seen as a public safety tool. But its potential to erode privacy and criminalize vast numbers of people while generating both revenue and data for Big Tech is an important direction for future research. Bhagat (2021) The Techfare State: The ‘New’ Face of Neoliberal State Regulation 46.3 How Asia Works Smith on Studwell I like How Asia Works because it tells a coherent story about how countries get rich. Basically, Studwell says it’s a three-step process: Land reform: Forcibly buy up tenant farms from landlords and give it to the tenants; this increases farm productivity per unit of land area, gives rural people more to do, provides small farmers with some startup capital should they choose to sell their farms and move to town, and pushes landlords themselves to move to cities and use their talents to start more productive businesses. Export discipline: Push companies to export instead of just selling domestically. Cut off support to companies that try to export and fail. This will push companies to increase productivity in order to compete in world markets, especially by learning foreign technology. Financial control: Push banks to support exporters instead of putting their money into real estate bubbles and the like. It’s very difficult to test whether this model really works, or whether the successful development of countries like South Korea and Taiwan was due to something else. We can look at evidence for pieces of the theory — for example, the idea that small farms tend to be more productive than medium-sized ones seems fairly well-supported in the data, and there’s also some evidence that pushing companies to export does cause them to raise their productivity. But Studwell’s model is so complex that it’s hard to test all the pieces together. And if you need all the pieces in place — for example, if export promotion doesn’t work without the “discipline” of winding up failing firms, or if land reform fails if you don’t allow farmers to sell their land, or if export discipline itself doesn’t work without land reform — then testing the pieces individually won’t give us the answers we want. Because it’s so hard to test, the theory serves less as a tried-and-true policy prescription and more as a launching point for ideas about how to manage a developing economy Smith on Krugman, Fujita and Venables’ The Spatial Economy We might start to wonder if successful development policies simply determine countries’ place in a queue. My longtime readers will also know that in addition to How Asia Works, I love Krugman, Fujita, and Venables’ The Spatial Economy. And in the final section of that (highly technical) book, the authors turn what was a humble theory of urbanization into a grand theory of global development. And the upshot of that grand theory is that countries have to basically wait in line to get rich. There’s just no way for them to all hop on the rapid industrialization train all at once. Better policy can let you cut to the front of the line, but then the countries you cut in front of are out of luck. This is a highly stylized, pretty speculative theory, which is even harder to prove than Studwell’s. But it kinda-sorta fits the observed pattern in Asia — first Japan and Hong Kong and Singapore grew quickly, then Taiwan and South Korea, then China, now Vietnam and Indonesia. Malaysia and Thailand got a head start on China but then slowed down after the financial crisis of ‘97, while China accelerated — perhaps because China “cut in line” in front of the Southeast Asian tigers. But now, with China slowing down, perhaps Malaysia is back at the front of the line. Anyway, this would be a depressing, fatalistic sort of world, where development is a zero-sum-game in the short term. Hopefully it’s not true — I’d much rather believe in a Studwellian world where the right smart growth policies can boost lots of countries at once. But we may never know which is right. Noah Smith on joe Studwell Smith on Chang and Studwell We don’t really know how economic development happens, and to put too much faith in the Chang/Studwell story would be unwise. Smith (2021) Jamaica is doing OK Could it do better than OK? 46.4 Social Provisioning of Needs Vogel Abstract Meeting human needs at sustainable levels of energy use is fundamental for avoiding catastrophic climate change and securing the well-being of all people. In the current political-economic regime, no country does so. Here, we assess which socio-economic conditions might enable societies to satisfy human needs at low energy use, to reconcile human well-being with climate mitigation. Using a novel analytical framework alongside a novel multivariate regression-based moderation approach and data for 106 countries, we analyse how the relationship between energy use and six dimensions of human need satisfaction varies with a wide range of socio-economic factors relevant to the provisioning of goods and services (’provisioning factors’). We find that factors such as public service quality, income equality, democracy, and electricity access are associated with higher need satisfaction and lower energy requirements (‘beneficial provisioning factors’). Conversely, extractivism and economic growth beyond moderate levels of affluence are associated with lower need satisfaction and greater energy requirements (‘detrimental provisioning factors’). Our results suggest that improving beneficial provisioning factors and abandoning detrimental ones could enable countries to provide sufficient need satisfaction at much lower, ecologically sustainable levels of energy use. However, as key pillars of the required changes in provisioning run contrary to the dominant political- economic regime, a broader transformation of the economic system may be required to prioritise, and orga­ nise provisioning for, the satisfaction of human needs at low energy use. Vogel Memo Our analytical framework conceptualises the provisioning of human needs satis­ faction in an Ends–Means spectrum. Our framework considers energy use as a means, and need satisfaction as an end, with provisioning factors as intermediaries that moderate the relationship between means and ends. We thus operationalise O’Neill et al.’s (2018) framework by reducing the sphere of biophysical resource use to energy use (for analytical focus), and reducing the sphere of human well-being to human need satisfaction (for analytical coherence). Our operation­ alisation of human need satisfaction follows Doyal and Gough’s (1991) Theory of Human Need, reflecting a eudaimonic understanding of well- being as enabled by the satisfaction of human needs, which can be evaluated based on objective measures. Only 29 countries (28%) in our sample reach sufficient levels in all need satisfaction dimensions assessed here (health, nutrition, drinking water access, safe sanitation, education, minimum income). Each of these need-satisfying countries uses at least double, many even quadruple, the 27 GJ/cap deemed the maximum level of energy use that could be globally rendered sustainable. Our bivariate regression analysis confirms that while energy use is significantly correlated with need satisfaction, high levels of energy use seem neither necessary nor particularly beneficial for need satisfaction. Whereas at low levels of energy use, need satisfaction steeply increases with energy use, need satisfaction improvements with additional energy use quickly diminish at moderate levels of energy use and virtually vanish at high levels of energy use. High energy use alone is not sufficient to meet human needs. At low to moderate levels of energy use, there is a large spread in observed need satisfaction outcomes. which cannot be explained by energy use alone. Need satisfaction outcomes are statistically better explained when a relevant provisioning factor is included as an inter­ mediary that moderates the relationship between need satisfaction and energy use. Across multiple dimensions of human need, the relationship between need satisfaction and energy use varies significantly and sys­ tematically with the configuration of certain provisioning factors. We distinguish three types of provisioning factors. Beneficial provisioning factors are associated with socio- ecologically beneficial performance (higher achievements in, and lower energy requirements of, human need satisfaction). Countries with high values of a beneficial provisioning factor tend to achieve higher levels of need satisfaction at a given level of energy use, and tend to reach a particular level of need satisfaction with lower levels of energy use, compared to countries with median values of the provisioning factor. Detrimental provisioning factors are associated with socio-ecologically detrimental performance (lower achievement in, and greater energy re­ quirements of, human need satisfaction). Countries with high values of a detrimental provisioning factor tend to exhibit lower need satisfaction at a given level of energy use, and tend to reach a particular level of need satisfaction only at higher levels of energy use, compared to countries with median values of the provisioning factor. Lastly, non-significant provisioning factors do not show significant interactions with the rela­ tionship between energy use and need satisfaction. Figure: Most human needs are currently not sufficiently met within sustainable levels of energy use. Cross-country relationships between different need satisfaction variables (y) and total final energy use (x) are shown as black lines, with data shown as grey dots. The green dashed line illustrates the 27 GJ/cap deemed the maximum level of energy use that can globally be rendered sustainable. Thresholds for sufficient need satisfaction are shown by the dotted blue lines. R2_adj is the coefficient of determination, adjusted for the number of predictors. Vogel (2021) Socio-economic conditions for satisfying human needs at low energy use: An international analysis of social provisioning (pdf) "],["economic-measurements.html", "47 Economic Measurements 47.1 Our BESDA economy 47.2 GDP", " 47 Economic Measurements GDP is insufficient as a measure to capture what is essential about the economy. 47.1 Our BESDA economy 47.1.1 GDP and EBITDA While the deficiencies of GDP as a measure have been well known, less emphasized has been the fact that every single financial statement with which we build GDP exhibits the same deficiency of being a limited barometer of value. Ironically, the main users of these financial statements, in the business and financial sectors, are wise to the incompleteness of certain metrics within financial statements, but act in a way that indicates they are oblivious – or perhaps just willing to overlook – the incompleteness of financial statements writ large. To explain, consider that GDP exhibits clear parallels with the profit metric of EBITDA (earnings before interest, taxes, depreciation and amortization), Though there are technical differences of formulation, GDP and EBITDA both represent partial measures of “wealth creation” disembedded from a fuller conception of value. However, while financiers are wise to the deficiencies of EBITDA, they have not acknowledged that the same pattern of incompleteness reappears at the level of the overall financial statement – and then at the yet higher level of GDP. With the “DA”, EBITDA conveys the profitability of a company as if it would never again have to spend a dollar on keeping its factories, equipment, property and software in good repair and up to date. In other words, EBITDA excludes the cost of maintaining in good condition the whole infrastructure upon which a company depends! It is the homeowner’s fantasy of how wealthy they would be if they never had to fix or repair anything in their house ever again. EBITDA came to prominence during the leveraged buyout (LBO) boom of the 1980s. As Moody’s recounted in 2000: “LBO sponsors and bankers have promoted the use of EBITDA for its obvious image benefits. EBITDA creates the appearance of stronger interest coverage and lower financial leverage.” As a general rule, beware profit metrics promising image benefits. Forbes was blunter still: “EBITDA is essentially a tool that shows what a company would look like if it wasn’t actually that company.” EBITDA is now clearly recognized as a “wool-over-your-eyes” measure, such that accounting authorities deny it official status. It is a “non-GAAP” metric – not a Generally Accepted Accounting Principle. Its ongoing ubiquity – besides being trivially easy to calculate – is because it masks the fact that a business may be overleveraged – that it may have borrowed against its future more than it can ever repay. GDP is a “wool-over-all-of-our-eyes” metric for the same reason that it excludes the full cost of maintaining in good condition the social and ecological infrastructure upon which the whole economy depends. In steering society by GDP, we are effectively managing the planet on an EBITDA basis. GDP is not just a benignly incomplete measure of wealth, it is the tool with which we are conning ourselves. Businesspeople – and homeowners - know how these stories end. Eventually the under- investment in infrastructure catches up with you. Of course, by then, you hope to have passed the asset – and the problem – on to someone else. This is feasible, if not best form, where the asset is not the whole planet. The deception works for as long as you can get away with the under-investment and the factories and software hold up. Buffet’s partner, Charlie Munger, is characteristically more forthright on the topic: “I think that, every time you see the phrase ‘EBITDA earnings’, you should substitute the phrase ‘bullshit earnings’.” By analogy, GDP is “bullshit wealth”. That we have been able to enjoy the comforts of its deception without mishap for so long is simply because it was introduced against higher levels of social and ecological infrastructure that we have not yet completely run down. The under-investment is only now becoming apparent. 47.1.2 A BESDA economy Long-term or ESG (environmental, social and governance) investors may protest that they understand all this but that their own investment process insulates them from such blinkered thinking. (“We don’t use EBITDA”). Yet the point is that the whole financial system is operating on a “before ecological and social depreciation and amortization” basis – call it BESDA, perhaps. So, every single financial metric on the Bloomberg screen is a BESDA metric – profits- BESDA, earnings per share-BESDA, return on capital-BESDA, return on equity-BESDA, etc. The millions of financial numbers processed daily by our increasingly automated markets – which, in turn, steer our economy and drag our culture along behind, ripping up nature in its wake – are all BESDA numbers. It might not only be EBITDA with which we are conning ourselves, but every financial number in the book. They all represent different degrees of disembedded value, some of which we have unmasked, some of which we have not. We have a sustainability challenge because the entire financial system repeats the problems of the discredited EBITDA metric at the level of the whole economy. This is the invisible conceptual cage we have wrapped around our decision-making and from within which the ESG movement is frantically trying to make a difference. Alas, given the incompleteness of our markets, the ESG movement increasingly resembles a hopeful grafting of good intentions onto an unchallenged accounting reality that remains the largely intact source of our problems. This is the root cause of our collective “greenwish” in which we are hoping that well-intended efforts to make the world more sustainable are much closer to achieving the necessary change than they really are TRUECOST Trucost, the sustainable consulting firm, estimated in 2013 that large swathes of primary industry – including agriculture and energy companies – would simply not be profitable if they had to pay the full costs of their 14 environmental damage. In 2011, the American Economic Review, published similar work showing that the solid waste combustion, sewage treatment and oil- and coal-fired power production industries generated air pollution damages – air pollution alone – that were greater 15 than their economic value added (EVA). On this fuller accounting perspective, these are effectively EVS – economic value subtracted – industries. Duncan Austin: Pigou and the dropped stitch of economics RWER95 (pdf) 47.2 GDP For measurement to be accurate, the units must be stable. Unlike natural scientists, however, economists are not in the business of carefully defining units using universal physical constants. Economists instead use prices, a social construct, as their unit of analysis. The problem is that prices are unstable units of measurement. Relative prices between commodities vary wildly over time. This instability means that prices fail the only requirement of a good unit — to be uniform over time. Instead of reporting the severe uncertainty in ‘real’ GDP, governments report a single official value. This value hides a myriad of subjective decisions that are used to ‘correct’ for unstable prices. Instead of wasting time with a useless quantity that reveals nothing profound about the world, we should seek new pluralistic methods for understanding aggregate economic activity. Price instability translates into uncertainty in the growth of ‘real GDP’. While the US government reports only one official measure of ‘real’ GDP, it quietly maintains a database of ‘vintage’ GDP estimates. These are estimates calculated with different base years. Using this ‘vintage’ data, we can quantify the uncertainty in the growth of ‘real’ GDP caused by unstable prices. Notice that the official measure of US ‘real’ GDP is at the upper end of the range of uncertainty. We doubt this is a coincidence. In fact, it is common for national governments to boost GDP growth by changing the base year. India recently showed a small increase in GDP growth by choosing a new base year. While this boost was small, it can sometimes be spectacularly large. Nigeria, for instance, recently changed its base year from 1990 to 2010. As a result, real GDP doubled, making Nigeria the largest economy in Africa. Base-year changes have led to similar boosts to GDP growth in Ghana, Kenya, Tanzania, Uganda and Zambia. The NIPA Handbook, the US Bureau of Economic Analysis notes: The fundamental problem confronting the efforts to adjust GDP and other aggregates for inflation is that there is not a single inflation number but rather a wide spectrum of goods and services with prices that are changing relative to one another over time. The index numbers for the individual components can be combined statistically to form an aggregate index, but the method of aggregation that is used affects the movements of the resulting index. Ambigous The growth of ‘real’ GDP is fundamentally uncertain. Or perhaps a better word is ambiguous. Mainstream economists reach a very different conclusion. Their response is to simultaneously admit that calculating ‘real’ GDP requires arbitrary choices, but then to report a single value as though it was the ‘truth’. ChainWeighting The US government currently calculates ‘real’ GDP by adjusting nominal GDP with an aggregate index formed through the multiplication of successive Fisher indexes in adjacent time periods. In popular parlance, this method is called ‘chain-weighting’. Rather than choose a single base year in which to fix prices, chain-weighting uses a technique that resembles a rolling base year. The method is meant to simulate the effect of changing prices and spending patterns over time. This method was adopted in the mid-1990s. The official justification was that structural changes in the US economy, especially rapidly falling computer prices, compelled the government to end the fixed base year method. GDP treats everything with a price as contributing positively to society. Again, this comes down to the assumption that all prices reveal utility. If machine guns sell for the same price as MRI machines, neoclassical theory tells us that both contribute the same utility to society. In response to such absurdities, ecological economists have developed alternative indicators that subtract the value of social ‘bads’ from the value of social ‘goods’. While well-motivated, this approach still assumes that we can aggregate the ‘real’ value of ‘goods’ and ‘bads’. But because prices are unstable, this aggregate is still ill-defined. ‘Real’ GDP is a regressive measure of social progress. Not only is it ill-defined and based on flawed premises, it equates market value with social welfare. This justifies the income of the powerful. Alternatives The key, we believe, is to separate the study of economic distribution from the study of economic scale. The former is the appropriate domain of prices. The latter is best measured using biophysical units. Prices: Distribution We believe it is important to distinguish between economic distribution and economic scale. ‘Real’ GDP assumes that prices can be used to measure economic scale. In contrast, we assume that prices do nothing of the sort. Prices are a tool for distributing resources. The proper place for prices, then, is for understanding economic distribution. Scale: Energy To measure the scale of the economy, we think it is appropriate to focus on energy. Physicist Eric Chaisson argues that energy is the universal currency of science. By measuring economic scale using energy, we put economics in line with the rest of science. And if we are concerned with sustainability, there is no better starting point than to focus on energy use. After all, the profligate use of fossil fuels under a capitalist economy is the primary driver of climate change. Energy has many forms as it is flows through society. One possibility is to focus on primary energy consumption, and see how this relates to changes in social structure. Another possibility is to measure ‘useful work’ — the consumption of end-use energy. Still another possibility is to measure the aggregate flow rate, which is a measure of all annual energy conversions in an economic system. The study of economic growth, which should focus on biophysical flows. Continuing to use ‘real’ GDP as a measure of social progress implicitly accepts a theory (neoclassical economics) that has long been used as an ideological justification for capitalist power. Fix GDP 47.2.1 GDP Revisions Assa Abstract What are the implications of changes in measurement standards of GDP for global convergence debates? What are the political economy implications? To answer the former question, we examine the changes in national accounting standards from the early 1990s. Revisions to the System of National Accounts (SNA) – the international standard for constructing GDP – include several major changes to how production is measured, including the reclassification of financial intermediation services, R&amp;D, and weapons systems as productive activities – all areas in which countries in the West has had an advantage in recent decades. In addition, there has been an increase in the proportion of imputations in the 1993 and 2008 revisions, which privileges the economic structures of the West. Overall, we find that these changes have had the effect of boosting the GDP of the West relative to the rest of the world and thus to an underestimation of global convergence compared to previous measures of GDP. To answer the second question, the paper unpacks the political economy implications of national accounting standards favouring Western economies along several axes, including the impacts on voting shares in international institutions, domestic policy incentives and epistemological debates about sustainable development. Assa and Kvangraven (2021) Measurement for Convergence Debates and the Political Economy of Development Assa Abstract Over the last half century, a large literature has developed on both the nature and the drivers of uneven development. While different methodologies and theoretical approaches to the issue of convergence abound, the use of GDP growth as a measure of economic growth has, remarkably, gone unquestioned. This paper reviews the convergence debates to date, and examines what the changes to the System of National Accounts (SNA) - the international standard for constructing macroeconomic indicators such as GDP - imply for assessing economic convergence. The 1993 and 2008 revisions to the SNA include several major changes to how production is measured - including the reclassification of financial intermediation services, R&amp;D, weapons systems and owner-occupied dwellings as productive activities - all areas in which developed countries have had an advantage in recent decades. We argue that these changes to the production boundary constitute a form of ‘kicking away the ladder,’ i.e. redefining the yardstick of development to fit the new strengths of developed economies. We analyze data series for a range of countries concurrently available under the 1968 SNA, 1993 SNA and 2008 SNA standards. The earlier measure shows a larger and faster convergence of most countries in ‘the Rest’ with those of ‘the West’. Going a step further, we build on Basu and Foley’s (2013) Measured Value-Added concept as a proxy of ‘Core GDP’. This indicator omits any sector for which value-added is imputed based on net incomes, in the absence of an independent measure of output. This allows us to examine more countries and a longer, more consistent time series than concurrent SNA data, but the conclusions are the same - developing countries have caught up more in Core-GDP terms than the contemporary imputation-heavy measure of GDP would suggest. These findings suggest that the current measure of GDP has become decoupled from core employment-generating activities, and is therefore a misleading measure of growth in an economy. Furthermore, it is inconsistent with the understanding the Sustainable Development Goals of inclusive and sustainable growth. Finally, the paper considers the political economy implications of the changes in GDP methodology, such as the justification of voting shares in international financial institutions, epistemological consequences, and domestic political economy considerations. Assa and Kvangraven (2018) Imputing Away the Ladder: Implications of Changes in National Accounting Standards for Assessing Inter-country Inequalities (pdf) Kvangraven Economic growth was first measured by governments in the 17th century. In the modern era, the United Nations took over responsibility for measuring output in 1953, and was joined in 1993 by the World Bank, IMF, OECD and EU. They all feed into decisions about the international measurement rules, which are taken by the UN inter-secretariat working group on national accounts (ISWGNA), and all countries are meant to comply. This reflects a gradual move away from national governments controlling the statistics to financial institutions having a larger say. Both in 1993 and again in 2008, the so-called “production boundary”, which determines what is included in GDP, was broadened by the ISWGNA to include many activities that were hitherto excluded or at most seen as intermediate inputs. Thanks to these reforms, financial intermediation, research and development, and the production of weapons all began to be counted within GDP data across the world. For example, in 1993 the income banks earned on interest from lending to households was included in GDP for the first time. And then in 2008, even bank money that had nothing to do with intermediation services began to be included. Since western countries such as the UK and US have specialised in these activities in recent decades – the US is first in weapons and second in financial services and R&amp;D, while the UK leads on financial services – the changes have disproportionately benefited their GDP numbers. Kvangraven in The Conversation 47.2.2 GDP Alternatives Coyle How should we measure economic success? Criticisms of conventional indicators, particularly gross domestic product, have abounded for years, if not decades. Environmentalists have long pointed out that GDP omits the depletion of natural assets, as well as negative externalities such as global warming. And its failure to capture unpaid but undoubtedly valuable work in the home is another glaring omission. But better alternatives may soon be at hand. In 2009, a commission led by Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi spurred efforts to find alternative ways to gauge economic progress by recommending a “dashboard” of indicators. Since then, economists and statisticians, working alongside natural scientists, have put considerable effort into developing rigorous wealth-based prosperity metrics, particularly concerning natural assets. The core idea is to create a comprehensive national balance sheet to demonstrate that economic progress today is illusory when it comes at the expense of future living standards. In an important milestone in March of this year, the United Nations approved a statistical standard relating to the services that nature provides to the economy. That followed the UK Treasury’s publication of a review by the University of Cambridge’s Partha Dasgupta setting out how to integrate nature in general, and biodiversity in particular, into economic analysis. With the consequences of climate change starting to become all too apparent, any meaningful concept of economic success in the future will surely include sustainability. The next steps in this statistical endeavor will be to incorporate measures of social capital, reflecting the ability of communities or countries to act collectively, and to extend measurement of the household sector. The COVID-19 pandemic has highlighted how crucial this unpaid work is to a country’s economic health. For example, the US Bureau of Labor Statistics intends to develop a more comprehensive concept of living standards that includes the value of such activity. But many advocate thinking about economic success and failure in terms of well-being, a broader and fuzzier concept. The idea that policy decisions should focus on what ultimately matters in people’s lives is intuitively appealing. And a number of governments, from New Zealand to Scotland, have recently adopted explicit well-being policy frameworks. Public policy based on well-being thus still lacks a theoretical underpinning. One recent UK study, co-produced by researchers and people experiencing poverty, found that while basic material needs including health were important to well-being, autonomy and a sense of purpose mattered just as much. The top-down aggregate indicators devised by social scientists and statisticians cannot capture such findings. Keep in mind that the concept of well-being is much richer than most other economic indicators. Importantly, the comprehensive wealth and well-being approaches outlined here are complementary: the assets measured by the former provide the means to achieve the latter. Indeed, New Zealand’s policy framework makes this link explicit. What is exciting about these alternative approaches to assessing and measuring the economic success of a community or country is the amount of practical progress already made in defining concepts, creating metrics, and building expert consensus about the direction policymaking should take. Ditching GDP as the main gauge of prosperity was always impossible in the absence of broad agreement about what the alternative might be. And it will take many more years of work at the statistical coalface to develop a framework as sophisticated and well-embedded as GDP and related economic indicators. But the direction of change is clear, and the impetus to bring it about is powerful. Coyle (2021) GDP’s Days Are Numbered "],["economic-modelling.html", "48 Economic Modelling 48.1 Model-land 48.2 End of Theory 48.3 Jackson-Victor 48.4 Eurogreen Model 48.5 HARMONEY", " 48 Economic Modelling It takes a model to beat a model. 48.1 Model-land Thompson Abstract Both mathematical modelling and simulation methods in general have contributed greatly to understanding, insight and forecasting in many fields including macroeconomics. Nevertheless, we must remain careful to distinguish model-land and model-land quantities from the real world. Decisions taken in the real world are more robust when informed by estimation of real-world quantities with transparent uncertainty quantification, than when based on “optimal” model-land quantities obtained from simulations of imperfect models optimized, perhaps optimal, in model-land. The authors present a short guide to some of the temptations and pitfalls of model-land, some directions towards the exit, and two ways to escape. Their aim is to improve decision support by providing relevant, adequate information regarding the real-world target of interest, or making it clear why today’s model models are not up to that task for the particular target of interest. Thompson (2019) Escape from Model-land (pdf) 48.2 End of Theory Bookstaber The End of Theory: Financial Crises, the Failure of Economics, and the Sweep of Human Interaction Our economy may have recovered from the Great Recession—but not our economics. In The End of Theory, Richard Bookstaber discusses why the human condition and the radical uncertainty of our world renders the standard economic model—and the theory behind it—useless for dealing with financial crises. What model should replace it? None. At least not any version we’ve been using for the past two hundred years. Instead, Bookstaber argues for a new approach called agent-based economics, one that takes as a starting point the fact that we are humans, not the optimizing automatons that standard economics assumes we are. Bookstaber’s groundbreaking paradigm promises to do a far better job at preventing crises and managing those that break out. As he explains, our varied memories and imaginations color our economic behavior in unexpected hues. Agent-based modeling embraces these nuances by avoiding the mechanistic, unrealistic structure of our current economic approach. Bookstaber tackles issues such as radical uncertainty, when circumstances take place beyond our anticipation, and emergence, when innocent, everyday interactions combine to create sudden chaos. Starting with the realization that future crises cannot be predicted by the past, he proposes an approach that recognizes the human narrative while addressing market realities. Sweeping aside the historic failure of twentieth-century economics, The End of Theory offers a novel and innovative perspective, along with a more realistic and human framework, to help prevent today’s financial system from blowing up again. Bookstaber (Book Page) 48.3 Jackson-Victor The “new normal”: Hyper-Capitalism, Proto-Socialism, and Post-Pandemic Recovery Jackson Abstract Post-pandemic recovery must address the systemic inequality that has been revealed by the coronavirus crisis. The roots of this inequality predate the pandemic and even the global financial crisis. They lie rather in the uneasy relationship between labor and capital under conditions of declining economic growth, such as those who have pre- vailed in advanced economies for almost half a century. This paper explores the dynam- ics of that relationship using a simple stock-flow consistent (SFC) macroeconomic model of a closed economy. It examines in particular the role of two key factors—the savings rate and the substitutability (elasticity of substitution) between labor and capital—on the severity of systemic inequality under conditions of declining growth. The paper goes on to test the efficacy of three redistributive measures—a graduated income tax, a tax on capital and a universal basic income—under two distinct structural scenarios for an economy with a declining growth rate. We find that none of these measures is sufficient to control structural inequality when institutions aggressively favor capital over labor (hyper-capitalism). Taken in combination, however, under con- ditions more favorable to wage labor (proto-socialism), these same measures have the potential to eliminate inequality, almost entirely, even as the growth rate declines. Jackson Memo The two key structural factors, which determine the evolution of inequality under a declining growth rate, are (1) the savings rate and (2) the elasticity of substitution between labor and capital. Depending on the configuration of these factors, two radically different futures may emerge. Under one future, which we have described here as “hyper-capitalism” (Scenario 1), a constant savings rate and high sub- stitutability between capital and labor lead to accelerating inequality, even under a progressive combination of redistributive measures. Under another kind of future, which we describe as proto-socialism (Scenario 2), a declining savings rate and low substitutability between capital and labor, lead to declining inequality, which in combination with progressive redistributive policies, have the potential to eliminate inequality almost completely. Hyper-capitalism is likely to emerge in a world where labor is increasingly (and easily) substituted with capital and the interests of the owners of capital are privileged over the rights of workers. These privileges encourage capitalists to continue to save even as the growth rate declines, leading to a rising capital to output ratio and an escalating inequality. Such a scenario could, for example, accompany a world in which an aggressive drive towards automation or the imple- mentation of artificial intelligence (AI) by monopolistic companies removes the need for wage labor across large swathes of the econ- omy. Failure to protect the livelihoods of the immiserated work force facilitates continued savings and investment by asset owners. By the same token, it concentrates incomes (and wealth) increasingly in a minority of the population, leading to the kinds of dystopian trends in inequality illustrated in Scenario 1. 13 Proto-socialism on the other hand aims for strong institutions to protect the rights of workers, introduce a job guarantee, and establish an adequate minimum wage. Such interventions slow down the sub- stitution of capital for labor. Attempts by capitalists to maintain a con- stant savings rate under these conditions lead (Figure 3a) to a dramatic collapse in the rate of return on investment, and a partial reversal in the relative fortunes of workers and capitalists. Faced with the prospect of declining rates of return, these conditions are more likely to lead to a decline in the rate of savings (Scenario 2) and a reduction in the capital intensity of the economy, features that will reinforce a more equal distribution of incomes. In short, proto-socialism is likely to involve a transition away from resource-intensive mass production processes and toward the evolu- tion of an economy of quality and service (Jackson, 2017). It might well also involve institutional innovations which better represent the interests of workers in the management of firms (Ferrera, 2017), bet- ter distribute the rewards of innovation to the populace (Varoufakis, 2016) and allow government to operate as an “employer of last resort” (Minsky, 1986). It will not have passed unnoticed that the sectors that emerge stronger under proto-socialism are precisely the labor-intensive sectors associated with care, distribution and maintenance—the frontline ser- vices of the pandemic—described at the beginning of this paper. Other labor-intensive sectors such as those associated with crafts, creativity, and community-based recreation and leisure (Jackson, 2021) are also likely to flourish under these conditions. Proto-socialism, in other words, could provide a robust basis for a post-pandemic recovery— even under conditions of low-growth. Jackson (2021) Confronting inequality in the “new normal” : Hyper-capitalism, proto-socialism, and post-pandemic recover (pdf) Thanks to (???) and Peter Victor. This paper is crucial to challenging the assumption, represented in the IPCC’s existing scenarios, that slower growth rates mean rising inequality. It all depends on policy, and the power of labour vis-à-vis capital. (Jason Hickel) 48.4 Eurogreen Model Feasible alternatives to green growth Abstract D’Alessandro Climate change and increasing income inequality have emerged as twin threats to contemporary standards of living, peace and democracy. These two problems are usually tackled separately in the policy agenda. A new breed of radical proposals have been advanced to manage a fair low-carbon transition. In this spirit, we develop a dynamic macrosimulation model to investigate the long-term effects of three scenarios: green growth, policies for social equity, and degrowth. The green growth scenario, based on technological progress and environmental policies, achieves a significant reduction in greenhouse gas emissions at the cost of increasing income inequality and unemployment. The policies for social equity scenario adds direct labour market interventions that result in an environmental performance similar to green growth while improving social conditions at the cost of increasing public deficit. The degrowth scenario further adds a reduction in consumption and exports, and achieves a greater reduction in emissions and inequality with higher public deficit, despite the introduction of a wealth tax. We argue that new radical social policies can combine social prosperity and low-carbon emissions and are economically and politically feasible. D’Alessandro (2020) Feasible alternatives to green growth (Paywall) SI (pdf) D’Alessandro Presentation Green Growth The main response to the global challenges posed by climate change are currently based on Green Growth policy proposals, namely: • mainstream and institutional paradigm focused on technological optimism; • market-oriented view: trickle-down effect should improve welfare and job creation; • one-size-fits-all solution: GDP growth Critiques to the ability of market mechanisms and innovations to: - foster material decoupling (Wiedmann, 2015) - meet planetary boundaries (Steffen, 2015, O’Neill, 2018) - avoid critical transitions (Scheffer, 2012) - ensure social justice: within-country inequality (Piketty, 2014) - overcoming the rebound effect: % RES and CO2 per capita Green Deal Recognizes the need to address inequality and environmental issues in a unified perspective combining social policies with green growth measures Post-Growth Advocates that continuous economic growth and ecological sustainability are incompatible: down-shift of economic scale. Social policies becomes essential to face inequality EUROGREEN A macrosimulation model tailored to compare the long-run effects, synergies and trade-off of these three alternative narratives. Indicators GHG emissions with respect to 1990. Targets: −40% in 2030 and −80% in 2050 • Gini coefficient for income inequality: from 0% (no ineq.) to 100% (max ineq.). Computed over 13 groups (3 skill by 4 work status + capitalists) including incomes from labour, financial assets and wealth • Deficit/GDP: fiscal sustainability • GDP growth • Unemployment: total and by skill • Energy Mix: shift in source composition in electric power generation and TPES. Discussion • Our results suggest that there are no win-win solutions • Similar reductions in emissions can result in radically different social consequences in terms of income distribution, employment, and fiscal stability. • Green Growth Paradox ⟹ The effectiveness of GHG reductions depends on the failure to promote GDP growth. • Techno-scepticism: Environmental policies alone fails to deliver the advocated improvements in employment and income distribution • Radical social policies (JG and WTR) can combine social prosperity and low-carbon emissions • Lower aggregate demand helps emission target achievement D’Alessandro (2020) feasible Alternatives - Presentation (pdf) O’Neill ** ‘Green Growth’ will increase inequality and unemployment unless accompanied by radical social policies.** The economy is embedded within society, which is in turn embedded within the biosphere. Economic processes are therefore analysed in terms of flows of biophysical resources and social outcomes not just in terms of flows of money, as in conventional macroeconomic models. Ecological macroeconomic models allow for multiple non-substitutable goals to be explored (e.g. sustainability, equity, and human well-being). These models have been developed to address issues such as the link between growth and inequality 5 and the effect of climate change on financial stability. Green growth reduces greenhouse gas emissions, but inequality and unemployment both rise. The Green New Deal dramatically lowers unemployment and reduces inequality, but at the expense of an increase in the government deficit-to-GDP ratio. Degrowth reduces emissions and inequality further than the other two scenarios, but it leads to a higher increase in the deficit-to-GDP ratio (because GDP decreases). In short, there is no win win scenario. These results have important implications. First, they suggest that a purely market-based green growth strategy is likely to have serious negative side effects. These side effects may be corrected by complementing environmental policies with strong social policies, such as working-time reduction, a guaranteed jobs programme, and a wealth tax. Second, the results suggest that degrowth can dramatically reduce environmental impact and lead to improved social outcomes (e.g. more leisure time, higher employment, greater equality), provided the appropriate policies are in place. Third, a Green New Deal, with an explicit focus on achieving a just transition 7 , may represent a compromise that advocates of both green growth and degrowth can support. The Eurogreen Model makes a number of important contributions, but like any model it also has limitations. Importantly, the model does not assess whether the degree of decoupling assumed in its green growth scenario is actually possible, an assumption that has been challenged empirically. The degrowth scenario does not include a number of additional changes that have been put forward by degrowth authors, such as alternative business models, new measures of progress, or public money creation 9 . For example, central banks could potentially create money to help fund a low-carbon transition (as they created money to bail out the banks), which would reduce the government deficit. We need to choose our economic policies carefully. We cannot expect economic growth to deliver sustainability, or green growth to deliver social equity. If we want to achieve a sustainable and just society, then we need to move beyond the pursuit of growth, and target these outcomes directly. ONeill (2020) Beyond Green Growth (pdf) Russel (2020) Climate crisis: Is it time to ditch economic growth? (DW) Mudge (2020) Fact check: Does climate protection stifle economic growth? 48.5 HARMONEY King Abstract This paper explains how the Human and Resources with MONEY (HARMONEY) economic growth model exhibits realistic dynamic interdependencies relating resources consumption, growth, and structural change. We explore dynamics of three major structural metrics of an economy. First, we show that an economic transition to relative decoupling of gross domestic product (GDP) from resource consumption is an expected pattern that occurs because of physical limits to growth, not a response to avoid physical limits. While increasing operational resource efficiency does increase the level of relative decou- pling, so does a change in pricing from one based on full costs to one based only on marginal costs that neglect depreciation and interest payments. Marginal cost pricing leads to higher debt ratios and a perception of higher levels of relative resource decoupling. Second, if assuming full labor bargaining power for wages, when a previously-growing economy reaches peak resource extraction and GDP, wages remain high but profits and debt decline to zero. By removing bargaining power, profits can remain positive at the expense of declining wages. Third, the internal structure of HARMONEY evolves in the same way the post-World War II U.S. economy. This is measured as the distribution of intermediate transactions within the input- output tables of both the model and U.S. economy. King Memo HARMONEY v1.1 is a system dynamics model centered on simulating a set of ordinary differential equations using stock-flow consistent tracking of monetary flows. HARMONEY v1.1 is still a toy model, which is to say it is not yet calibrated (we’re working on it!) to a real economy, such as the United States. Nonetheless, it has critical features and structural assumptions that make it applicable and valuable for comparing its trends to long-term trends in real-world data. This is to say, an important part of HARMONEY is that it has a conservation of flow principle for both mass (as physical resources, energy or minerals, extracted from the environment) and money (at any given instant flows of money are tracked between firms, households, and private banks). While this idea has been around for many decades, this is still relatively unique for macroeconomic models. Here are several assumptions in the design of the model that help explain why it can mimic long-term real-world trends relating energy consumption and economic variables The resource that supports the economy is a regenerative renewable resource stock, such as a forest. Resource (mass, energy) consumption is required for three purposes in the model, just like the real world: To operate machines (as fuel) To become new machines when they are manufactured (embodied in new capital) To “operate” or feed people to keep them alive (as food) Money is effectively defined as all of the following the compensation labor (workers) receive, the profits received by companies, money (as credit) is created when banks give loans to companies to invest in capital at levels beyond their profits, and the money is destroyed when companies pay back debt, and the interest payments on the debt, or loans given to companies. There is no government in the model. Population declines when there is not enough resource consumption for households. The HARMONEY model overcomes three neoclassical limitations: the inadequate incorporation of natural resource consumption as required physical inputs to operate capital,become embodied in new capital investment, and keep people alive; the lack of consideration of credit, or private debt, in a modern economy; and the assumption that factors of production contribute to growth in relation to their cost share. Unlike neoclassical growth theory (exogenous or endogenous), the post-Keynesian and biophysical structure of the HARMONEY model does not assume an aggregate production function, TFP, or directly impose scaling of GDP to aggregate labor, capital, or natural resources consumption. Thus, the model enables a different exploration into the effects of resource efficiency and whether the economy has similar energy-GDP scaling as biological systems, and for the same reasons, throughout a growth cycle. Global primary energy consumption (PEC) and gross world product (GWP) scale approximately linearly from 1900-1970, and since 1970 scale sublinearly at \\(PEC ∝ GWP^{2/3}\\) Post-1980 trends show PEC of countries scales with their GDP nearly as \\(PEC ∝ GDP^{3/4}\\) … explicitly considers the “energy cost of maintaining the structure and function” of an economy as a complex system …does not address the exact scaling (i.e., value of b) between energy consumption and GDP, but it explains why we expect a transition from superlinear or linear scaling to sublinear scaling, just as observed in biological systems. …also contributes to the discussion of decoupling of GDP from PEC via increases in energy effi- ciency. Sublinear scaling in the economy, often referred to as a state of declining energy intensity (= PEC/GDP), is often seen as a consequence of increasing energy efficiency. …economy-wide rebound effects might erode more than half the reductions in engineering energy efficiency investments. King Conclusion The purpose of this paper was to explore the coupled growth and structural dynamic patterns of the HARMONEY model (v1.1) as updated from King (2020). The differences in the simulation results in this paper versus King (2020) derive from the more robust method in solving for prices and the explicit inclusion of wage bargaining power that augments a short-run Phillips Curve. Despite the assumption of a single regenerative natural resource (akin to a forest) to support the modeled economy, HARMONEY v1.1 exhibits several important high-level structural, biophysical, and economic patterns that compare well with global and U.S. data, and thus provide insight into long-term trends. The HARMONEY model provides a consistent biophysi- cal and monetary basis for explaining the progression in global and country-level data from an increasing or near constant energy intensity (energy consumption/GDP) to one of decreasing energy intensity. That is to say, both HAR- MONEY and global data first show a period of increasing growth rates, when the growth rate of natural resource con- sumption exceeds or is nearly equal to the growth rate of GDP, followed by a period of decreasing growth rates when the growth rate of resource consumption is lower than that of GDP. Thus, given this latter condition referred to as a state of relative decoupling, we conclude that it occurs due to a natural progression of self-organized growth, and not necessarily from independent conscious choice by actors within the economy to pursue resource efficiency. While we show that explicit choices to increase resource consumption efficiency in capital (e.g., machines) do increase the level of relative decoupling, we also show the choice of price formation affects apparent decoupling just as much. When basing prices on only marginal costs the econ- omy appears more decoupled than if prices are based on full costs that include depreciation and debt interest payments. Further, marginal cost pricing generates higher debt ratios than full cost pricing, implying higher debt levels might pro- vide only a perception of a more decoupled economy. Thus, relative decoupling of GDP from resource consumption rep- resents an expected stage of growth, still similarly dependent on resource consumption, rather than a stage during which an economy is less constrained by resource consumption. When assuming full labor bargaining power for wages, such that wages increase with inflation, once resource con- sumption stagnates, profit shares decline to zero and wage share increases. An explicit reduction in labor bargaining power at peak resource consumption enables some profits to remain. Thus, the HARMONEY model provides a basis for arguing that because profits decline to zero once resource consumption peaks under a full bargaining power situation, a new pressure emerges to reduce wage bargaining power of labor to ensure some level of profits at the expense of labor. This reasoning helps explain the wage stagnation and declin- ing wage share experienced in the U.S. since the 1970s. King (2021) Interdependence of Growth, Structure, Size and Resource Consumption During an Economic Growth Cycle (pdf) (pdf SI) King Summary (blog) King (2019) HARMONEY-1 (pdf) King Website Fix on King Figure shows King’s key result. Without tuning it to do so, the HARMONEY model predicts that as resource use plateaus, the wage share of income should decline (top right). It so happens that this is exactly what ocurred in the United States. As energy use (per person) plateaued, the wage share of income plummeted (top left). HARMONEY also predicts that after resource use peaks, debt (as a share of GDP) should explode and then later peak (bottom right). Again, the model’s prediction is eerily similar to US history (bottom left). Figure: Results from King’s HARMONEY model. Top left: the wage share of income in the US declined as energy use per person plateaued. Top right: King’s HARMONEY model predicting the same phenomenon. Bottom left: The growth and peak of US corporate and financial debt. Bottom right: King’s HARMONEY model predicting the same phenomenon. One more thing to mention is that HARMONEY does not use an aggregate production function. This is important, because there are many problems with such functions. Perhaps the most glaring flaw is that the standard production function (the Cobb-Douglas) is a tautology. It is a rearrangement of a national accounting identity. Hence, when systems modelers use such a function, they undermine what may otherwise be a sound model.3 By not using a production function, HARMONEY avoids this misstep. Fix on King ‘Superorganism’ "],["agent-based-macro.html", "49 Agent Based Macro", " 49 Agent Based Macro Poledna Abstract We develop the first agent-based model (ABM) that can compete with benchmark VAR and DSGE models in out-of-sample forecasting of macro variables. Our ABM for a small open economy uses micro and macro data from national and sector accounts, input-output tables, government statistics, census and business demography data. The model incorporates all economic activities as classified by the European System of Accounts as heterogeneous agents. The detailed structure of the ABM allows for a breakdown into sector level forecasts. Potential applications of the model include stress-testing and predicting the effects of changes in monetary, fiscal, or other macroeconomic policies. Poledna (2021) Economic Forecasting with ABM (pdf) "],["cost-benefit-analysis.html", "50 Cost Benefit Analysis 50.1 Discounting", " 50 Cost Benefit Analysis Spash on Dasgupta Review Decades ago environmental CBA developed a range of methods for imputing monetary values, but with limited applicability under specific conditions (Hanley &amp; Spash, 1993; Spash, 2005). For a start, these methods only apply to marginal changes in environmental goods or services, not least because the value of money itself (its marginal utility) alters when there are large changes affecting income; also, economic welfare measures assume other things (e.g. all other prices) remain the same which is violated by large changes. Clearly things like mass extinction of species and human induced climate change are not small, marginal, changes. Two approaches are employed by social CBA: revealed and stated preference methods. The first relies upon existing markets that can be associated with environmental attributes (e.g. air pollution affecting house prices) and so is severely restricted. The second uses surveys designed to illicit, primarily, willingness-to-pay for environmental changes. Dasgupta attends to one stated preference approach: the contingent valuation method (CVM). He claims that: ‘CVM is attractive because it appeals to our democratic instinct, that people should be asked for their opinion on matters that may be of concern to them’ (Dasgupta, 2021, p. 304). In addition, the CVM is promoted as widely applicable (unbounded by existing markets) to revealing values for everything from aesthetics to biodiversity loss. The ability to include a range of value categories contributing to an individual’s utility extends to including: ‘respondents’ sense of a species’ existence value—perhaps even its intrinsic value’ (Dasgupta, 2021, p. 304). Normally environmental economists define a set of four values: direct use, option, existence and bequest value. However, Dasgupta (Dasgupta, 2021, p. 301), for no apparent reason, claims a different set of six sources of value for biodiversity that mix-up objects of value with types of values. While all his examples are consequentialist and based on creating utility for humans, he confuses concepts of existence value with sacred values, moral worth and intrinsic value. So let us turn to the value problems. Economic welfare theory requires that people should be compensated, not pay, for an environmentally degrading imposition on them (e.g. pollution, biodiversity loss). However, this is generally not undertaken because people could ask large sums, destroying the economic calculus, so economists prefer to restrict respondents replies to their income (i.e. ability to pay) regardless of their own theoretical requirements for validity. Besides being the incorrect measure, willingness-to-pay is not a democratic approach seeking an opinion, as claimed by Dasgupta. Despite their attempts to control respondents, CVM produces results deemed unacceptable because people appear willing-to-pay too much or refuse to bid (i.e. protest). Attempts have then been made to redesign the surveys to get the responses economists want and hence they developed choice experiments, cited as a problem solving advance by Dasgupta (2021, p. 304). Here respondents have restricted ability to protest, or violate the economists model of how they should behave, and can only refuse to answer completely and fall into the ignored nonrespondent category. Failing ‘success’ with survey design, collected data may be subject to manipulation to get the desired values. The welfare economics underlying social CBA assumes that all values can be reduced to individual preferences as expressions of utility. Refusing to make trade-offs is also a principled positon disallowed by mainstream economics or treated as an anomaly. The valuation question of social CBA, ‘what is your maximum willingness-to-pay for more/less X?’, implicitly assumes there is no moral objection to the question itself. More frogs do not equate to fewer tigers. What then is ‘natural capital’? Health (mortality/morbidity) as a capital investment is even worse. Producing money numbers here requires the conjuring trick of talking about abstracted non-real people who are represented as ‘statistical lives’, under the VSL. For example, the results are used in transportation assessment to decide upon road building programmes and the installation of safety equipment. However, the public rejection of this approach is exposed when there is a train crash, people are killed and the public discover the lack of safety equipment is due to the calculation that it cost more than the expected fatalities times the VSL. Politicians rarely defend the numbers in such circumstances, although their transport departments may continue to use them on a daily basis. IPCC A major example of the failings of VSL arose during the third assessment report of the Intergovernmental Panel on Climate Change (IPCC). Willingness-to-pay informed VSL, based on Fankhauser (1995, p. 47), gave a range from $0.2–$16.0 million with an average of $3 million, and $1.5 million adopted as the VSL for developed countries. Adjustment was made for income to give ‘an arbitrary value of $300,000 for middle income and $100,000 for low income countries’. The result was a factor of fifteen difference between VSL in high ($1.5 million) and low ($0.1 million) income countries. A storm raged when the IPCC chapter employing this approach appeared (see Spash, 2002a, Chapter 7). Representatives from industrially developing nations, led by India and China, refused to accept the report citing it as absurd, discriminatory, unethical, technically inaccurate and anti the poor. Shortly after the IPCC VSL controversy a prime example of commensurability problems arose when CBA was applied to climate change by Nordhaus (1998a, 1998b). He claimed increased moridity/mortality would be outweighed due to increased leisure opportunities by a factor of 30 to 10 in China and by 38 to 3 in the USA. An example Nordhaus was using at the time concerned claiming that golfers may view global warming as a boon to year-round recreation. So, if we extend this logic to global studies, more golfing days in Florida could compensate for dead people in China. The commensuration of values in The Review is no different. Classes of capital are values, equated and summed. Human capital is an aggregation of values so that, for example, more ‘education’ can compensate for increased risk of death. More than this, if education pays better financial dividends than avoiding loss of life then, according to the economic accountants, the optimal world should have more education and more death. 50.1 Discounting The mechanism chosen by Dasgupta to allocate natural resources between current and future generations is a social discount rate (SDR). Alternatives, such as allocation on grounds of justice, rights or needs, are therefore excluded, While acknowledging the major ethical objections to discriminating against future generations via discounting, Dasgupta (2021, Chapter 10) nevertheless chooses a positive discount rate. He justifies this using the dubious argument that since returns on judiciously chosen investments are positive (by assumption), fairness requires discriminating against future generations, because otherwise the current generation would be unduly limited in consumption and condemned to excessive poverty. This is a productivist logic based on assuming the future is always better-off. Dasgupta follows standard neoclassical theory in treating future outcomes (flows of costs and benefits) for public policy projects as subject to a social time preference (STP). The basic positon here is to follow a formulae, called the Ramsey rule, that determines the SDR and STP as follows: \\[SDR = r + h g = STP\\] where g is annual per capita growth of consumption, η is the elasticity of marginal utility of consumption and ρ is the utility discount rate, consisting of a component for pure time preference, δ, and, in HM Treasury practice, a component for certain types of risk, L. Components of the formulae are so uncertain that economists appeal to surveying themselves to get estimates, as if this provided objective data. In The Review commitment to an actual number is vague, and subject to speculation as to economic growth and uncertainty. Elsewhere Dasgupta (2008) has argued that δ could be zero, while, contrary to others, he argues for a much higher η in the range 2–3 or more. The basic rate used by HM Treasury, in its Green Book, is 3.5%, 6 where δ = 0.5, L = 1 so that ρ = 1.5 with the remainder consisting of consumption growth g = 2 and η = 1. If Dasgupta’s argument for η is adopted then the STP would be between 5.5% and 7.5%, but he has argued favouring δ = 0 which would give 5.0–7.0%. This is extremely high. For comparison consider how Nordhaus uses such rates to recommend catastrophic global warming as economically rational. He states that ‘the cost-benefit optimum rises to over 3°C in 2100’ (Nordhaus, 2018, p. 452), and his Figure 5 shows a 2100 optimum around 3.6°C and rising, because he recommends discounting the future at around 5% (Nordhaus, 2018, p. 455), writing off any importance of future damages by 2100 (damages would weigh around 2% of their value today meaning, for example, under VSL an action saving 2 people today at the cost of killing 97 people in 2100 would be a net gain, an optimal choice). This is neither unique to Nordhaus nor new, for example, the economic working group of the IPCC third assessment used discount rates between 5% and 12%. A common claim, also made by Nordhaus, is that empirically observable rates of return should be used. However, in actual economies the rate of return on risk free investment has been zero or negative in real terms for years (Freeman et al., 2018, p. 16), but discounting has persisted regardless of the theoretical justifications. In fact there is no such thing as a singular rate in actual economies. The recognition that differential rates for different projects is theoretically justified, and specifically for projects with environmental impact, has led HM Treasury to discount at a lower rate (1.5%) for project impacts on health and life. However, while recognized as formally correct Dasgupta rejects this on the grounds, not of theory but, that it will be ‘cumbersome’ in practice and ‘lead inevitably to errors’. Indeed he states it would be ‘unsafe’ because (now) the ‘social evaluator’ (or ‘citizen investor’) cannot be trusted to get things right. Instead he recommends a single rate applied to all projects. The reader might wonder at such pragmatism, for if all the problems of neoclassical economics can be so easily dismissed on a whim as impractical and too cumbersome for his social/citizen evaluator/investor this rather begs the question why he bothers us with his models, theories and extensive mathematical detours and what other fallibilities his central decisionmaker might suffer from. That markets are not guides to intergenerational fairness, ethics or equity, would seem to bring the whole approach into question. In his paper on discounting for climate change Dasgupta concludes: ‘Intergenerational welfare economics raises more questions than it is able to answer satisfactorily’. However, as usual, Dasgupta’s recognition of the problems has no impact on his esteem for and continued use of neoclassical economics and, unsurprisingly, he recommends discounting on this basis. Spash (2021) The Dasgupta Review deconstructed: an exposé of biodiversity economics (pdf) "],["tax.html", "51 Tax 51.1 Corporate Tax", " 51 Tax 51.1 Corporate Tax Profit Shifting OECD Process There remain significant difficulties in the OECD process, with its two-pillar proposals. First, there is no common ground on ‘Pillar One’. This is the element which would go beyond the archaic arm’s length principle and introduce some element of formulary apportionment (that is, allocating a share of each multinational’s global profits to the places where they actually do business, in the form of sales and employment). The US (under Biden, as under Trump) wants Pillar One to apply to all businesses; the EU is focused on the big tech multinationals; and the OECD proposal to identify ‘consumer-facing’ businesses falls somewhere in between these. As things stand, the OECD proposal is highly complex and would retain arm’s length pricing for most profits, and therefore result in relatively little reduction in profit shifting – making it largely unattractive for most countries. At the same time, the proposal would require global treaty change, meaning that it could very easily be blocked – including by the US Congress, regardless of whether the Biden administration had come around to support it. ‘Pillar Two’ contemplates a global minimum corporate tax rate. This has the potential to go a long way to stop profit shifting, not by making it harder to achieve but by making it much less rewarding – since multinationals would, in theory, end up being taxed at the minimum rate even if they managed to shift the profits to a zero rate jurisdiction. Here again though, the current OECD proposals are highly complex, and have been very unambitious. And an argument about ‘rule order’ – in simple terms, whether the home country of a multinational goes first in levying any top-up tax, or the various host countries – has exposed the major distributional question. Despite some initial optimism, most non-OECD members have by now become thoroughly disillusioned with the process. An outcome that favours OECD members, despite lower-income countries bearing disproportionately high revenue losses due to profit shifting, would be unconscionable – but, sadly, not entirely unexpected. Lastly, the insistence that the two pillars are inseparable, and must be delivered jointly, creates a hugely complicated contraption requiring great resources to move ahead, but with little certainty over any benefits. Way Forward Stepping out of the limitations of the OECD process, things very quickly start to look much brighter. This is for three main reasons. First, the two pillars can be separated – and that means the unworkable and unambitious ‘Pillar One’ can be left behind, along with the requirement for global treaty change. Instead, a global minimum corporate tax can be taken forward by a coalition of the willing. (In fairness to the OECD secretariat, they have raised this possibility at times also, recognising the practical difficulties of their Pillar One.) With the US and Germany (and the European Commission) committed to a minimum tax, broad agreement on the shape could be reached relatively quickly. Second, the OECD leadership’s longstanding insistence on a very low minimum rate of 12.5% can be set aside. The Biden administration has indicated a rate of 21%. The Independent Commission for the Reform of International Corporate Taxation has proposed 25% as an absolute minimum; while discussions among various groups of lower-income countries have suggested higher rates still, to ensure that they are not disadvantaged. Negotiating upwards from 21% – and with the possibility of different countries taking their own approaches as appropriate – would provide a quite different dynamic. And third, the setting aside of Pillar One creates the possibility of pursuing a more ambitious approach to the minimum tax. Our proposal for the METR, or Minimum Effective Tax Rate for multinationals, does just this. We propose a method that builds on the technical efforts of the OECD secretariat, who have done sterling work in establishing various approaches to identify and to apportion taxable profits, but shifts the politics substantially. METR In effect, the METR combines the two pillars by identifying under-taxed profits, and then apportioning these for ‘top-up’ taxation on a formulary basis, according to the location of multinationals’ real activity. In this way, the METR cuts through any ‘rule order’ debates and instead treats all countries, home or host, on an equivalent basis. Politically, the momentum for globally inclusive solutions at the UN, rather than the rich countries’ club at the OECD, will continue to grow – and especially if a one-sided minimum tax solution emerges from the OECD now. The FACTI panel report called for the negotiation of a UN tax convention, which would provide the basis for an intergovernmental body under UN auspices to set corporate tax rules in future – including a global minimum tax rate designed to benefit all. That the US administration is now leading the push for an ambitious global minimum tax rate confirms this as the new norm. The decisions set to be made in the next couple of months, over technical design and political inclusion, will determine just how effective this can be in bringing an end – finally – to the decades-long race to the bottom in corporate tax. With the confirmation of this new narrative, it seems likely that what is left undone in the OECD process will be resolved through a combination of unilateral and UN action. Global Corporate Minimum Tax "],["welfare.html", "52 Welfare 52.1 The Social Guarantee 52.2 Net Domestic Consumer Surplus", " 52 Welfare 52.1 The Social Guarantee The Social Guarantee enshrines every person’s right to life’s essentials: education, health and social care, a decent home, childcare, nutritious food, clean air and water, energy, transport and access to the internet. For this to happen, all people must have access to collectively provided services that meet their needs, as well as to a fair living income. SocialGuarantee.org 52.2 Net Domestic Consumer Surplus Fouquet Roger Fouquet is currently working on estimating the Net Domestic Consumer Surplus (NDCS) for the United Kingdom over the last three hundred years. NDCS measures the area below the demand curve for each consumer category and above the price line, and sums all the consumer categories to produce an aggregate value. NDCS offers a new macroeconomic indicator of economic welfare and a complement to GDP. The preliminary results indicate that the broad trend in Net Domestic Consumer Surplus (NDCS) per capita is similar to GDP per capita – showing the improvements in welfare from the end of the nineteenth century, major increases since the 1950s and a decline following the Great Recession of 2007-8. Figure: Net Domestic Consumer Surplus (NDCS) per capita in the UK, 1700-2017 GDP per capita heavily under-estimates the consumer welfare gains at early stages of economic development and over-estimates the consumer welfare gains at later stages of economic development. For instance, each pound or dollar produced today is not generating as much consumer welfare as 90 years ago. Fouquet "],["countries.html", "53 Countries 53.1 Argentina 53.2 China 53.3 Italy 53.4 Jamaica 53.5 Norway 53.6 Russia 53.7 Turkey 53.8 Ukraine", " 53 Countries 53.1 Argentina Argentina traditionally has a lot of inflation, so it’s a good place to look for evidence; this is also probably why it produces so many great macroeconomists- 53.2 China China basically destroyed every idea western intellectuals had about economic growth and development. Practically nobody admits how embarrassing this is for economists and political scientists. (Yuen Yuen Ang) I think Acemoglu’s focus on democracy and non-extractive institutions is a bit of outlier in the growth lit, which mostly would contend that China’s high levels of average human capital and physical capital investments should lead to substantial growth. I think China actually is democratic and non-extractive at the local level, something that Acemoglu misses by focusing on national stuff. A big part of the post-Mao reforms was turning local development over to local governments and making them accountable for performance. To be honest China since Deng Xiaoping has followed the Fredrich List- Alexander Hamilton form of political economy. They are following the same policies from Meiji Japan and Imperial Germany. Twitter Thread 53.3 Italy Tooze How much growth does Italy really need? I don’t want to engage in the degrowth debate here. But Italy is also interesting from that aspect. By most reasonable criteria, despite this stagnation in gdp per capita, the standard of living in Italy for a large majority of the population is very attractive, enviable indeed. Behind a rawlsian veil of ignorance, would you pick a “success story” like the United States over Italy? There are very real deficits of an institutional kind, in technical modernization, not to mention the future problems of the climate, energy transition etc. What Italy needs is not brute-force growth at any price, but faster, “smart” growth, reform of institutions to improve everyday life, above all in the legal system and state administration, and a major investment in its education system, which currently fails far too many young Italians. Tooze (2022) Draghi for Presiden? 53.4 Jamaica Smith Jamaica is classified as a middle-income country by the World Bank. In terms of GDP per capita (PPP), it’s at about $10,000, putting it somewhere in the same neighborhood as Vietnam, Tunisia, or Jordan. That’s rich enough for Jamaicans not to starve, and for most to have generally decent living situations, but not enough to eliminate extreme poverty or create the kind of broad middle class we’d recognize as such in a developed country. Jamaica’s socialist policies in the early 70s — nationalization of industry, import barriers, subsidies for basic goods, high deficits, and so on — did long-term damage, while Barbados’ relatively neoliberal policies avoided hurting its economy much during those troubled years. This, if true, would be a pretty direct rebuke to the ideas of people like Ha-Joon Chang, who argue that nationalization, import barriers, and tolerance of rapid inflation are a more effective growth strategy than the traditional “neoliberal” orthodoxy of privatization, free trade, sound money, and balanced budgets. And interestingly, Jamaica’s more leftist turn in the 70s didn’t result in enduringly lower inequality than Barbados — the two countries’ Gini coefficients are almost equal, both in the mid-40s. The more important question here has to be why Jamaica hasn’t seen a takeoff in growth, the way the Dominican Republic. has. Remember, a growth takeoff — rather than simply stagnation at a higher level — is the ultimately goal of “development state” policies like those recommended by Ha-Joon Chang. It might be that each country just has to wait its turn in line, waiting for the so-called “flying geese” of global capital to decide that this is the next place to produce shoes and clothes and toys. Under this theory, countries can at best hope to cut in line by having slightly better education and infrastructure than the next country over. But if we assume there is something countries can do to jump-start industrialization, then the best candidate for what they can do is the “development state”. My favorite formulation of this is in the book How Asia Works, which — though it doesn’t get everything quite right — is the simplest and most reasonable guide to the basic idea. The author, Joe Studwell, distills the experiences of successful East Asian industrializers into three basic steps: Land reform, to employ the rural population and raise farm output while freeing up landlords to start manufacturing companies “Export discipline”, meaning a country promotes exports in manufacturing industries as a tool to absorb foreign technologies, learn how to climb the value chain, and generate foreign exchange Financial repression, meaning that the finance sector is forced to fund manufacturing and exports rather than putting all its money in stuff like real estate or Bitcoin or whatever. On land reform, Jamaica fails badly; concentrated land ownership and widespread tenant farming is a big reason for its continued high inequality. The Jamaican Left keeps proposing land reform, and it keeps not getting done. This is probably depriving Jamaican manufacturing of talent; a number of the country’s richest and most well-educated people are engaged in relatively unproductive landlording instead of starting the next Hyundai or Samsung. How about export discipline? Here, Jamaica faces a more formidable enemy than political gridlock — its own abundance of natural resources. Jamaica has huge reserves of bauxite, which is used to make aluminum. This accounts for an enormous percentage of its exports. Exporting aluminum-related minerals makes Jamaica’s currency more expensive (since it means other countries have to buy a lot of Jamaican dollars in order to buy the minerals). That makes Jamaican exports less competitive. This is known as “Dutch disease”. The second problem posed by resource abundance is high wages. Yes, we all think high wages are good, but if you’re trying to get in on the cutthroat game of global manufacturing exports, you need to start at the bottom, with simple low-value activities like making clothes and toys and light electronics assembly. This is how Bangladesh and Vietnam — and, indeed, the Dominican Republic — kicked off their own exponential growth. But with a per capita GDP of around $10,000 in 1990, Jamaica was much richer than Vietnam or Bangladesh or the D.R. In fact, it’s still richer than the first two. Because it had so much bauxite (and, possibly, so much tourism), Jamaican wages were necessarily uncompetitive when they needed to be competitive. In fact, Jamaica did make a big push for export-oriented industrialization, starting with exactly the kinds of labor-intensive light manufacturing industries that have worked well for the successful industrializers. Established in the 1970s and 1980s, the Jamaican Free Zones were similar to China’s Special Economic Zones — they provided tax exemptions for businesses, facilitated foreign investment, and favored export-oriented industries like textiles. But the Free Zones never took off. Even with all the tax breaks, Jamaican manufacturing was just never competitive. In an economy that’s used to middle-income living standards, the low wages and brutal conditions necessary to be competitive in labor-intensive manufacturing just didn’t provide people with what they considered to be decent work. Whereas Bangladeshi and Vietnamese workers would endure harsh conditions for a while but eventually get better lives as companies learned how to do more complex manufacturing, Jamaica’s high starting wages short-circuited that process. If industrialization is a process of learning to walk before you learn to run, then according to this theory, Jamaica was born with a fancy aluminum wheelchair. This might be the brutal, unfortunate truth for developing countries — those that are endowed with plentiful natural resources might simply be destined to start out ahead but eventually fall behind. The fact is, we don’t really know how economic development happens, and to put too much faith in the Chang/Studwell story would be unwise. After all, the Dominican Republic is pulling off a successful manufacturing-based industrialization, and yet its top export is still gold. So having resource endowments doesn’t ensure stagnation; Jamaica still has a chance. With the rise of highly complex service industries, there’s always the chance that Jamaica may in some sense be able to “leapfrog” to higher-value activities. Jamaica is trying to do this with the Jamaica Logistics Hub, which envisions the island becoming a waystation for trade throughout the Caribbean and Latin America. This can be a good strategy for a small country — is has worked well for countries like Dubai and Singapore, each of which is actually more populous than Jamaica. Becoming a logistics hub will involve a lot of new infrastructure — in fact, China is helping Jamaica build out its facilities. Ports, roads, and various processing facilities are all being built. According to some sources, the country is now investing about a quarter of its GDP, almost as good as Vietnam, up from about a fifth of GDP five years ago. Jamaica has avoided becoming a kleptocratic basket case like many resource exporters. Jamaica is doing OK, but it deserves to be a rich country. It just has to keep experimenting until it finds a model that will get it there. Smith (2021) Jamaica is doing OK Could it do better than OK? 53.5 Norway 53.5.1 Oil and Gas ssb 53.5.2 Emissions ssb 53.6 Russia 53.7 Turkey Tooze Since the early 2000s Turkey has relied heavily on foreign borrowing (mainly by the private sector) to finance large current account deficits brought on by rapid growth. Erdogan had ridden a wave of capital inflows that were attracted to Turkey by slightly higher interest rate margins. Figure: Systematically promote growth through construction rather than manufacturing, agriculture and exports. Turkey’s economic experiment ran much longer than it should have, thanks to the more elastic supply of finance. The economic costs will be larger as a result. Erdogan has long cultivated an economic ideology that consists of a brew of convenient fragments of Muslim doctrine, productivism, hostility towards outside pressure. He is prone to associating any critics with external threats. Warring with Turkey’s secular business interests is now a key device through which Erdogan rallies his electoral troops. He is egged on by advisors such as Cemil Ertem who systematically promotes the idea of politico-economic autonomy. The main message given by Ertem’s presentation is economic independence. Over and over, he emphasises that it is impossible to be economically independent while carrying out a financial policy based on high interest rates or International Monetary Fund recommendations. Ertem asserts that a policy of high interest rates has triggered a vicious circle of low exports, lower employment, high imports, growing external debt and a country with an external dependence, which again requires higher interest rates, completing a full circle. “As a result, the country is posting a high current account deficit and depending on short-term hot money inflows and raising the external debt,” he says in the presentation. “This economic model, due to its external dependency, is laying [the] ground for economic attacks.” The chief economic adviser says his new model, based on lower interest rates, will increase exports and decrease imports, leading to a current account surplus and higher growth with high employment. He believes it will make Turkish exports more competitive with a depreciated lira. “We will incentivise foreign direct investments instead of short-term hot money inflows, and stabilise the foreign finances,” Ertem says. “And that’s how we will become a stronger country that is protected from the external financial shocks.” Certainly the devaluations of recent years have had an effect. “exports jumped 33 percent in November, reaching $21.5bn, while the current account posted a $3.16bn surplus for October. Unemployment has also decreased, by about two percentage points, from 13.1 percent to 11.2 percent in October year-on-year. GDP grew by seven percent in the third quarter of 2021.” With import prices soaring, the economic circumstances threaten to crush domestic demand; a recent 50 per cent rise in the minimum wage will wipe out the cost advantages of the currency depreciation. For devaluation to yield dramatic benefits would require an industrial policy to back it up. In recent months, as Erdogan launched another easing cycle, he has reportedly cited China’s economic transformation in the wake of 1978 reforms as evidence that his model would bear fruit. It is economically crazy to think that a country can build an export-oriented economy simply on the back of a trashed currency.If that were the case, Zimbabwe would be a tech superpower. Some businesses are gaining from the slide in the currency. Most of the companies listed on the Borsa Istanbul are benefiting from the weak lira - publicly listed airlines, defence groups, carmakers and chemicals producers as companies that enjoy foreign currency-denominated revenues and Turkish lira-denominated staffing costs. Erdogan is prioritising exporters over households. Turk Dolar: The new tool, labelled the “Turkish dollar” by some on social media, offers a solution to this problem: if investors convert their foreign currencies into lira and deposit them in a savings account with a certain term of maturity, Turkey’s treasury guarantees that it will get the same return as forex markets. And if the forex markets drop below the official interest rates, the investor will still get an official interest rate return. Tooze (2021) Turkey’s financial crisis Özgür Orhangazi,A. Erinç Yeldan in Development and Change (2021) Noah Smith But there were always three flies in the ointment of Turkey’s long boom. The first was a reliance on external borrowing. The second was political instability, ultimately leading to bad macroeconomic policy. And the third was Erdogan’s bizarre love of low interest rates. The root of the problem was external borrowing. Turkey’s fairly high investment rate isn’t matched by an equally high savings rate, meaning that it has had to run a big current account deficit While some of this investment was in the aforementioned productivity-boosting manufacturing industries, a fair amount went into real estate (encouraged by the Erdogan administration) and into grandiose construction projects undertaken by the government. Since many of these projects were undertaken or encouraged for populist political reasons, they didn’t earn a great return on investment and ultimately set the country up for a rapid reversal of capital inflows. The second factor making things worse was that a ton of Turkey’s external debt was short-term, leaving it even more vulnerable to a sudden stop in foreign investment. Let’s ask why Erdogan was so eager to encourage foreign investment into sectors that weren’t going to boost long-term productivity and set the country up for a crisis. The fairly obvious answer is political instability. When populist leaders face challenges to their rule, a very common response is to pump up short-term economic growth in order to shore up political support, even at the expense of long-term productivity growth. Erdogan needs to use his near-dictatorial power to channel investment away from speculative real estate or white elephant construction projects, and back toward manufacturing industries. Noahpinion (2022) Turkey you are doing so well 53.8 Ukraine Alex Smith About half of all wheat consumed in Lebanon in 2020 came from Ukraine, according to data from the Food and Agriculture Organization (FAO). Relying on bread and other grain products for 35 percent of the population’s caloric intake, Lebanon is critically dependent on Ukrainian wheat. Of the 14 countries that rely on Ukrainian imports for more than 10 percent of their wheat consumption, a significant number already face food insecurity from ongoing political instability or outright violence. For example, Yemen and Libya import 22 percent and 43 percent, respectively, of their total wheat consumption from Ukraine. Egypt, the largest consumer of Ukrainian wheat, imported more than 3 million metric tons in 2020—about 14 percent of its total wheat. Ukraine also supplied 28 percent of Malaysian, 28 percent of Indonesian, and 21 percent of Bangladeshi wheat consumption in 2020, according to FAO data. Smith (2022) A Russia-Ukraine War Could Ripple Across Africa and Asia Tooze (2022) Strategy of tension "],["economics-1.html", "54 Economics 54.1 Economics is Pseudo-Science 54.2 Economic Goods and Services 54.3 Economics Discipline 54.4 Second Best Theory 54.5 Economics Empirical Turn 54.6 History of Economics 54.7 Weaponizing Economics 54.8 History of Growth Thinking", " 54 Economics 54.1 Economics is Pseudo-Science Kyger and Fix ‘Essentialism’, as we see it, is the reification of a theory — a transformation from ‘provisional explanation’ to ‘timeless truth’. Mainstream economics makes so many false claims that we could write a book debunking them. (Which is why Steve Keen did just that [7].) Although economics presents itself as a hard science, under the hood it is essentialist dogma, held in place by tradition. Outside economics, the term has an unambiguous meaning: a ‘natural law’ is an empirical regularity with no known exception. The laws of thermodynamics are a prime example. Left alone, objects ‘naturally’ converge to thermodynamic equilibrium. Leave a hot coffee on the table and it will soon cool to room temperature. The outcome is the same today as it was yesterday. It is the same for you as it is for me. It is a ‘natural law’. By documenting and explaining this empirical regularity, we are following the recipe laid out by Locke, Hume, and Kant. Observe the real world and try to explain consistent patterns. When economists appeal to ‘natural law’, however, they are doing something different. Take the so-called ‘natural’ rate of unemployment. If this rate were like the laws of thermodynamics, unemployment would gravitate towards a single value. Try as you might, it would be impossible to change unemployment from this ‘natural’ rate. Needless to say, unemployment does not work this way. Instead, it fluctuates greatly, both in the short term, and over the long term. So when economists refer to the ‘natural’ rate of unemployment, they don’t mean an empirical regularity. They mean an essence. As Milton Friedman defined it, the ‘natural’ rate of unemployment is that which is “consistent with equilibrium in the structure of real wages” [9]. So whenever (and wherever) the labor market is in ‘equilibrium’, unemployment is at its ‘natural’ rate. So how do you tell when the market is in ‘equilibrium’? Good question … nobody knows. That’s because market ‘equilibrium’ is not something economists observe. It is something economists imagine and then project onto the world. It is an essence. To convince yourself that this true, pick any economics textbook and search for the part where the authors measure market ‘equilibrium’. Find the section where they construct the ‘laws’ of supply and demand from empirical observations. Look for where they measure demand curves, supply curves, marginal utility curves, and marginal cost curves. Seriously, look for these measurements. You will not find them.5 You won’t find them because they are unobservable. These concepts are essences. The equilibrium-seeking free market is an idea that economists project onto the world, and then use to interpret events. Anything that fits the vision is ‘proof’ of the essence. Anything that seems contradictory is dismissed as a ‘distortion’. And that brings us to economics education. The core content in Econ 101 has changed little over the last half century (if not longer). And that’s not because the ‘knowledge’ is secure. It’s because the content of Econ 101 is a tradition. The point of Econ 101 is to indoctrinate the next generation in the ‘essence’ of economics. This powerful combination of essentialism and traditionalism has made economics a “highly paid pseudoscience” In his essay ‘The Methodology of Positive Economics’, Friedman argues that you cannot test a theory by comparing its assumptions to reality [13]. Instead, you must judge the assumptions by the predictions they give. If the predictions are sound, says Friedman, so too are the assumptions. (For why this is a bad idea, see George Blackford’s essay ‘On the Pseudo-Scientific Nature of Friedman’s as if Methodology’ Scientists want to know if their hypothesis is ‘correct’. The problem, though, is that making this judgment is inherently subjective. The evidence for (or against) a hypothesis is always contingent and incomplete. And so scientists must make a judgment call. The purpose of statistics is to put numbers to this judgment call by quantifying uncertainty. It’s a useful exercise, but not one that removes subjectivity. Unfortunately, in many corners of science, statistical tools have become reified as the thing they were never designed to be: a decision-making algorithm. Scientists apply the tools of (standard) statistics as though they were an essential truth, a ritualistic algorithm for judging a hypothesis. Fortunately, there is a growing movement to reform hypothesis testing. One option is to pre-register experiments to remove researchers’ ability to game statistics. Another option is to lower the ‘traditional’ level of statistical significance. While we welcome both changes, we note that they do not solve the fundamental problem, which is that judging a hypothesis is always subjective. For that reason, we favor a transition to Bayesian statistics. Bayesian statistics is up front about the subjective element of judging a hypothesis. In fact, when you use the Bayesian method, this subjectivity gets baked into the calculations (in what Bayesian’s call a ‘prior probability’). Figure: Essentialist totems in economics, biology and statistics. Clockwise from top left: the neoclassical model of the equilibrium-seeking market, the Hardy-Weinberg model of genetic equilibrium, and the normal distribution — the ‘equilibrium’ behavior of infinitely many random samples. As physicist Sabine Hossenfelder shows in her book Lost in Math, the appeal to aesthetics leads scientists astray more often than it leads them to the truth. Kyger and Fix (2021) Essentialism and Traditionalism in Academic Research 54.2 Economic Goods and Services Roser Economic goods are scarce in relation to the demand for them Uneconomic Breathing!! An economic good or service is provided by people to each other as a solution to a problem they are faced with and this means that they are considered useful by the person who demands it. And a last characteristic that is helpful in deciding whether you are looking at an economic product is ‘delegability’. An activity is considered to be production in an economic sense if it can be delegated to someone else. This would include many of the goods and services on that long list we considered earlier, but would exclude your breathing, for example. Many discussions about economic growth are extraordinarily confused. People often talk past one another. I believe the reason for this is that the discussion of what economic growth is, gets muddled up with how it is measured. Growth is often measured as an increase in income or inflation-adjusted GDP per capita. But these measures are not the definition of it. Roser (2021) What is economic growth? 54.3 Economics Discipline Rodrik on CORE-ECON A key advantage of the CORE approach is that it tackles issues like inequality and climate change head-on. But the pedagogically more interesting move is that it replaces the standard benchmarks of economics with alternative benchmarks that are more realistic and useful. For example, in contrast to conventional economics, CORE assumes that individuals are pro-social and myopic, rather than selfish and far-sighted. Competition is imperfect, with winner-take-all characteristics, rather than perfect. Power is ever-present in the form of principal-agent relationships in labor and credit markets, instead of being treated as either diffuse or exogenous. Economic rents are ubiquitous and often required for well-functioning economies, not rare or the result of policy error. Such a new paradigm for teaching and doing economics will produce better understanding of social outcomes. But we should recognize that it will not produce a new paradigm for economic policy. And that is as it should be.All of our previous policy paradigms – whether mercantilist, classical liberal, Keynesian, social-democratic, ordoliberal, or neoliberal – had important blind spots because they were conceived as universal programs that could be applied everywhere and at all times. Inevitably, each paradigm’s blind spots overshadowed the innovations it brought to how we think about economic governance. The result was overreach and pendular swings between excessive optimism and pessimism about government’s role in the economy. The right answer to any policy question in economics is, “It depends.” We need economic analysis and evidence to fill out the details of what the desired outcome depends upon. The keywords of a truly useful economics are contingency, contextuality, and non-universality. Economics teaches us that there is a time for fiscal expansion and a time for fiscal retrenchment. There is a time when government should intervene in supply chains, and a time when it should leave markets to their own devices. Sometimes, taxes should be high; sometimes, they should be low. Trade should be freer in some areas, and regulated in others. Mapping the links between real-world circumstances and the desirability of different types of interventions is what good economics is about. Our societies are confronted with vital challenges that require new economic approaches and significant policy experimentation. The Biden administration has launched a bold and long-overdue economic transformation. But those who are seeking a new economic paradigm should be careful what they wish for. Our goal should be not to create the next ossified orthodoxy, but to learn how to adapt our policies and institutions to changing exigencies. Rodrik CORE-ECON 54.4 Second Best Theory A classic finding in the economic literature – the theory of second best – illustrates that, in the presence of many market failures, leaving one failure unaddressed is suboptimal. Conversely, addressing a particular market failure has the potential to improve the overall allocation of resources, even if it worsens some other market failure. Scnabel (2021) 54.5 Economics Empirical Turn Noah Smith In recent decades, three huge and important changes have happened in the economics profession. All of these changes work against both the free-market wave of the 70s and 80s and the rise of well-funded “economism” in the public sphere. First, the profession has become much more empirical. Whether or not something works in theory is less important now than whether it works in practice. Papers still have theory sections, but they’re more phenomenological — proposed explanations for observed phenomena, rather than a mathed-up form of philosophy. Meanwhile, new econometric methods relying on quasi-experiments are rapidly becoming dominant. The empirical turn means that economists are more open to being persuaded by the evidence. The second change was an increased willingness of academic economists to enter the public discussion. Leading media figures like Thomas Piketty, Paul Krugman, and Gabriel Zucman now lean to the left, and the influence of the generally left-leaning Econ Twitter is growing. Both of these help balance out the legacy institutions of the 80s free-market media machine. But the third and most important change is in the econ profession itself. The free-market revolt is over. Economists’ concern about inequality is growing rapidly. Noah Smith (2015) Is economics an excuse for inaction? 54.6 History of Economics Noah Smith There’s a sort of popular myth that economics began with Adam Smith’s declaration that the “invisible hand” of the market would lead to a good society. In fact, while Smith did recognize the importance of market forces and self-interest, his vision of a good society didn’t stop there. Adam Smith decries the existence of inequality and poverty, blames property rights for this inequality, advocates progressive taxation as a remedy, and is innately suspicious of profit. He sounds more like Thomas Piketty than Milton Friedman. Smith’s suspicion of profit and enthusiasm for redistribution are baked into the very core of economic theory. The zero-profit condition says that in a well-functioning market, the rate of profit should be no more than the cost of capital — if you see companies making big margins, you should suspect that the market isn’t working Meanwhile, Smith’s call for redistribution is inherent in the Second Welfare Theorem, considered one of the basic theorems of economics — and something that every intro student is taught. The Second Welfare Theorem says that if you change the initial distribution of wealth in society, you can basically get any outcome you like. This puts the burden of proof on those who think we shouldn’t redistribute wealth — it forces them to bring proof that the harms from taxation are just too high. Though there have been some economists who opposed redistribution, enthusiasm for the idea is traditionally very dominant within the profession. Even Milton Friedman, that great champion of laissez-faire, supported the idea of a negative income tax that would give people more cash the poorer they were. Over the decades, leading economists found other reasons for government intervention in the economy. Just a few examples: John Maynard Keynes, the father of modern macroeconomics and an incredibly influential figure, came up with the idea of fiscal stimulus to solve the problem of recessions — an idea that is almost universally accepted today among economists. Paul Samuelson, arguably the most influential economist of the 20th century, and the author of much of the modern economics curriculum, came up with the theory of public goods — things like infrastructure and research and public parks that the private sector won’t provide on its own. Kenneth Arrow, one of the profession’s leading lights, explained why the free market doesn’t work in the health care industry. Joseph Stiglitz, yet another Nobel prize winner, showed that under some simple assumptions, land should be taxed at 100% of its value — basically, total redistribution of the wealth from land ownership (the idea was originally due to the 19th century economist Henry George). Along with George Akerlof, Michael Spence, and others, Stiglitz pioneered the theory of asymmetric information, which shows yet another reason free markets break down (and which provides another justification for government health insurance). This is by no means an exhaustive list. But it shows how economists at the very top of the field — the people cited are not just Nobel winners but legendary names within the profession — spent their effort finding reasons to justify action by the ruling class to alleviate inequality, poverty, and market breakdown. The libertarian economics of the 70s and 80s had its roots in the Mont Pelerin Society and the University of Chicago economics department in the mid 20th century. Economists like Friedrich Hayek, Milton Friedman, and George Stigler were consciously and openly ideological in their promotion of small-government ideology. This culminated in Friedman’s famous 1980 television special, Free to Choose, in which he combined libertarian principles with (often simplistic and obsolete) economic theory to endorse laissez-faire approaches across a broad swath of policy issues. This free-market revolt left lasting marks on the profession. The new discipline of “law and economics” was dominated by the Chicago school, providing strong support to big business in its quest to become even bigger. Macroeconomics became temporarily dominated by anti-interventionists. Economists like Robert Lucas and Ed Prescott made models claiming that recessions are optimal economic outcomes (!!!), and that attempts to fight them only make things worse. The free-marketers had a deep impact on econ education. Greg Mankiw’s textbooks, which generally favor free-market ideas, have become the standard introductory undergrad textbook, and are only now starting to be displaced by materials like Krugman’s textbooks and the CORE Project. Noah Smith (2015) Is economics an excuse for inaction? 54.7 Weaponizing Economics Franta Abstract The role of particular scientists in opposing policies to slow and halt global warming has been extensively documented. The role of economists, however, has received less attention. Here, I trace the history of an influential group of economic consultants hired by the petroleum industry from the 1990s to the 2010s to estimate the costs of various proposed climate policies. The economists used models that inflated predicted costs while ignoring policy benefits, and their results were often portrayed to the public as independent rather than industry-sponsored. Their work played a key role in undermining numerous major climate policy initiatives in the US over a span of decades, including carbon pricing and participation in international climate agreements. This study illustrates how the fossil fuel industry has funded biased economic analyses to oppose climate policy and highlights the need for greater attention on the role of economists and economic paradigms, doctrines, and models in climate policy delay. Franta Memo An early handbook written for regulated industries even has among its top recommendations, ‘Coopt the Experts’ (Owen and Braeutigam 1978). The handbook explains (p. 7): “Regulatory policy is increasingly made with the participation of experts, especially academics. A regulated firm or industry should be prepared when­ ever possible to coopt these experts. This is most effectively done by identifying the leading experts in each relevant field and hiring them as consultants or advisors, or giving them research grants and the like. This activity requires a modicum of finesse; it must not be too blatant, for the experts themselves must not recognize that they have lost their objectivity and freedom of action.” By the early 1980s, at least some economists were already counteracting calls for policies that would help prevent and minimize global warming. In 1983, the US National Academy of Sciences published Changing Climate, a report presenting an overview of contemporary climate science and policy thinking (National Research Council 1983). The portions by scientists, for the most part, warned that continued fossil fuel use would have dire con­ sequences. The economists, in contrast, counseled against policy action, suggesting that global warming might not be that bad. Thomas Schelling of Harvard University argued that migration and adaptation would be preferable to reducing fossil fuel emissions. ‘It would be wrong to commit ourselves to the principle,’ he wrote, ‘that if fossil fuels and carbon dioxide are where the problem arises, that must also be where the solution lies’ (p. 449). William Nordhaus of Yale University agreed, writing that although a fossil fuel tax would reduce emissions, ‘[t]he strategies suggested . . . by Schelling . . . cli­ mate modification or simply adaptation to a high CO 2 and high temperature world – are likely to be more economical ways of adjusting’ (p. 151). Yet neither economist provided a detailed analysis to support his conclusions. While the scientists’ warnings were supported by decades of research, the economists’ reassurances were closer to hopeful guesses. Yet the economists’ arguments, though speculative, were given credibility by the National Academies report. as pressure grew in the late 1980s to prevent severe global warming, it may not be surprising that the fossil fuel industry turned to economists to help influence public policy. Important among these economists were those at Charles River Associates, a US-based consulting firm that played a key role in weakening, delaying, or defeating a wide range of climate policies over the following years, including US carbon pricing proposals and international climate agreements. These economic consultants helped convince the public and policymakers that climate policy would be costly, global warming would be relatively unimportant, and there would be little harm in delaying action. Their work was paid for by the fossil fuel industry, a fact often concealed from the public, and their methodologies were incomplete in favor of the industry. Franta (2021) Weaponizing economics: Big Oil, economic consultants, and climate policy delay (pdf) 54.8 History of Growth Thinking Garzon Economic growth is indeed habitually seen as something desirable, limited in space and time and even as a reflection of a process of natural evolution of societies. The very notion of economic growth is intrinsically connected with the social notion of progress, both of which arise from the Enlightenment and have been victims of forced, equivocal analogies with the natural sciences, particularly based on Darwinist theory (Nisbet, 1980). In short, we have firmly internalized and naturalized the notion of economic growth. The main problem underlying conventional economics is its reliance on a conceptualization of the economy which deliberately ignores the physical context of which it is necessarily part, as well as the most elementary laws of physics. This means working on the assumption that resources and energy are unlimited, without even considering the fallout of the activity or the planet’s limited carrying capacity. In view of the hegemonic nature of economic thought insofar as it is capable of moulding the framework of social thought, this is crucially important, because it makes finding effective solutions to the eco-social crisis virtually impossible. Defective economic models Economic growth can be seen as the result of greater production capacity on the part of a particular society. To simplify, this means that a society which produces a larger quantity of product than it did in the previous year is said to have grown economically by an amount equal to the difference between the two levels of output. In this way, a country which produces 10 units of food in a particular year and produces 12 units of food the following year is said to have experienced a 20% growth in food units. These two new food units are considered as economic surplus. The systematic build-up of economic surpluses lies behind the development of societies, inasmuch as historically it has enabled societies to become more complex (Cesaratto, 2020). Capitalism is an economic system which emerged around five centuries ago and introduced a series of incentives, through competition, to discipline companies and force them to grow in each period, as well as to reinvest profits in order to raise their production capacity to a higher level, in addition to awarding a growing share of those profits to the people who supplied the capital. In this way, under capitalism the whole entrepreneurial fabric is pushed towards boosting its production capacity. This is what, under particular institutional arrangements, has driven the spectacular increase in economic activity, infrastructure and, finally, the living standards of people over the past two hundred years. The historical reality of capitalism has, however, demonstrated that the process of economic growth is neither constant nor spared from serious upheavals (leading to phenomena such as unemployment and lack of paid work for large sectors of the society). Economists have also devoted themselves to the task of untangling the difficulties of economic growth for more than two hundred years. Most of them, however, have used a set of theoretical instruments blind to the ecological issue, i.e. the ecological prerequisites for economic growth and the ecological consequences of that growth. Classical economists, the founders of Political Economy as a discipline, have nevertheless undoubtedly been aware of some of what we might call the social metabolism, i.e. the relationship between nature and the economy (Haberl et al, 2016; González de Molina, M., 2014). The physiocratic school, the predecessor of the above, whose principal exponent was François Quesnay, had already interpreted the economic question in the 18th century on the basis of agrarian flows and concluded that any surplus is possible thanks to the gifts given to us by nature. David Ricardo, on the other hand, was aware of differing soil fertility and put together a theory of decreasing land yields which led him to think that capitalism could not grow indefinitely. Reverend Thomas Malthus introduced his now famous thesis on population growth as a constraint on economic growth. And Karl Marx and Friedrich Engels considered that capitalism would come up against limits to its own development due to the downward trend of the rate of return, although all of this fell within an essentially teleological philosophy of history according to which the whole system would inevitably advance phase by phase until it culminated in communism (Garzón, 2017). A particular remark should, however, be made in the case of Marx, since research over the last few decades has shown that Marx was also a thinker extremely interested in the scientific advances of his time and that he himself accorded considerable importance to the concept of social metabolism (Burkett, 2006; Foster, 2020; Saito, 2022). In the 20th century, economic thinking, in striving to make the discipline more scientific, moved even further away from the physical and even social conditions under which any economy must necessarily operate. Neoclassical thought, as reformulated by Walras, Marshall &amp; Jevons, amongst others, permeated economic science as a whole and led to a break with the previous Political Economy, giving rise to notions of production and wealth completely disconnected from a natural base (Naredo, 2015). Meanwhile, the search for theoretical explanations of economic growth and its possible failures continued with the economists Harrod and Domar, who developed a model which concluded that economic growth was fundamentally unstable and that meeting the conditions for stability was extremely complicated (Harrod, 1939; Sen, 1970). That Keynesian-inspired model provoked a response from neoclassical economists such as Robert Solow and Swan, who laid the foundations for the paradigm of economic growth and whose models are still being studied as a priority in every economics faculty around the world. These are the models which, in the end, define to a large extent economists’ scope of thought. The cornerstone of every model of economic growth is the aggregate production function. This function represents the economic production process and, in its most basic formulation, only involves capital and labour, while resources and energy are always considered as fully available. In this way, capital and labour are consolidated as the only production resources which, together, generate the surplus of an economy. This surplus, in its turn, makes up the amount to be distributed between wages and profits. This is the root of a large proportion of policy discussions around accumulation and distribution in capitalist societies. Ethical and political issues as important as the level of wages or profits or, even more, their relative share of income, arise from the implicit question concerning the effects of those changes on economic growth. Each model belongs to a distinct school of thought due to its specific configuration, determined by different starting assumptions. In general, neoclassical models consider that restrictions on growth come from the supply side, so they suggest that profits must be increased to encourage accumulation, while post-Keynesian models focus on restrictions from the demand side and usually suggest changes in the distribution of income and increases in wages (or public expenditure) to support demand. The large majority of current discussions of economic policy fall within this perspective. Nevertheless, the paradigm is always shared, and the debate really turns on ways to maximize economic growth. Students of economics are often surprised, when studying these models, especially the most basic ones, that there is apparently no possibility of unlimited growth existing. For example, Solow’s model establishes that the production factors, capital and labour, have decreasing returns, which supposes that each additional unit provides an ever-smaller quantity of product. In its dynamics, the model tends towards a stationary state where there is no economic growth. Nevertheless, when technical progress, in whichever possible formulation, is incorporated in these basic models, it is then possible for potentially unlimited growth to exist. This is what happens with the AK growth or endogenous growth models, as well as all models incorporating growing returns in the aggregate production function (Acemoglu, 2009; Romer, 2000). In the end, students soon learn that unlimited economic growth is technically possible thanks to technology and, in the case of certain heterodox models drawing inspiration from Allyn Young, Gunnar Myrdal, Nicholas Kaldor and Anthony Thirlwall, also the central role played by the industrial sector (Blecker &amp; Setterfield, 2019). This brief review of the relationship between economic models and public policy should make it clear above all that economists, past and present, generally tend to think within analytical and conceptual frameworks defined on the basis of the search for maximum economic growth. Garzon (2022) The limits to growth: eco-socialism or barbarism "],["economists.html", "55 Economists 55.1 Joan Robinson 55.2 Pierro Sraffa 55.3 Diane Coyle 55.4 Mario Draghi 55.5 James Galbraith 55.6 John Maynard Keynes 55.7 Paul Romer 55.8 Frederick Soddy 55.9 Dennis Snower 55.10 Adam Smith 55.11 Milton Freeman 55.12 Paul Krugman 55.13 Herman Daly 55.14 Wassily Leontief 55.15 Friedrich Hayek 55.16 William Nordhaus 55.17 Axel Leijonhufved", " 55 Economists 55.1 Joan Robinson The orthodox economists have been much preoccupied with elegant elaboration of minor problems.… Marx’s intellectual tools are far cruder, but his sense of reality is far stronger. In the autumn of 1975, there was one name “on everyone’s list for this year’s Nobel Prize in Economics,” Business Week magazine trumpeted: the Cambridge economist Joan Robinson. The week before the prize announcement, the magazine predicted that Robinson would be the first woman to win the prize. A major interpreter of John Maynard Keynes and Karl Marx, she was one of the most prominent economists of her generation. But when the names of the winners were read out at the Royal Swedish Academy of Sciences, Robinson’s name was not among them. When she died in 1983, just shy of her eightieth birthday, she had not won the prize – despite words of support from laureates as different as Paul Samuelson and Milton Friedman. What went wrong? More than perhaps any other factor, one man was to blame: Mao Zedong. Robinson’s writing in praise of Mao’s China – from her defense of the ruinous Great Leap Forward to her zesty praise of the Cultural Revolution – was likely what lost her the Prize. Fang Qin, an economist at Fudan University in Shanghai, put it plainly: “She is considered the most important female economist in history, but she did not win the Nobel Prize because she publicly praised the Cultural Revolution.” “How could it happen that, under cover of Mao Tse-tung thought, a medieval drama of ambition and treachery could play itself out?” Gewirtz (2017) Mao NobelPrize At the outset of this debate over capital theory, Professor Joan Rob- inson argued that capital could not be aggregated using any single measure because capital assets include a tremendous variety of goods: physical equipment, real estate, trademarks, financial products, and so on. 61 The normal remedy — and a move that lies at the heart of the neoclassical theory of capital — is to group these disparate entities together as a homogeneous stock or fund, using their market value to render them commensurate. Piketty adopts this approach, yet a prob- lem arises when we want to calculate the rate of return on capital that has been aggregated in this way. According to standard economic theory, the return on capital should be equivalent to its rental price at equilibrium — that is, to the cost of hiring capital at a given moment, independent of any short-term shocks or anomalies. This return should be equal to the “marginal product” of capital, because (assum- ing equilibrium) the cost of capital will equal the value of the additional production it enables. The problem with this solution is that calculating the equilibrium price of capital assets requires assuming a given rate of interest, as shown by several technical results from the 1960s and 1970s. 62 But (again according to standard economic theory) the rate of interest is itself endogenously related to the price of capital assets: it is supposed to reflect the marginal product of capital. Calculating the rate of return on capital would thus seem to require assuming the very thing that the calculation is supposed to discover, making the exercise circular. Grewal (2014) Review of Piketty (full pdf) 55.2 Pierro Sraffa But what determines the rate of return on a capital asset if it is not simply the value of the additional production it enables? The force of the debate over this question stemmed from the ideological stakes of the conventional answer. The theory of marginal productivity was thought to naturalize capital and its share of national income, rational- izing profit as simply what was owed to one factor of production, capi- tal, just as wages were what was owed to labor. 64 Against this, Profes- sor Piero Sraffa’s seminal contribution was to show that the rate of return could not be derived from the marginal product of capital but was rather an independent variable — given, he suggested, by factors “outside the system of production,” such as monetary policy. 65 In ef- fect, according to Sraffa, the equilibrium price of capital goods reflects a history of social struggle over the terms of economic cooperation and cannot be understood simply in terms of the aggregate production function. Grewal (2014) Review of Piketty (full pdf) 55.3 Diane Coyle 55.3.1 What happened to Cambridge Economics? Galbraith on Coyle One can understand and even sympathize with Coyle’s project. The real world has overtaken Friedrich von Hayek and his lead disciple, former British Prime Minister Margaret Thatcher. Today’s profound inequalities are becoming politically unacceptable. Financial crises are endemic, and now climate change is upon us, too. The free-market, deregulate-and-privatize verities of Coyle’s professional youth have lost their appeal. But as Coyle points out, the discipline is still exceptionally disciplined. Academic success demands publication in one of only five “top” journals, all of which are tightly controlled by acolytes of the mainstream orthodoxy. For most economists today, the only practical way to get ahead is to build on (and therefore accept) that orthodoxy. Deference, even sycophancy, is required. Thus, Coyle herself recites from the catechism: “What markets do brilliantly, nevertheless, is coordinate the use of resources in a process of discovery and challenge. The information signaled by the prices set by demand and supply is a wonderful coordinating device.” The new microeconomists point to problems such as pervasive “asymmetric information” – a favorite theme of the very progressive neoclassical economist Joseph E. Stiglitz. Others emphasize common flaws and sources of friction in markets – sticky wages, sticky prices, monopoly power – while still others focus on social costs and the provision of public goods. And yet all these “departures” still hew to the orthodoxy that treats perfectly informed, fully rational, price-adjusting buyers and sellers in perfectly competitive markets as the ideal type. It doesn’t seem to matter that the ideal type doesn’t exist anywhere in practice and never has. The presumed purpose of economic policy is to iron out all the flaws so that the world will behave “as if” it conformed to the ideal. A characteristic manifestation of this belief structure is the fashionable idea of “new antitrust,” which prescribes breaking up companies like Facebook, Google, and Amazon in order to ensure price competition in those industries. Another example is advocacy of carbon pricing as a mechanism to slow global warming. And even more pernicious is the case for “flexible labor markets” as a cure for joblessness. On this last point, Coyle writes that “both the Greek and Italian economies are widely thought to be hamstrung by an accumulation of regulations at the expense of competition, innovation and economic growth.” (Note the passive voice: “are widely thought.”) Mainstream economists may indeed think such things; but they are wrong. The Greek labor market was wholly deregulated a decade ago by IMF fiat. What disappeared was not unemployment but formal work and the middle class. Moreover, there is ample evidence that what is really good for jobs is union-driven wage solidarity, as practiced over the years in Scandinavia, Austria, and at times in Ireland. This fact has eluded mainstream economics and will continue to do so, because articles advancing such insights cannot get published in the “top five” journals. Prices Are Not the Key to Everything Coyle subscribes to the grand illusion that price adjustment is the economy’s prime mover. But as the Cambridge Keynesian economist Nicholas Kaldor noted in his slim 1985 book, Economics without Equilibrium, “the intuitive belief that prices are the key to everything” is simply wrong. The foundation on which Coyle places modern mainstream economics is a myth. As Kaldor put it: “… the important conclusion is that the signal that causes an economic ‘agent’ to do something different – produce more or produce less, or switch his manufacturing facilities from some varieties to others – is always a quantity signal, not a price signal. … In the actual adjustment of supply and demand, prices play only a very subordinate role, if any.” When I attended the University of Cambridge in 1974-75, I read Keynes, met Piero Sraffa, listened to Joan Robinson, and studied with Kaldor, Luigi Pasinetti, Richard Goodwin, Ajit Singh, Wynne Godley, Robin Marris, and Adrian Wood. Back then, it was understood at Cambridge that markets do nothing like what Coyle claims they do. Just as Einstein had erased Euclid’s axiom of parallels, Keynes’s General Theory had long since obliterated the supply curves for labor and saving, thereby eliminating the supposed markets for labor and capital. It followed that the prices of production were set by costs (mostly labor costs and interest rates), while quantities were determined by effective demand. Markets were not treated as if they were magical. It was obvious that most resources and components did not move under the influence of an invisible hand. Rather, they moved according to contracts between companies on terms set by negotiation, as had been the case for more than a hundred years. Technology was managed by organizations – mostly by large corporations – in what was sometimes called “the new industrial state.” But the Cambridge school of economics that understood these things has died out. It was targeted in the great intellectual purge of the Thatcher era, and it was pried from its footholds in North America by early-stage McCarthyism, Reaganism, the MIT self-proclaimed Keynesians, and the Chicago School. Only a few scattered survivors remain today. The Return of Increasing Returns “Economics,” Coyle correctly argues, “needs to have at its heart increasing returns and the kind of dynamics they imply. The characteristics of a knowledge economy are distinctive.” But this “vibrant area of research … is not yet the mainstream benchmark, and still less so in the lecture hall or the corridors of power.” As it happens, here is Kaldor on the same topic: “The progress of knowledge … is very often the result gained from experience – learning by doing. And as the great American economist, Allyn Young, emphasized in his famous paper ‘Increasing Returns and Economic Progress,’ published shortly before his early death in the winter of 1928-1929 – a paper which for reasons that are not clear to me did not have the influence in his native country that it so clearly deserved – once we allow for increasing returns the laws of economics take on quite a different appearance.” Young saw almost a century ago, and Kaldor emphasized 40 years back, that increasing returns generate cumulative causation: the advancing gains of leaders over laggards produce increasingly extreme inequalities and disequilibrium. Ultimately, these issues lead us right back not only to Keynes but also to his “circus” of peers such as Kaldor, Sraffa, and Robinson. These earlier Cambridge economists did not develop “forecasting models,” and macro-policy prescriptions were, for them, a sideline. They and their successors (above all Pasinetti, who continues to publish in his 90s), practiced a unified theoretical economics that encompassed money, banking, production, employment and unemployment, market power, international trade, industrial corporations, and technological change. Coyle’s belief in a distinctive applied microeconomics based on markets and price signals is an artifact of the “neoclassical synthesis” constructed in post-war Cambridge. It was here – in Cambridge, Massachusetts – that MIT and Harvard economists bifurcated the discipline, reduced Keynes’s thinking to formulas, and set the stage for the Chicago cult of rational “microfoundations.” Cambridge has forgotten Cambridge, and it is poorer for it. Galbraith (2022) What’s Left of Cambridge Economics? 55.4 Mario Draghi Judah on Darghi There is a Europe of the mind: of Beethoven, summer holidays and the smell of coffee. Then there is Europe as it actually functions today — the Europe of Mario Draghi. Understand Draghi and you understand how power works in the EU. He has built a technocratic Europe and risen to its heights. Whilst his generation was wild, flirting with extremism and dreaming of new worlds on campus, Draghi was tame and burdened by responsibility. An outsider in May ’68. He was well trained by the Jesuits. They taught him to be prudent, reserved and to listen. He’s a social Catholic. It is a class marker that inexorably ties him to Massimiliano Massimo, the Jesuits’ Roman Eton, where Draghi studied with the sons of ministers and tycoons. t is the sign of a severe, rigorous education at the hands of scholar-priests; and it is privilege. For Europeans, it is often a way to draw attention to his manner: pedagogic; precise, shadowy and, if necessary, ruthless. The Jesuits have a mantra from their founding Saint Ignatius of Loyola about serving the vision of God: todo modo, which in English means whatever it takes. As a wave of political killings followed ’68, Draghi learned the first lesson of political life. Always find the right mentor. His name was Frederico Caffè. Amidst the clamour he lived, his students said, “like a monk”. Caffè was influential: Italy’s great Keynesian economist. Convinced Draghi was brilliant, he introduced him to Franco Modigliani, the Italian economist at MIT, who accepted him as a student. But he still had to complete his thesis. “It was on the single currency and I concluded that the single currency was madness, something absolutely not to do,” said Draghi, at an event honouring his mentor. Those who would shape the economic discourse of the age taught Draghi at MIT. He proudly points out that five of his professors won Nobel Prizes — Paul Samuelson, Bob Solow, Franco Modigliani, Peter Diamond, and Robert Engle. His peers — Ben Bernanke, Paul Krugman, Kenneth Rogoff, Olivier Blanchard — would become high priests of the Federal Reserve, New York Times, austerity and the IMF respectively. As the new world of floating exchange rates, free flowing capital and empowered central bankers was starting to emerge, a circle of economists was coalescing. Together they shaped the neoliberal age. Draghi was not looking for dogma. Unlike his mentors, Draghi’s economics has never set into a theory but has kept moving, always one point to the left from wherever the centre is. He sees it as pragmatism. By forty, he had disappointed the left wing Caffè. Draghi was now a Director at the World Bank. In April 1987, overtaken with grief that neoliberalism has triumphed over the left in economics, his disciples dead or fading, Caffè, the great Keynesian, disappeared. He was never seen again. Some say he committed suicide; others that he had taken himself to a monastery in the Alps, to hide from the world he saw coming. Heading the Treasury from 1991; it was here that the fortysomething civil servant did whatever it took to join the single currency: regulating Italy’s banks, managing its debt and privatizing over €100bn. Draghi was more than indispensable. He built Italian neoliberalism. There was no better school than Rome for Euro politics: it was already a game of weak politicians and powerful technocrats Capitalism, he believed, had rules. As long as the politicians got out of the way and the technocrats set the right structure, stable growth would follow. This was the MIT philosophy. Across continents, his former student peers were rising and rising. As economists they believed in intervention: to help make the market function. This was why the Euro was imperative. Capitalism could provide the rules — and the structure — that Italy lacked. Politicians would now be curtailed in macroeconomic policy. Signing up to a single currency put the fundamental levers of the macroeconomy — key fiscal and monetary policies — beyond domestic politics. This strategy was known as il vincolo esterno, the external bind. This is what elites feared in the 1990s: without il vincolo, a return to the 1970s. Draghi’s generation believed they had got it all right. Then 2008 hit. The financial crisis revealed these engineers had made a terrible mistake. They had broken a system they would now spend the rest of their careers trying to fix. This would transform central bankers from the technocratic rule setters of capitalism into the political crisis managers who guided it — and in so doing reorder power in the EU forever. By signalling that Frankfurt was only prepared to put its liquidity behind a certain type of politics he opened the door to ousting Berlusconi. A technocratic government replaced him — which the fallen leader called an EU “coup”. “In every press conference since I became ECB President, I have ended the introductory statement with a call to accelerate structural reforms in Europe.” Central Bankers had crossed the line: no longer technocrats, they were now politicians. In Frankfurt, Draghi would master the three modes of European power: the charismatic — the politics of persuasion — with which he would claim power for his bank; the technical — the politics of rules — with which he would be the EU enforcer in Greece; and the analytical — the politics of numbers — with which he would win the battle to guide capital flows with quantitative easing. Together these would come together into Draghipolitik — with which he would move the German dial. His challenge was in the very design he agreed to. Genuinely free markets, which had opened in the 1970s with the lifting of capital controls, closed. Directed capitalism came to Europe with the ECB incentivising markets to buy riskier assets by buying over $2.8 trillion safer ones by 2018. It was the ultimate act of intervention without redistribution. A whisperer, an enforcer, a number cruncher. These are not the qualities one expects of a great man. But that is to misunderstand how the EU works. Its machine was built to depoliticise politics; and those who do that best, thrive; the unassuming bureaucrat becomes Napoléon. As the crisis made the state more dependent on finance; finance become more dependent on the state. And men like Draghi were central to it. These victories reveal enormous skill. They turned the ECB into an even more powerful institution than the Bank of England. But they also underline how badly his generation got it wrong. They had bet on a half-built house for Europe as the key to stability. But a monetary union without a fiscal union brought instability. They had bet on setting neoliberal rules for capitalism and stepping back: and it had blown up. They had bet on austerity: then faced a depression. These errors made them — the world’s elite central bankers who then had to fix it all — more powerful than most politicians. All through life, Draghi’s personal and political bets have paid off. But at the same time, his great bet, the one he took with Italy — il vincolo esterno — has failed. The geopolitics failed: it has not helped manage German power. The economics failed: Italy has maintained one of the toughest fiscal regimes in Europe, running a primary surplus almost every year since 1995. Yet Italy has got poorer. In 2000, its average standard of living was 98.6 per cent of that of Germany. Today, Italian per capita income is 20 per cent below those over the Alps. These are the long term consequences of austerity, bottled reform and the Euro making exports uncompetitive. The debt Italy ran up in the 1980s has become its albatross. Draghi’s growth never came. Italy has been trapped in a loop of increasingly weak populists, punctuated by weak technocrats. “The truth,” said the historian Marcel Gauchet, “is that Europeans do not know what they have built.” This is what the struggles of Draghi reveal. The real politics of Europe is the politics of Eurozone debt. A Euro collapse is now unlikely. That is his legacy. The risk that Europe faces now is that the Euro system — the unfinished house — slowly does to the EU as a whole what it has done to Italy, putting it on a permanently lower growth trajectory. The EU needs collectivised debt for more collective stimulus. Until someone can pull off the next painful step of consolidation, the risk is that the Union continues to lose the battle for globalisation. But the price of Draghipolitik is this: it is consolidation without democracy. Empowered elites with alienated voters. Politics only men like him can play. But what is this Europe? This system, Draghi’s system, is one that has depoliticised itself in order to survive. That it has. But at the cost of no longer being able to distinguish between stability and stagnation. A system that can only do the bare minimum. Not whatever it takes. Judah on Draghi 55.5 James Galbraith Consider what has happened, in recent years, to five of the leading ideas of modern economics. Inflation is everywhere and always a monetary phenomenon. This dictum is the most famous single thought associated with Milton Friedman. It was once, briefly in the early 1980s, the driving philosophy of the Federal Reserve. Its architect, and many of his students, have won the Nobel Prize. But in practice, monetarism has been completely, silently abandoned. Measures of money (notably M2) have been growing rapidly for years, with no inflationary effect. Monetarism as such is, today, an academic dead letter. There wasn’t one monetarist topic on the AEA’s calendar this year, and a new academic monetarist hasn’t emerged in decades. And yet, the signal policy achievement of the monetarist movement remains intact. Thirty years ago, Friedman-style monetarists wiped out all alternative theories of inflation. The ideas of “cost push” and “wage-price spirals,” on which the successful anti-inflation strategies of the 1960s had been based, disappeared. To this day, there exist no alternatives for fighting inflation, except higher interest rates, recession, and unemployment. These are the hard measures, the brutal measures, for which we have the monetarists to thank. Full employment without inflation is impossible. Four years ago, virtually all “serious” economists, including many self-described Keynesians, agreed: There existed a “natural rate of unemployment.” This was in the vicinity of 6 percent, and below it inflation was certain to rise. The number, it turns out, had no basis in serious study; it was first made up by Robert J. Gordon as an illustration for his textbook. Since that time, unemployment has been continuously below 6 percent, without rising inflation. It is now almost exactly 4 percent, the formal target of the Full Employment Act. Faced with the embarrassing facts, only a handful of economists continue to defend the natural rate idea. And yet, the natural rate movement still influences policy. Some of its survivors vote on the Federal Reserve’s Open Market Committee. They are presently driving interest rates upward on precisely the pretext that low unemployment must otherwise soon bring rising inflation. It is a notion for which no evidence exists. And except for the damage that higher interest rates will do, it would be hard not to laugh. Rising pay inequality stems from technological change. “Skill-biased technological change” became in the 1990s the profession’s pet rationale for the splitting apart of the pay structure. Translation: The “markets” were rewarding those talented and farsighted enough to acquire new skills, particularly in the computer age. This position is now dismissed by all with a serious grip on the facts. Among other things, the rise in pay inequalities, which had not been timed carefully in the first studies, occurred largely before the wide distribution of personal computers. And the theory cannot account at all for declining pay inequalities after 1994, just when the diffusion of computers and information technologies was speeding up. And yet, the notion that education can cure the inequality problem remains a staple of economics teaching. It also remains the central policy approach to inequality of “third way” politicians in the United States and Europe, including President Clinton. Once again, conventional policy thought lingers on, even as the research fad has faded out. Rising minimum wages cause unemployment. A furious fight on this issue ensued as recently as 1995 when two distinguished researchers, Alan Krueger of Princeton and David Card of the University of California, Berkeley, broke ranks to declare that the evidence contradicted this thesis. Since then, the minimum wage has gone up twice, and unemployment has continued to decline. Card and Krueger were right–and so was their fundamental criticism of basic labor market theory. And yet, you will not find more than a grudging acknowledgment of this in economics textbooks, virtually all of which will continue to teach false propositions to new generations of students. Nor have labor market economists thrown their professional weight behind a rising minimum wage. Sustained growth cannot exceed 2.5 percent per year. This lulu was a compound of two errors: the idea that productivity growth was fixed, by mysterious forces, at less than 1.5 percent, and the idea that the growth of the labor force could not long exceed another percent or so. But it turns out that productivity growth picks up when unemployment is low (one entirely sensible reason being that businesses make better use of labor). And it also turns out that there are more potentially employable people out there, after three decades of policy-imposed stagnation, than the economists thought. Even at 4 percent measured unemployment, the economy has been zipping along at 3.5 percent growth or better for several years. In fact, present dangers to growth come very much more from unfounded worries about capacity constraints and labor shortages than from the constraints and shortages themselves. Economically Correct The evidence flatly contradicts each of the five dogmas I have just listed. Why is this so? The reason is fairly clear. Leading active members of today’s economics profession, the generation presently in their 40s and 50s, have joined together into a kind of Politburo for Correct Economic Thinking. As a general rule–as one might expect from a gentleman’s club–this has placed them on the wrong side of every important policy issue, and not just recently but for decades. They predict disaster where none occurs. They deny the possibility of events that then happen. The prevailing theory is the idea that price and quantity are set in free competitive markets through the interaction of supply and demand. It is this idea, and no other, that lies at the core of the economist’s way of thinking. And it is also the source of the profession’s problem in getting almost anything important right. Galbraith (2001) How the Economists Got It Wrong 55.6 John Maynard Keynes Keynes famously said of politics’ tendency to follow economics with a lag: ‘Practical men, who believe themselves to be quite exempt from any intellectual influence, are usually the slaves of some defunct economist. Madmen in authority, who hear voices in the air, are distilling their frenzy from some academic scribbler of a few years back.’ On Scarcity Extinction (Lanchester) John Maynard Keynes’s famous 1930 essay “The Economic Possibilities for Our Grandchildren.” Keynes speculated that if the world continued to get richer we would naturally end up enjoying a high standard of living while doing much less work. He thought that “the economic problem” of having enough to live on would be solved, and “the struggle for subsistence” would be over: When the accumulation of wealth is no longer of high social importance, there will be great changes in the code of morals. We shall be able to rid ourselves of many of the pseudo-moral principles which have hag-ridden us for two hundred years, by which we have exalted some of the most distasteful of human qualities into the position of the highest virtues. We shall be able to afford to dare to assess the money-motive at its true value. The love of money as a possession—as distinguished from the love of money as a means to the enjoyments and realities of life—will be recognized for what it is, a somewhat disgusting morbidity, one of those semi-criminal, semi-pathological propensities which one hands over with a shudder to the specialists in mental disease. The world has indeed got richer, but any such shift in morals and values is hard to detect. Money and the value system around its acquisition are fully intact. Greed is still good. The study of hunter-gatherers, who live for the day and do not accumulate surpluses, shows that humanity can live more or less as Keynes suggests. It’s just that we’re choosing not to. A key to that lost or forsworn ability, Suzman suggests, lies in the ferocious egalitarianism of hunter-gatherers. Lanchester (2017) The Case against Civilization 55.7 Paul Romer NY Times on Paul Romer Paul Romer was once Silicon Valley’s favorite economist. The theory that helped him win a Nobel prize — that ideas are the turbocharged fuel of the modern economy — resonated deeply in the global capital of wealth-generating ideas. In the 1990s, Wired magazine called him “an economist for the technological age.” The Wall Street Journal said the tech industry treated him “like a rock star.” Not anymore. Today, Mr. Romer, 65, remains a believer in science and technology as engines of progress. But he has also become a fierce critic of the tech industry’s largest companies, saying that they stifle the flow of new ideas. He has championed new state taxes on the digital ads sold by companies like Facebook and Google, an idea that Maryland adopted this year. And he is hard on economists, including himself, for long supplying the intellectual cover for hands-off policies and court rulings that have led to what he calls the “collapse of competition” in tech and other industries. “Economists taught, ‘It’s the market. There’s nothing we can do,’” Mr. Romer said. “That’s really just so wrong.” Mr. Romer’s current call for government activism, he said, reflects “a profound change in my thinking” in recent years. It also fits into a broader re-evaluation about the tech industry and government regulation among prominent economists. They see markets — search, social networks, online advertising, e-commerce — not behaving according to free-market theory. Monopoly or oligopoly seems to be the order of the day. Of all the economists now taking on big tech, though, Mr. Romer is perhaps the most unlikely. He earned his undergraduate and doctoral degrees from the University of Chicago, long the high church of free-market absolutism, whose ideology has guided antitrust court decisions for years. Mr. Romer spent 21 years in the Bay Area, mostly as a professor first at Berkeley and then Stanford. While in California, he founded and sold an educational software company. In his research, Mr. Romer uses software as a tool for data exploration and discovery, and he has become an adept Python programmer. “I enjoy the solitary exercise of building things with code,” he said. “People I like are frequently unhappy with me,” he said. Mr. Romer, who joined the faculty of New York University a decade ago, said that preparing for his Nobel lecture in 2018 prompted him to think about the “progress gap” in America. Progress, he explained, is not just a matter of economic growth, but should also be seen in measures of individual and social well-being. n the United States, Mr. Romer saw worrying trends: a decline in life expectancy; rising “deaths of despair” from suicides and drug overdoses; falling rates of labor participation for adults in their prime working years, from 25 to 54; a growing wealth gap and increasing inequality. Such problems, to be sure, have many causes, but Mr. Romer believes one contributing cause has been an economics profession that belittled the importance of government. His new growth theory recognized that the government played a vital part in scientific and technological progress, but mainly by funding basic research. Looking back, Mr. Romer admits that he was caught up in the “small government bubble” of the time. “I substantially underestimated the role of the government in sustaining progress,” he said. “For real progress, you need both science and government — a government that can say no to things that are bad,” Mr. Romer said. NY Times 55.8 Frederick Soddy Reinert The Nobel Prize winner that predicted a crisis between nature and capital. A scientist who used much of his time on economics was rewarded a Nobel Prize in 1921. Admittedly, Frederick Soddy (1877–1956) received the prize in chemistry, for his work on radioactivity. But in the period from 1921 to 1934 Soddy wrote four books campaigning for a radical restructuring of the global monetary system. ‘There is no wealth but life’ is the basic message. Placing money as a kind of enemy for humankind. Here is a new type of economics: we have standard neoclassical economics, based on the metaphor of equilibrium between supply and demand, and we have evolutionary (Schumpeterian) economics based on a metaphor from biology (innovations as mutations). Soddy offered us a third angle: economics rooted in physics, in the laws of thermodynamics. Humans survive, he wrote, based on the use of natural resources. If these resources are exhausted, we shall be in deep trouble. At the time Soddy was not taken seriously, but he is now seen as a forerunner for ecological economics. Romanian-born economist Nicholas Georgescu-Roegen (1906–1994) continued working in this tradition. Soddy points to the fundamental difference between the biophysical resources and consumables — what he calls ‘real wealth’ — that are subject to the laws of thermodynamics. This wealth will rot, rust, wear out, or be consumed. Money and debt — which he calls ‘virtual wealth’ — are only subject to the laws of mathematics. Money can grow without limits, whereas the real economy cannot. In this mismatch, says Soddy, lies the roots of most of our economic problems. in a very informative New York Times op-ed in 2009, US ecological economist Eric Zencey (1953–2019) notes that Frederick Soddy had distilled his vision into five policy prescriptions, of which four since have become conventional wisdom: to abandon the gold standard, to let international exchange rates float, to use federal surpluses and deficits as macroeconomic policy tools that could counter cyclical trends, and establish bureaus of economic statistics (including a consumer price index). Soddy’s fifth proposal — the only one that remains outside today’s bounds of conventional wisdom — was to stop banks from creating money (and debt) out of nothing. Reinert (2021) The Nobel Prize winner that predicted a crisis between nature and capital 55.9 Dennis Snower Behavioural Economics can’t fix it This is probably the most exciting and fruitful time ever to become an aspiring economist. Why? Because economics is reaching its Copernican Moment – the moment when it is finally becoming clear that the current ways of thinking about economic behavior are inadequate and a new way of thinking enables us to make much better sense of our world. It is a moment fraught with danger, because those in power still adhere to the traditional conventional wisdom and heresy is suppressed. Behavioral economics began as a compendium of “anomalies” that the neoclassical system could not explain. Some of these anomalies have been addressed by behavioral theories such as prospect theory or social preference theory, but many have not. Different theories explain different anomalies; there is no overarching theory to explain them all. And since behavioral economics is devoted primarily to individual fixes, it has retained many of the basic axioms above, such as methodological individualism, consumption as central for wellbeing, understanding economic events in terms of probability theory and the tendency toward equilibrium. However, these axioms are also open to question. Regarding methodological individualism, who says that the individual is the only level of selection? After all, Homo Sapiens owe their evolutionary success largely to their ability to cooperate with one another, in larger number than other mammals. Regarding consumption as central to wellbeing, who says that our material appetitive needs dwarf our social needs, such as the need to care and be cared for, or the need to belong to a community, or the need to shape your fate through your own efforts? Regarding our ability to understand economic events in terms of probability theory, who says that we can imagine all conceivable future states of the world and that we can assign probabilities to each of them? After all, many of the most important events that young people look forward to in the future — whom they will marry, where they will live, what jobs they will get, how much they will earn, what their state of health will be, when they will retire, how long they will live — are simply unknown unknowns. Not only has the neoclassical system encountered endless discrepancies between predictions and evidence and thus has accumulated endless fixes, but it also has had little success in addressing the great economic questions of our time. For example: If the free-market system is meant to satisfy our needs efficiently, why is it despoiling our environment? Why is it generating inequalities and other inequities that threaten the social cohesion of our societies? Why does it leave so many people economically insecure, vulnerable to unemployment and trapped in dead-end jobs? Why does it not correct for the excesses of consumerism, workaholism and digital addictions, frequently leading to anxiety, depression, burnout, substance abuse and crime? Why is it giving us so little guidance in promoting public compliance with social distancing rules during the Covid-19 pandemic, even though such compliance has economic causes and consequences? Why does it keep so many businesses focused on short-term profit and shareholder value, even though so many business leaders are genuinely concerned about the environment and the wellbeing of their customers and employees? Now the practitioners’ patience with mainstream economics is wearing thin. Unlike the academic economists, the practitioners must actually address the great economic questions of our time. Nor can the practitioners be content with the economists’ standard policy toolbox, since these instruments are obviously not overcoming the growing problems of climate change, social conflict, “deaths of despair,” containment of the Covid-19 pandemic, and much more. And finally, the practitioners are no longer enamored by the mainstream narrative on the division of responsibilities. Consumers in their millions are taking an interest in the social, political and environmental consequences of consumption and production activities, school children are out in the streets in protest about climate change, international organizations are beginning to measure economic performance beyond GDP (such as through the OECD’s Better Life Index and the UN’s Sustainable Development Goals), businesses are beginning to measure business performance beyond shareholder value (such as through Environmental, Social and Governance criteria along with the initiatives of the WEF International Business Council, the OECD Business for Inclusive Growth coalition, the Value Balancing Initiative, the British Academy’s Future of the Corporation programme), national governments are beginning to design budgets with regard to notions of wellbeing that extend beyond consumption of goods and services (such as New Zealand’ wellbeing budget). In short, the practitioners are not waiting for the mainstream economics profession to adjust to reality; instead, they are forging ahead on multiple fronts, extending the domain of economics to the existential challenges we face. Fortunately, we now have access to a powerful body of thought that can guide this new encounter. The evolution of our natural world can be understood in terms of variation, replication and selection. The evolution of ideas can be understood in such terms as well: new ideas keep cropping up; they are transmitted from person to person; and the ideas that get selected to survive are often to be ones that enable us to navigate our environment most effectively. Selection can act not only on individuals, but also on groups. “Selfishness beats altruism within groups. Altruistic groups beat selfish groups. Everything else is commentary.”(E.O. Wilson and D.S. Wilson (2007), “Rethinking the Theoretical Foundations of Sociobiology,” Quarterly Review of Biology, 82(4), 327-348) The level of functional organization thus depends on the relative strength of within- and between-group selection. This is a different starting point from the one underlying mainstream economics. The discipline of economics is based on classical physics, i.e. the inanimate world. Evolution, by contrast, is appropriate to the animate world. Not a bad point of departure for economics. After all, humans are living creatures. If we choose this path, economics will be reaching its Darwinian – not Copernican – Moment. This is why now is probably the most exciting and fruitful time ever to become an aspiring economist. Snower 55.10 Adam Smith Richard Smith Adam Smith’s economics is an idea whose time has passed. Specialization, planless, anarchic production for market, single-minded pursuit of profit maximization at the expense of all other considerations, was the driving engine that generated the greatest advances in industrial and agricultural productivity, and also the greatest accumulation of wealth the world has ever seen. But that same engine of development, now immensely larger and running at full throttle, is overdeveloping the world economy, overconsuming the world’s resources, flooding the world’s waters and atmosphere with toxic and warming pollution, and propelling us off the cliff to ecological collapse, if not extinction. Adam Smith’s fatal error – fatal for us – was his assumption that the “most effectual” means of promoting the public interest, the common good of society, is to just ignore it and focus exclusively on the pursuit of individual economic self-interest. Even with respect to the public interest of the economic welfare of society, Smith’s thesis that the invisible hand of the market would automatically bring about “universal opulence which extends itself to the lowest ranks of the people” as “a general plenty diffuses itself through all the different ranks of the society” could hardly have been more mistaken. Two-and-a-quarter centuries after Smith wrote, global capitalist development has produced the most obscenely unequal societies in history. Two-and-a-quarter centuries after Smith wrote, global capitalist development has produced the most obscenely unequal societies in history, with half the world living on less than two dollars a day, billions of people living in desperate poverty, many times more than the entire population of the world in Smith’s day, while a tiny global elite, even just a few hundred individuals, concentrate an ever-growing share of the world’s wealth, which they lavish on “opulence” on a hitherto unimagined scale. On this breath-taking failure of social scientific prediction alone, Smith’s economic theory ought to have been ridiculed and drummed out of the profession long ago, as such a comparable predictive failure would have been in the natural sciences. With respect to the public interest of broader societal concerns, which today would include the environment, Smith’s philosophy of economic individualism as the means to maximize the public interest – the common good of society – is not only completely wrongheaded, it’s suicidal. And it is completely at odds with the world’s scientists and scientific bodies who are crying out for a plan – a plan to stop global warming, to save the forests, to save the fisheries, to stop ocean acidification, to detoxify the planet, to save the thousands of creatures from extinction, etc. Leaving the global economy in the hands of private corporations, subject to the demands of the market, is the road to collective eco-suicide. Richard Smith (2015) Green Capitalism (pdf) Noah Smith on Adam Smith There’s a sort of popular myth that economics began with Adam Smith’s declaration that the “invisible hand” of the market would lead to a good society. In fact, while Smith did recognize the importance of market forces and self-interest, his vision of a good society didn’t stop there. Here are some Adam Smith quotes: “Our merchants and masters complain much of the bad effects of high wages in raising the price and lessening the sale of goods. They say nothing concerning the bad effects of high profits. They are silent with regard to the pernicious effects of their own gains.” “It is not very unreasonable that the rich should contribute to the public expense, not only in proportion to their revenue, but something more than in that proportion.” “No society can surely be flourishing and happy of which by far the greater part of the numbers are poor and miserable.” “Wherever there is great property there is great inequality. For one very rich man there must be at least five hundred poor, and the affluence of the few supposes the indigence of the many.” “People of the same trade seldom meet together, even for merriment and diversion, but the conversation ends in a conspiracy against the public, or in some contrivance to raise prices.” And so on. Adam Smith decries the existence of inequality and poverty, blames property rights for this inequality, advocates progressive taxation as a remedy, and is innately suspicious of profit. He sounds more like Thomas Piketty than Milton Friedman. Smith’s suspicion of profit and enthusiasm for redistribution are baked into the very core of economic theory. The zero-profit condition says that in a well-functioning market, the rate of profit should be no more than the cost of capital — if you see companies making big margins, you should suspect that the market isn’t working right. This is the basis of the antitrust movement, which is again gaining strength in America with the appointment of Lina Khan to chair the FTC. Though there are a few populist firebrands in the antitrust movement, much of it is an intellectual movement driven by economists. Meanwhile, Smith’s call for redistribution is inherent in the Second Welfare Theorem, considered one of the basic theorems of economics — and something that every intro student is taught. The Second Welfare Theorem says that if you change the initial distribution of wealth in society, you can basically get any outcome you like. This puts the burden of proof on those who think we shouldn’t redistribute wealth — it forces them to bring proof that the harms from taxation are just too high. Though there have been some economists who opposed redistribution, enthusiasm for the idea is traditionally very dominant within the profession. Even Milton Friedman, that great champion of laissez-faire, supported the idea of a negative income tax that would give people more cash the poorer they were. And though economists do generally believe that very high taxes have some costs, a 2013 survey found that 97% of economists favored federal tax hikes, compared to only two-thirds of the general public, and a 2020 survey finds that most economists think raising the top marginal rate wouldn’t hurt economic growth. Noah Smith (2015) Is economics an excuse for inaction? Austin on Smith In 1714’s The Fable of the Bees – among the first panegyrics to the market system – Bernard de Mandeville emphasized the market’s seemingly magical power to transmute the individual ‘Vice’ of greed into the ‘Virtue’ of greater good. Not only did the market have the power to neutralize greed, but it also positively required greed as, in modern terms, the multiplier of effective demand and hence the driver of the economy overall. 73 De Mandeville’s commendation of greed met strenuous and widespread objection. John Wesley, the contemporary theologian, condemned Mandeville as a latter-day Machiavelli: ‘…till now I imagined there had never been in the world such a book as the works of Machiavel. But de Mandeville goes far beyond it.’ 74 But events took their course, with the practical benefits of markets asserting themselves, such that Adam Smith – 60 years later! – could offer a more palatable account of market dynamics. Mandeville’s ‘vice’ became ‘self-love’ and ‘self-interest’ in Smith’s telling. Where Mandeville had been the radical breaking new ground, Smith had the luxury of placing a professorial seal on the matter for an audience already won over. At the heart of this shift was a major cultural reappraisal of the character of ‘greed’ – or ‘Vice’ or ‘self-interest’ or ‘self-love’. Over a relatively short period, human culture flipped from a narrative of ‘greed is bad’ to an exciting new hypothesis: ‘greed might be OK, you know’. Over time, conviction would grow. By 1987, of course, ‘greed was good’. Austin (2021) Market-led Sustainability is a ‘Fix that Fails’… (pdf) 55.11 Milton Freeman Richard Smith Adherents of the Chicago school simply deny that there is any environmental problem, certainly none that the market can’t solve. Thus, in a 1991 interview, Milton Friedman ridiculed environmentalists with his trademark condescending and nasty vitriol: “The environmental movement consists of two very different parts. One is the traditional conservation groups, who want to save resources et cetera. The other is a group of people who fundamentally aren’t interested in conservation at all, and who aren’t primarily interested in pollution. They’re just long-term anti-capitalists who will take every opportunity to trash the capitalist system and the market economy. They used to be communists or socialists, but history has been unkind to them, and now all they can do is complain about pollution. But without modern technology, pollution would be far worse. The pollution from horses was much worse than what you get from automobiles. If you read descriptions of the streets of New York in the nineteenth century…” And in his sadoeconomic screed Free to Choose, the anti-communist warhorse complained that: “…whatever the announced objectives, all of the movements of the past two decades—the consumer movement, the ecology movement, the back-to-the-land movement, the hippie movement, the organic food movement, the protect-the-wilderness movement, the zero-population-growth movement, the ‘small is beautiful’ movement, the antinuclear movement—have always had one thing in common. All have been antigrowth. They have been opposed to new developments, to industrial innovation, to the increased use of natural resources. Agencies established in response to these movements have imposed heavy costs on industry after industry…” [and so on]. Friedman’s redneck eco-know-nothingism has long defined the far-right wing of US economic theology but his confident assumption that endless growth is sustainable is shared by the entire profession of mainstream economists. Richard Smith (2015) Green Capitalism (pdf) 55.12 Paul Krugman Richard Smith If we look at the far-left extreme of acceptable economic thought, say Paul Krugman, we hear the same “can’t stop progress” mantra: writing in the New York Times Krugman wonders “if there isn’t something a bit manic about the pace of getting and – especially – spending in fin-de-siècle America”: “But there is one very powerful argument that can be made on behalf of recent American consumerism: not that it is good for consumers, but that it has been good for producers. You see, spending may not produce happiness, but it does create jobs, and unemployment is very effective at creating misery. Better to have manic consumers American style, than the depressive consumers of Japan… There is a strong element of rat race in America’s consumer-led boom, but those rats racing in their cages are what keep the wheels of commerce turning. And while it will be a shame if Americans continue to compete over who can own the most toys, the worst thing of all would be if the competition comes to a sudden halt.” Paul Krugman is a brilliant economist but the Smithian premises of his theoretical framework cannot allow that we could actually run out of resources to make all those toys. Richard Smith (2015) Green Capitalism (pdf) 55.13 Herman Daly Richard Smith Beyond growth or beyond capitalism? Recent publications have revived interest in Herman Daly’s proposal for a Steady- State Economy. This paper argues, first, that the idea of a steady-state capitalism is based on untenable assumptions, starting with the assumption that growth is optional rather than built- into capitalism. I argue that irresistible and relentless pressures for growth are functions of the day-to-day requirements of capitalist reproduction in a competitive market, incumbent upon all but a few businesses, and that such pressures would prevail in any conceivable capitalism. Secondly, this paper takes issue with Professor Daly’s thesis, which also underpins his SSE model, that capitalist efficiency and resource allocation is the best we can come up with. I argue that this belief is misplaced and incompatible with an ecological economy, and therefore it undermines Daly’s own environmental goals. I conclude that since capitalist growth cannot be stopped, or even slowed, and since the market-driven growth is driving us toward collapse, ecological economists should abandon the fantasy of a steady-state capitalism and get on with the project figuring out what a post–capitalist economic democracy could look like. Capitalism without growth? In the 1970s and 80s, Herman Daly launched a broadside assault on the academic discipline of economics assailing its dogmatic and neo-totalitarian embrace of neoclassical economics and its willful blindness to our looming environmental crisis. In ground-breaking and widely influential books and articles Daly assailed the “stupor of economic discourse” by holding up to his colleagues what he called the “wild facts” of our ecological crisis: the growing hole in the ozone shield, the alarming evidence of rising CO2 levels, the shocking rates of natural resource consumption, the frightening rates of extinction and loss of biodiversity and so on, which mainstream economists ignored (and most continue to ignore to this day). The ecological crisis is caused, Daly argued, by too much growth: “the scale of human activity relative to the biosphere has grown too large” and most especially, by ever- growing consumption in the advanced industrialized countries. Daly attacked the mainstream’s “idolatrous” “religion of growth,” its “growthmania,” its “fetish” of limitless consumption. 13 Daly’s critique of the neoclassical defense of growth is probably the most devastating critique to come from within the profession. But despite his “radical” break with the mainstream’s fetish of growth, Daly did not at all break with his colleagues’ fetish of the market organization of production, the capitalist market economy. On the contrary. His proposal for a Steady-State Economy was based, he said, “on impeccably respectable premises: private property, the free market, opposition to welfare bureaucracies and centralized control.” So in his Steady-State model, Daly embraces capitalism but he rejects the consequences of market-driven economic development, especially overconsumption and environmental destruction. For more than 30 years Daly has chanted his mantra of “development without growth” but he has yet to explain, in any concrete way, how an actual capitalist economy comprised of capitalists, investors, employees and consumers could carry on from day to day in “stasis”. Daly rejects any such interference with market organization of production because, like his mainstream colleagues, he believes that “the market is the most efficient institution we have come up with” and the only option we have. 38 He can say this because he subscribes to a capitalist conception of efficiency. Capitalist economists since Adam Smith have defined economic efficiency from the standpoint of the production unit – the factory, mill, mine, etc. (which, conveniently, the capitalists own). So in capitalist terms, the most efficient production method, technology, or economic system is the one that gets the most output from the least input, so produces the cheapest widgets and generates the most product/sales/wealth for a given investment of labor and raw materials. So Daly says the market “is wonderful for allocation”. “Markets singlemindedly aim to serve allocative efficiency.” Richard Smith (2015) Green Capitalism (pdf) 55.14 Wassily Leontief Leontief An uneasy feeling about the present state of our discipline has been growing in some of us who have watched its unprecedented development over the last three decades. This concern seems to be shared even by those who are themselves contributing successfully to the present boom. They play the game with professional skill but have serious doubts about its rules. The trouble is caused, however, not by an inadequate selection of targets, but rather by our inability to hit squarely any one of them. The uneasiness of which I spoke before is caused not by the irrelevance of the practical problems to which present day econo- mists address their efforts, but rather by the palpable inadequacy of the scientific means with which they try to solve them. Tthe consistently indifferent performance in practical applications is in fact a symptom of a fundamental imbalance in the present state of our discipline. The weak and all too slowly growing empirical foundation clearly cannot support the proliferating superstructure of pure, or should I say, speculative economic theory. Much is being made of the widespread, nearly mandatory use by modem eco- nomic theorists of mathematics. To the extent to which the economic phenomena possess observable quantitative dimen- sions, this is indisputably a major forward step. Unfortunately, any one capable of learning elementary, or preferably advanced calculus and algebra, and acquiring acquaintance with the specialized terminology of economics can set himself up as a theorist. Uncritical enthusiasm for mathematical formulation tends often to conceal the ephemeral substantive content of the argument behind the formidable front of algebraic signs. In the presentation of a new model, attention nowadays is usually centered on a step-by-step derivation of its formal properties. But if the author—or at least the referee who recommended the manu- script for publication—is technically com- petent, such mathematical manipulations, however long and intricate, can even with- out further checking be accepted as correct. Nevertheless, they are usually spelled out at great length. By the time it comes to interpretation of the substantive conclusions, the assumptions on which the model has been based are easily forgotten. But it is precisely the empirical validity of these assumptions on which the useful- ness of the entire exercise depends. What is really needed, in most cases, is a very difficult and seldom very neat assessment and verification of these assumptions in terms of observed facts. Here mathematics cannot help. An attempt to compensate for the glaring weakness of the data base available to us by the widest possible use of more and more sophisticated statistical techniques. These are intended to stretch to the limit the meager supply of facts. Like the economic models they are supposed to implement, the validity of these statistical tools depends itself on the acceptance of certain convenient assump- tions pertaining to stochastic properties of the phenomena which the particular models are intended to explain; assump- tions that can be seldom verified. Continued preoccupation with imag- inary, hypothetical, rather than with observable reality has gradually led to a distortion of the informal valuation scale used in our academic community to assess and to rank the scientific performance of its members. Empirical analysis, according to this scale, gets a lower rating than formal mathematical reasoning. Devising a new statistical procedure, however tenuous, that makes it possible to squeeze out one more unknown parameter from a given set of data, is judged a greater scientific achievement than the successful search for additional information that would permit us to measure the magnitude of the same parameter in a less ingenious, but more reliable way. The pursuit of a more fundamental understanding of the process of production inevitably leads into the area of engineering sciences. To penetrate below the skin-thin surface of conventional consumption functions, it will be necessary to develop a systematic study of the structural characteristics and of the functioning of households, an area in which description and analysis of social, anthropological and demographic factors must obviously occupy the center of the stage. Establishment of systematic coopera- tive relationships across the traditional frontiers now separating economics from these adjoining fields is hampered by the sense of self-sufficiency resulting from what I have already characterized as undue re- liance on indirect statistical inference as the principal method of empirical research. An exceptional example of a healthy balance between theoretical and empirical analysis and of the readiness of professional economists to cooperate with experts in the neighboring disciplines is offered by Agricultural Economics as it developed in this country over the last fifty years. A unique combination of social and political forces has secured for this area unusually strong organizational and generous finan- cial support. Official agricultural statistics are more complete, reliable, and systematic than those pertaining to any other major sector of our economy. Close collaboration with agronomists provides agricultural economists with direct access to informa- tion of a technological kind. Leontief (1970) Theoretical Assumptions and Nonobserved Facts (pdf) 55.15 Friedrich Hayek Austin on Hayek One might place the ‘free market’ at the top of this structure today as being among the latest cultural developments, made possible by formal property rights. Interestingly, Hayek, for all that he bequeathed us the neoliberal trap we find ourselves in, offered a very helpful phrase for capitalism. He didn’t much like the term, preferring to refer to the market system as the ‘extended order of human cooperation’. He was alert to the idea that the market had emerged out of earlier human cooperation to form something substantially new. With hindsight, his mistake that we are now suffering from is that in his eagerness to limit the powers of government, which might easily stray to authoritarianism, he overestimated the degree to which the ‘extended order’ market system could fully supersede the underlying layers in promoting long-term human wellbeing. Austin (2021) Market-led Sustainability is a ‘Fix that Fails’… (pdf) 55.16 William Nordhaus Bichler Nitzan The LA Times called the bluff: William D. Nordhaus won the Nobel prize in economics for a climate model that minimized the cost of rising global temperatures and undermined the need for urgent action. ‘The economics Nobel went to a guy who enabled climate change denial and delay’: It has been a scary month in climate science. Hurricane Michael and a frightening report from the U.N. Intergovernmental Panel on Climate Change underlined the potential costs of human-caused global warming. Then to add insult to injury, William Nordhaus won the economics Nobel Prize. Nordhaus was recognized for his work developing a model to guide policymakers on how best to address the costs and benefits of limiting greenhouse gases. That’s a noble goal, but Nordhaus’ work has no more helped to defuse the threat of global warming than Neville Chamberlain’s appeasement of Germany prevented World War II. Rather, Nordhaus’ low-ball estimates of the costs of future climate change and high-ball estimates of the costs of containing the threat contributed to a lost decade in the fight against climate change, lending intellectual legitimacy to denial and delay. Bichler Nitzan (2018) The Nordhaus Racket: How to use capitalization to minimize the cost of climate change and win a ‘Nobel’ for ‘sustainable growth’ 55.17 Axel Leijonhufved Farmer on Leijonhufved My view of modern macroeconomics is much like my view of modern Hollywood movies. The pyrotechnics are spectacular but the plots are sadly lacking. Modern macroeconomics is a degenerative research program that took a wrong turn in the 1950s. Farmer (2022) Axel Leijonhufved Remembered "],["economic-sectors.html", "56 Economic Sectors 56.1 Housing 56.2 Ocean (‘Blue’) Economy 56.3 Blue Hegemony", " 56 Economic Sectors 56.1 Housing 56.1.1 Land Tenure Finance Ryan-Collins Abstract his article examines the links between private property in land and the financial system. Private landed property (PLP) has played an important role in supporting the growth of modern banking and credit systems, industrialization, and economic democratization. However, since the 1980s, high-income economies have exhibited a strong preference for PLP as a form of tenure, in the form of home ownership in particular. This pattern has combined with financial liberalization and innovation to create a land-finance feedback cycle with negative social and economic outcomes. They include a housing affordability crisis for younger and poorer socioeconomic groups; rising wealth inequality as land rents have become more concentrated; economic stagnation due to capital misallocation; and increased financial fragility as household debt has exploded. We illustrate these historical processes in the Anglo-Saxon “home-owning democracies,” where they have been strongest, focusing in particular on the United Kingdom, Australia, and the United States. This article considers how alternative tenure arrangements and reforms to finance and taxation could help mediate these dynamics. Ryan-Collins Memo Residential Capitalism The dominant model of land tenure in high-­ income economies is private ownership, whether the use is commercial or residential. This fact is rarely questioned, but it could be considered one of the great paradoxes of modern capitalist economies. For, unlike most com- modities, land (considered as location) does not observe the basic rules of supply and demand upon which capitalist exchange and markets depend for their operation and efficiency. Land has special properties—­ inherent scarcity, fixity, and irreproducibility. As a result, increased demand yields higher economic rents, which tend to be capitalized into the market value of land. To say that land—­which absorbs the growing wealth of the community and wider society in which it sits—­ should be privately owned and its value only lightly taxed (relative to income and profits) is perverse since the “owner” has done nothing to merit such gains. It was for this very reason that the founding fathers of modern economics—­Adam Smith, David Ricardo, John Stuart Mill, and Karl Marx—­viewed the landed class and land rents more gener- ally as a threat to capitalist development. An important reason private landed property (PLP) has become so entrenched as a mode of tenure is its intimate relationship with finance. Titled, privately owned land is arguably the most attractive form of collateral in existence for financial institutions, given the above-­ mentioned special properties. By supporting the develop- ment of modern banking, PLP encouraged economic development and industrialization in both settler colonies and feudal regimes. The perverse economic effects of PLP are then, to some extent, counter-­ balanced by its enabling of finance and capitalist development. But the interaction between land and finance is a delicate one. As a result of financial liberalization and globalization in advanced economies, financial speculation has become the dominant motive for investment in land and its appurtenances (most notably residential housing) and the source of negative consequences for society and the economy. Policy attention has focused almost exclusively on so-­called supply-­ side solutions to the housing problem, whether it be deregulating planning or zoning systems or just building more affordable homes, with much less attention paid to the demand side of the equation or the underlying institutions that have created the land-­finance cycle. In academia, the term “financialization” (of housing/real estate) has become popularized since the 2008 global financial crisis (GFC). However, a number of urban scholars, including Anne Haila, noted the problem of land being exploited as a financial asset rather than a factor of production or consumption good well before the GFC. In one of her final works, critiques classical economists, who underestimated the ongoing power of landowners in capitalist accumulation. She also notes more recent trends of both corporations and the public sec- tor selling off land in order to release capital to their core business and public services, respectively. She calls for “a theory explaining landowners’ power and alliance with financiers, and the relationship between real estate and finance sectors”. In this article, I focus on the relationship between PLP and the financial sector over time, with special attention to Anglo-­Saxon lib- eral capitalist economies, in particular the United Kingdom, the United States, and Australia, where this mode of tenure, along with financial liberalization, has been promoted most vigorously. In the hierarchy of risk of property titles, land carries the least risk, being superior to real capital goods, tradable assets, and contracted income. The more secure the debtor’s property title, the more easily he or she can secure credit and the lower the rate of interest. The rate of interest is then determined by the quality of collateral, and there is no “natural rate” that clears the goods market achieving equilibrium. This point has been noted by some economists critical of an equilibrium notion of perfect information. Credit constraints, which are judged by the degree of liberalization of the mortgage credit markets, are the “elephant in the room,” helping to ex- plain significant differences in house prices and consumption between countries such as Germany, the United States, the United Kingdom, and Japan. The deregulation of mortgage finance was initiated in the United States, the United Kingdom, and Australia following the collapse of the Bretton Woods agreement, which led to the freeing up of interna- tional credit flows and increasing competition between the New York and London financial sectors (Helleiner 1994; Krippner 2011). The election of conservative, free-­ market-­ oriented leaders (Reagan and Thatcher) in the United States and the United Kingdom led to the re- peal of regulations that prevented banks from competing with build- ing societies and other established housing finance institutions. Limits on interest rate charges and tax disadvantages were removed, along with other sectoral credit controls on mortgage credit—­the so-­called Big Bang. The removal of foreign exchange controls also made banks less dependent on domestic deposits for their funding, de-­linking do- mestic incomes from mortgage credit growth. From the mid-­1970s, U.S. banks were able borrow from abroad to finance mortgages, in particular from the largely unregulated “Euro-­dollar” market. Domestic financial innovations also enabled banks to attract deposit funding away from the thrifts. By opening up housing finance to a vast global investment sector, it broke down previous na- tional and local institutional barriers to the funding of home purchase. A key development motivating these dynamics in high-­ income economies in the 1980s was the emergence of a new international regulatory framework—­the “Basel Accords”—­that introduced for all banks minimum capital requirements that are related to the type of assets they held. Loans secured by mortgages on residential properties only carried half the risk weight (50 percent) of loans to non-­financial firms in the original Basel Accord. Securitized mortgages, which were viewed as more liquid and thus even less risky, only carried a 20 per- cent risk weight. The effect of these reforms was to allow banks to earn fees and net interest margins by holding 2.5 times more credit risk in real estate than they had before, without any increase in their capital requirements. These regulatory strategies can be seen as a classic example of the fallacy of composition. Regulators and banks, encouraged by policy- makers keen to boost homeownership levels, were right to consider that at the level of any individual bank, a residential mortgage loan will be less risky than an unsecured loan to a firm. But from the perspective of macroeconomic and macro-­financial stability, the synchronized expansion of mortgage credit well beyond the rate of growth of GDP and of incomes was clearly problematic. Until the 2007–­2008 crisis, however, central banks were reluctant to act, continuing to strictly observe their mandated focus on consumer price stability. Credit and finance are not neutral. Where they go determines their effect on the economy. Traditional lending to firms supports capital investment and helps pay wages, leading to increased GDP transac- tions, economic growth, and productivity. The increased growth in the economy enables firms to pay back both the principal and the interest, preventing the build-­up of excessive debt overhangs. But credit creation for the purchase of existing property and land increases property prices without stimulating investment or wages. Households must either take on more debt or reduce their spending, leading firms to cut back on investment, leading to lower profits and stagnating wages. This, in turn, feeds into more demand for mortgage debt as house prices continue to rise relative to incomes, generating a positive feedback cycle where increasing mortgage credit effectively creates its own supply. A study of 46 economies over 1990–­2011 found a negative relationship between the stock of bank lending to domestic real estate and economic growth but positive growth effects of credit flows to non-­financial business. In Anglo-­Saxon economies, homeownership levels appear to have peaked in the early 2000s and have been falling since then, despite further increases in mortgage debt relative to GDP. In these countries, housing wealth and land rents have become more concentrated in older and richer cohorts, with significant growth in “petty landlordism” and second-­home ownership. Privatized Keynesianism This model of economic development has been termed “privat- ized Keynesianism” or “house-price Keynesianism”. Encouraging the personal accumulation of assets, such as housing equity, as a means of meeting the cost of social care and retirement needs in an aging population also made political sense to neoliberal governments keen on reducing the role of the state. “Asset-­based wel- fare” began to emerge as a new policy framework, with homeowner- ship leading to less support for higher taxes to fund universal welfare provision and pensions. The embrace of financial liberalization and homeownership by Anglo-­Saxon capitalism may have also been driven, in part, by broader national economic strategies towards globalization. These countries saw their export industries, in particular manufacturing, facing fierce competition from China and other emerging markets and may have seen attracting foreign investment into real estate and other financial assets as a means to offset the resulting trade deficits. Other Western economies, such as Germany and Sweden, were able to preserve their manufacturing sectors and generate current account surpluses that made assetinflation a less attractive macroeconomic strategy. Post-­2008 Developments Post-­crisis, central banks have taken a closer interest in monitoring house prices and introduced macro-­ prudential policies aimed at restricting real estate credit to address “systemic risks” across national economies (Cerutti et al. 2017). Regulators have imposed limits to loan-­to-­value and loan-­to-­income ratios for mortgages and also targeted buy-­to-­let and interest-­only mortgages with some success in the United Kingdom, Australia, Switzerland, New Zealand, and Hong Kong. However, countervailing this has been extraordinarily loose mon- etary policy. Short-­term policy interest rates have been reduced to the zero lower bound, whilst quantitative easing (QE) programs have driven down medium-­ and longer-­term rates via the vacuuming up of government bonds from capital markets. The hope was that this would lead investors to invest more in risky, real-­economy investments such as debt and equity issued by companies. But the evidence suggests that, rather than stimulating real-­economy growth, QE has pumped up asset prices, in particular house prices. The “wall of liquidity” created by QE catalyzed a global search for higher yielding, but safe, assets). Landed property, particularly in rich global cities, proved to be one of the most attractive assets for investors with global reach, not least because they could easily source borrowing, backed by property assets, at ultra-­low interest rates from a banking sector still with a preference for real estate. Property prices in global cities have “synchronized,” with price dynamics closer to each other than with cities and regions in domestic hinterlands (Duca 2020). Although speculative buyers from both home and abroad usually target “prime” (very expensive) properties, speculation raises prices across these cities and means they become unaffordable for those on middle incomes. A financial sector that has become so dependent on high and rising collateral values. Only a developer protected from the profit motive, such as the state itself, can ever have any incentive to produce houses at a rate that would lower the cost of housing overall in the area they are being built. East Asian states, in particular Singapore and Hong Kong, adopted a strategy of public land value (or rent) capture, becoming “property states.” In Singapore, 90 percent of the land is owned by the state, which leases it out for development, enabling it to capture land value i ­ ncreases as leases come up for renewal; 82 percent of the resident population lives in high-­quality public housing provided by the state. This creates a virtuous circle of socialized non-­bank mortgage finance that has proven effective at providing affordable housing. The average house-­ price-­to-­income ratio in Singapore is one of the lowest in Asia and has been falling since a housing bubble in the mid-­1990s. Meanwhile, the system provides the Singapore government with a handsome source of public revenues. More aggressive macro-­prudential policy would seem the most obvi- ous and easiest first step for central banks and financial regulators seeking to reduce the flow of mortgage credit into real estate. The easiest way to introduce such a scheme might be to have some form of productive credit ratio, whereby a minimum ratio (such as 30 percent) of a bank’s assets should support non-­financial firms. Currently, that ratio is around 10 percent on average in the United Kingdom. Regulations should support banks that are able to de-risk their loans via methods other than property-based collateral. “Stakeholder banks” are more focused on business lending, do not have such stringent collateral requirements, and devolve decision-making to branches. They de-risk their loans not by requiring property as collateral but by building up strong and long-­lasting relationships with and understanding of the businesses they lend to. A tax on the incremental increase in the unimproved market value of land that would fall upon the landowner is the obvious policy choice, following Henry George’s ([1879] 1884) concept of a land value tax (LVT). By attaching a cost to owning land, LVT diminishes the incentive to buy land for speculative purposes—­in hopes of real- izing capital gains—­rather than for productive purposes or simply to provide shelter. By the 1980s, the interaction between titled land and finance morphed into a damaging feedback cycle whereby the financial sectors became addicted to property as the main source of profits, collateral, and dominant assets on the bal- ance sheets of financial institutions. The more credit flows into land, the higher house prices and collateral prices go, and the more attrac- tive property becomes as an asset against which to lend. Ultimately, this leads to land and house prices rising well above incomes, driving up land rents and creating financial fragility and widening wealth inequality. Ryan-Collins (2021) Private Landed Property and Finance: A Checkered History (pdf) 56.2 Ocean (‘Blue’) Economy Crona Abstract Ocean activities are rapidly expanding as Blue Economy discussions gain traction, creating new potential synergies and conflicts between sectors. To better manage ocean sectors and their development, we need to understand how they interact and the respective outcomes of these interactions. To provide a first comprehensive picture of the situation, we review 3187 articles to map and analyze interactions between economically important ocean sectors and find 93 unique direct and 61 indirect interactions, often mediated via the ocean ecosystem. Analysis of interaction outcomes reveals that some sectors coexist synergistically (e.g. renewable energy, tourism), but many interactions are antagonistic, and negative effects on other sectors are often incurred via degradation of marine ecosystems. The analysis also shows that ocean ecosystems are fundamental for supporting many ocean sectors, yet 13 out of 14 ocean sectors have interactions resulting in unidirectional negative ecosystem impact. Fishing, drilling, and shipping are hubs in the network of ocean sector interactions, and are involved in many of the antagonistic interactions. Antagonistic interactions signal trade-offs between sectors. Qualitative analysis of the literature shows that these tradeoffs relate to the cumulative nature of many ecosystem impacts incurred by some sectors, and the differential power of ocean sectors to exert their rights or demands in the development of the ocean domain. There are also often time lags in how impacts manifest. The ocean governance landscape is not currently well-equipped to deal with the full range of trade-offs, and opportunities, likely to arise in the pursuit of a Blue Economy in a rapidly changing ocean context. Based on our analysis, we therefore propose a set principles that can begin to guide strategic decision-making, by identifying both tradeoffs and opportunities for sustainable and equitable development of ocean sectors. Crona (2021) Sharing the seas: a review and analysis of ocean sector interactions (pdf) 56.2.1 Financing the Ocean Economy Sumaila Abstract The ocean, which regulates climate and supports vital ecosystem services, is crucial to our Earth system and livelihoods. Yet, it is threatened by anthropogenic pressures and climate change. A healthy ocean that supports a sustainable ocean economy requires adequate financing vehicles that generate, invest, align, and account for financial capital to achieve sustained ocean health and governance. However, the current finance gap is large; we identify key barriers to financing a sustainable ocean economy and suggest how to mitigate them, to incentivize the kind of public and private investments needed for topnotch science and management in support of a sustainable ocean economy. Sumaila (2021) Financing a sustainable ocean economy 56.3 Blue Hegemony Schutter Abstract The blue economy has become an influential concept in international and national marine governance discourse. Various contested interpretations exist, and different actors choose to emphasise different aspects of the triple goal of environmental, economic, and social improvements. However, despite disagreement over its in­ terpretations, the blue economy finds support in many different arenas. This paper explores the position of dominance that the blue economy has reached, and examines how supporters of the concept maintain and employ power to keep it relevant. The paper applies a mixed-methods approach: 29 semi-structured interviews with people in roles of formal decision-making across the fisheries sector, economic development and tourism sector, conservation and environment sector, and specific blue economy-institutions are supplemented by ob­ servations from the wider landscape during 4 months of fieldwork in Seychelles. Findings show that in inter­ national discourse, the blue economy obtains and maintains its influence through persuasion and through the construction of a ‘common sense’ and productive way forward, capable of achieving triple wins. Within this narrative, oceans are undergoing a reconfiguration as economic frontiers, and the blue economy places economic growth from oceans centrally within contemporary environmental governance. Maintaining the blue economy as a powerful concept on the ground is done through social power relations: the blue economy functions as a boundary object, contributing to depoliticisation of discussions about a shared vision. Depoliticisation allows Seychelles to continue using the concept despite simmering dissent among policy makers, practitioners, and resource users. Dominance of the blue economy on the international stage means that associating with it brings Seychelles visibility and influence. The usefulness of the concept in eliding tensions makes it difficult for counter- hegemony to arise, although alternatives are emerging elsewhere, such as blue justice. However, fundamental change is needed to re-politicise environmental decision-making and explicitly discuss values and images attached to the blue economy. Schutter Memo The blue economy has widely gained influence at the international stage, and ties in with the hegemonic regimes of contemporary global environmental governance. It has become influential through persua­ sion and consent, and the boundary object status is useful to facilitate communication but also to offer something to everyone. The triple bottom line promise creates an appealing sense of progressive change, benefiting the economic, environmental, and social dimension simul­ taneously. Interest in the blue economy is further fuelled by framings of the ocean as underdeveloped and underexplored [31,62], and in need of rational management. The blue modernisation narrative thus absorbs issues associated with the ocean economy (e.g. coral bleaching, pollu­ tion, industrial overfishing). It avoids “challeng[ing] the factors causing our ecological ills” [77], constituting a passive revolution of continued and even accelerated exploitation through closing off pathways to alternative trajectories [66]. The emerging conceptualisations and proposed ways of governing oceans also determine who is considered to be connected to the ocean, and consequently, which stakeholders have a voice in blue economy debates. The lack of a culture of local civil society engagement and the ambiguity in the concept means that local engagement in Seychelles has proven difficult. Instead, voices from the international civil society have managed to gain positions of influence, strengthened by the increased reliance on NGOs in marine environmental governance following eco­ nomic restructuring and cuts in government budgets. Schutter (2021) The blue economy as a boundary object for hegemony across scales (pdf) "],["about.html", "A About", " A About Dyre Haugen and Dyrehaugen is Webian for Jon Martin - self-owned Globian, Webian, Norwegian and Canarian with a background from industrial research policy, urban planning and economic development consulting on global, regional and urban scales. I am deeply concerned about the (insane) way humanity (i.e. capitalism) interfere with nature. In an effort to gain insights in how and why this happens stuff is collected from around the web and put together in a linked set of web-sites. The sites are operated as personal notebooks. However, these days things can be easily published to the benefit of others concerned with the same issues. But be aware - this is not polished for presentation or peer-reviewed for exactness. I offer you just to have a look at my ‘work-desk’ as it appears in the moment. Any comment or suggestion can be mailed to dyrehaugen@gmail.com You can follow me on twitter as @dyrehaugen. Thanks for visiting! "],["links.html", "B Links", " B Links Current Dyrehaugen Sites: rcap - On Capitalism (loc) rclm - On Climate Change (loc) recs - On Economics (loc) rfin - On Finance (loc) rngy - On Energy (loc) renv - On Environment (loc) rsts - On Statistics (loc) rurb - On Urbanization (loc) rvar - On Varia (loc) rwsd - On Wisdom (loc) Blogs: rde - Blog in English (loc) rdn - Blog in Norwegian (loc) Discontinued: jdt - Collection (Jekyll) (loc) hdt - Collection (Hugo) (loc) Not listed: (q:) dhe dhn jrw56 (z:) rcsa rpad rstart "],["news.html", "C NEWS C.1 210717 Carney calls for stronger Government Regulation", " C NEWS C.1 210717 Carney calls for stronger Government Regulation For the world to meet its climate goals, governments would have to force industries to follow clear rules, on everything from energy generation to construction and transport, and set carbon prices that would drive investment towards green ends and close down fossil fuels. “We need clear, credible and predictable regulation from government,” he said. “Air quality rules, building codes, that type of strong regulation is needed. You can have strong regulation for the future, then the financial market will start investing today, for that future. Because that’s what markets do, they always look forward.” Without such robust intervention from governments, markets would fail to address the crisis. Gurdian "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
